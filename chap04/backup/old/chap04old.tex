 
% Calculating RC_Schur vectors from eigenvalues?  (orthogonalize an S matrix??)
% (or is it better to keep track of transformation matrices during the QR method?)

Chapter \ref{chap01} reviewed the notation to be used throughout this text to frame efficient numerical methods for solving a variety of practical problems.
Using this notation, NRchap\ref{chap02} presented several algorithms for the efficient solution of systems of
the form $A\x = \b$ for the unknown vector $\x$; we will need to solve such systems repeatedly in the numerical algorithms to come.
Chapter \ref{chap03} introduced a selection of iterative algorithms for some basic equations which are too difficult to solve directly; we will resort to such iterative algorithms for many of the problems to come.
This chapter focuses on several additional concepts and algorithms that clarify the {\bf linear algebra} describing how the
matrices at the heart of more involved problems may be characterized and decomposed.
Note that {\it \S 4 is the most difficult and important chapter of Part I of this text.} 

\section{The four fundamental subspaces of a matrix}\label{sec.A.A.I}

An appropriate starting point in a discussion of linear algebra is to
describe the action of an $m\times n$ matrix $A$ in terms of its {\bf four fundamental subspaces},
as introduced in \S \ref{sec.A.A.H}, defined precisely below, summarized in
Figures \ref{cartoon1}-\ref{cartoon3}, and illustrated by example
in Table \ref{tab:sidewaystable}.
\beginmylistb
\item The {\bf column space} of $A$, a.k.a.~the {\bf range} of $A$, is
the subspace of all complex (or real) vectors $\y$ of order $m$ (that
is, all $\y\in \Yss$) such that $\y=A\x$ for at least one value of $\x$
(that is, it is the set of all vectors $\y$ spanned by the columns of
$A$).  It is denoted $\Css=\Ran(A)=\Span\{\a^{1},\a^{2},\ldots,\a^{n}\}$, and
has dimension $r$.
\item The {\bf row space} of $A$ is the subspace of all complex (or
real) vectors $\x$ of order $n$ (that is, all $\x\in \Xss$) such that
$\x^{H}=\y^{H} A$ for at least one value of $\y$.  In other words, it
is the set of all $\x$ such that $\x=A^{H} \y$ for at least one value
of $\y$.  It is thus denoted $\Rss=\Ran(A^{H})$, and has dimension $r$.
\item The {\bf nullspace} of $A$ is the subspace of all $\x\in \Xss$
such that $A\x=0$.  The nullspace is the orthogonal complement of the
row space.  It is denoted $\Nss=\Null(A)=\Rss^{\perp}$, and has
dimension $n-r$.
\item The {\bf left nullspace} of $A$ is the subspace of all $\y\in
\Yss$ such that $\y^{H} A=0$, and is the orthogonal complement of the
column space.  In other words, it is the set of all $\y$ such that
$A^{H} \y = 0$.  It is thus denoted $\Lss=\Null(A^{H})=\Css^{\perp}$,
and has dimension $m-r$.
\endmylist
If a square matrix $A$ is nonsingular, then $r=\Rank(A)=m=n$ (see Fact \ref{fact.A.A.H.A}), and thus
the dimension of both the nullspace and the left nullspace of $A$, as
depicted in Figure \ref{cartoon1}, are zero (that is, they both contain only the
zero element).  In this case, the mapping between $\Xss$ and $\Yss$ in
Figure \ref{cartoon1} is one-to-one.  That is, given any $\y\in \Yss$, one can
uniquely determine the corresponding $\x\in \Xss$ such that $\y=A\x$.
This mapping is given by the inverse such that, for any $\x\in \Xss$,
$A^{-1}\y=A^{-1}(A\x)=\x$.

If a matrix $A$ is singular or nonsquare, then the mapping between $\Xss$
and $\Yss$ in Figure \ref{cartoon1} is not one-to-one.  If $r<n$, then the component
$\x_{\scriptscriptstyle \Nss}$ of a vector $\x$ lying in the nullspace of $A$ is mapped to
zero by the transformation $A\x$.  If $r<m$, then the component
$\y_{\scriptscriptstyle \Lss}$ of a vector $\y$ lying in the left nullspace of $A$ may not
be reached by the transformation $\y=A\x$ for any $\x$.  The
``best'' mapping possible from $\y$ back to $\x$ under such conditions
is given by the {\bf Moore-Penrose pseudoinverse} $A^+$ (see Figure \ref{cartoon3} and \S \ref{sec.A.E});
the {\bf singular value decomposition} developed in \S \ref{sec.A.D.G} renders the computation and analysis of $A^+$ straightforward.

\section{The determinant}\label{sec.A.B}

\begin{subequations}
There are several measures with which the properties of a matrix may
be quantified.  Perhaps the most fundamental of these measures is the
{\bf determinant}, denoted $|A|$.  The determinant of a square matrix
$A$ may be defined iteratively as follows: 
\beginmylistb
\item $1\times 1$ {\bf case:} The determinant of a $1\times 1$ matrix $A=\begin{pmatrix} a_{11} \end{pmatrix}$ is just $|A|=a_{11}$.
\item $2\times 2$ {\bf case:} The determinant of a $2\times 2$ matrix
$\dis A=\begin{pmatrix} a_{11} & a_{12}\cr a_{21}& a_{22}\end{pmatrix}$
is $|A|= a_{11}\, a_{22} - a_{12}\, a_{21}$. 
\item $n\times n$ {\bf case:} The determinant of an $n\times n$ matrix
is defined as a function of the determinant of
several $(n-1) \times (n-1)$ matrices as follows: the determinant of
$A$ is a linear combination of the elements of row $\alpha$ (for any
$\alpha$ such that $1\le\alpha\le n$) and their corresponding
{\bf cofactors} $A_{\alpha \beta}$:
\begin{equation}
|A| = a_{\alpha 1}\,A_{\alpha 1} + a_{\alpha 2}\,A_{\alpha 2} + \cdots + a_{\alpha n}\,A_{\alpha n}
= \sum_{\beta=1}^{n} (-1)^{\alpha+\beta} a_{\alpha \beta} |M_{\alpha\beta}|\quad \textrm{for some } \alpha \in [1,2,\ldots n],
\label{eq:methofcofactors;a}
\end{equation}
where the {\bf cofactor} $A_{\alpha \beta}$ is defined as the
determinant of $M_{\alpha \beta}$ with alternating sign: $A_{\alpha
\beta} = (-1)^{\alpha +\beta} |M_{\alpha \beta}|$, where the {\bf
minor} $M_{\alpha \beta}$ is the matrix formed by deleting row $\alpha
$ and column $\beta$ of the matrix $A$.  Alternatively, the
determinant of an $n\times n$ matrix may be defined as a linear
combination of the elements of column $\beta$ (for any $\beta$ such
that $1\le\beta\le n$) and their corresponding cofactors:
\begin{equation}
|A| = a_{1\beta}\,A_{1\beta} + a_{2\beta}\,A_{2\beta} + \cdots + a_{n\beta}\,A_{n\beta}
= \sum_{\alpha=1}^{n} (-1)^{\alpha+\beta} a_{\alpha \beta} |M_{\alpha\beta}|\quad \textrm{for some } \beta \in [1,2,\ldots n].
\label{eq:methofcofactors;b}
\end{equation}
\endmylist
\end{subequations}

\subsection{Some important properties of the determinant, and their consequences}\label{sec.A.B.A}

\noindent The determinant has five important properties that may, with
some effort, be verified by its definition: \vskip0.1in

\begin{property} Adding a multiple of one row (or block row) of a matrix to another
row (or block row) leaves the determinant unchanged.  In particular,
\begin{equation*}
\left|\begin{matrix} a & b \cr c & d \end{matrix}\right| =
\left|\begin{matrix} a & b \cr 0 & d-\frac{c}{a}b\end{matrix}\right|=
\left|\begin{matrix} a-\frac{b}{d}c & 0 \cr c & d\end{matrix}\right|\qquad \textrm{and} \qquad
\left|\begin{matrix} A & B \cr C & D \end{matrix}\right| =
\left|\begin{matrix} A & B \cr 0 & D-C\,A^{-1}\,B\end{matrix}\right|=
\left|\begin{matrix} A-B\,D^{-1}\,C & 0 \cr C & D\end{matrix}\right|,
\end{equation*}
provided that each of the operations performed is valid (that is, in
the examples shown above, when $a\ne 0$, $d\ne 0$, $A$ is nonsingular,
or $D$ is nonsingular, respectively).
\end{property}

\begin{property} Exchanging two rows of the matrix flips the sign of
the determinant, e.g.,
\begin{equation*}
  \left|\begin{matrix}a & b \cr c & d\end{matrix}\right| = -
  \left|\begin{matrix}c & d \cr a & b\end{matrix}\right|.
\end{equation*}\vskip-0.06in
\end{property} 
    
\begin{property}\hskip-0.065in {\bf a} If $A$ is (upper or lower)
triangular (or diagonal), then $|A|$ is the product
$a_{11}\,a_{22}\cdots a_{nn}$ of the elements on the main diagonal.
In particular, the determinant of the identity matrix is $|I|=1$.

\noindent {\bf Property 3b} If $A$ is (upper or lower) block
triangular (or block diagonal), then $|A|$ is the product of the
determinants of the blocks on the main diagonal.
\end{property} 

\begin{property}\hskip-0.065in {\bf a} If $|A|\ne 0$, then the rows of
$A$ are linearly independent, and $A$ is nonsingular / invertible /
full rank (\ie, $A\x=\b$ has a unique solution $\x$).

\noindent {\bf Property 4b}  If $|A| = 0$, then the rows of $A$ are
linearly dependent, and $A$ is singular / non-invertible / rank
deficient (\ie, $A\x=\b$ either has an infinite number of solutions or
zero solutions, depending on $\b$).
\end{property} 

\begin{property} $|AB|=|A|\cdot |B|$.  In particular, $|AB|=0$
iff $|A|=0$ or $|B|=0$, and $|A^{-1}|=1/|A|$.
\end{property}
\clearpage

\def\AsupH{A^H}
\def\xsubA{\x_{\scriptscriptstyle \Rss}}
\def\AsupP{A^+}
\def\AsupI{A\protect^{-1}}

\doFigure{cartoon1}{~\vskip-0.5in\begin{psfrags}
\psfrag{n}[bl][]{\begin{parbox}{1.2in}{nullspace,\\ $\Nss=\Null(A)=\Rss^\perp$}\end{parbox}}
\psfrag{r}[tl][]{\begin{parbox}{0.9in}{row space,\\ $\Rss=\Ran(A^{H})$}\end{parbox}}
\psfrag{c}[tl][]{\begin{parbox}{1.2in}{column space,\\ {\color{white} co} $\Css=\Ran(A)$}\end{parbox}}
\psfrag{l}[bl][]{\begin{parbox}{1.2in}{left nullspace,\\ $\Lss=\Null(A^{H})=\Css^\perp$}\end{parbox}}
\psfrag{1}[bl][]{$0$}
\psfrag{2}[bl][]{$\x_{\scriptscriptstyle \Rss}$}
\psfrag{3}[bl][]{$\x_{\scriptscriptstyle \Nss}$}
\psfrag{4}[bl][]{$\x=\x_{\scriptscriptstyle \Rss}+\x_{\scriptscriptstyle \Nss}$}
\psfrag{5}[bl][]{$A\x_{\scriptscriptstyle \Rss}=\y_{\scriptscriptstyle \Css}$}
\psfrag{6}[bl][]{$A\x=\y_{\scriptscriptstyle \Css}$}
\psfrag{7}[tl][]{$A \x_{\scriptscriptstyle \Nss}=0$}
\psfrag{8}[bl][]{$\y_{\scriptscriptstyle \Css}$}
\psfrag{9}[bl][]{$0$}
\psfrag{Rm}[bc][]{$\y\in\Yss=\Complex^{m}$ (or $\Real^{m}$)}
\psfrag{Rn}[br][]{$\x\in\Xss=\Complex^{n}$ (or $\Real^{n}$)}

\centerline{~\hskip0.3in\includegraphics[width=5.0in]{figs/subspaces.eps}}
\end{psfrags}\vskip-0.17in
}{Cartoon depicting the four fundamental subspaces of the matrix $A$, adapted from Strang (1988).}

\doFigure{cartoon2}{~\vskip-0.4in\begin{psfrags}
\psfrag{n}[bl][]{nullspace}
\psfrag{r}[tl][]{row space}
\psfrag{c}[tl][]{column space}
\psfrag{l}[bl][]{left nullspace}
\psfrag{1}[bl][]{$0$}
\psfrag{2}[bl][]{$\y_{\scriptscriptstyle \Css}$}
\psfrag{2a}[bl][]{$\x_{\scriptscriptstyle \Rss}$}
\psfrag{3}[bl][]{$\y_{\Lss}$}
\psfrag{4}[bl][]{$\y=\y_{\scriptscriptstyle \Css}+\y_{\scriptscriptstyle \Lss}$}
\psfrag{5}[bl][]{$A \x_{\scriptscriptstyle \Rss}=\y_{\scriptscriptstyle \Css}$}
\psfrag{6}[bl][]{$A^H\y=\x_{\scriptscriptstyle \Rss}'$}
\psfrag{7}[tl][]{$A^H \y_{\scriptscriptstyle \Css}=A^H(A\x_{\scriptscriptstyle \Rss}) =\x_{\scriptscriptstyle \Rss}' \ne \x_{\scriptscriptstyle \Rss}$}
\psfrag{d}[bl][]{$A^H\y_{\scriptscriptstyle \Lss}=0$}
\psfrag{8}[bl][]{$\x_{\scriptscriptstyle \Rss}'$}
\psfrag{9}[bl][]{$0$}

\centerline{~\hskip0.3in\includegraphics[width=5.0in]{figs/subspacesH.eps}}
\end{psfrags}\vskip-0.17in
}{As in Figure \ref{cartoon1}, illustrating the mapping due to $\protect\AsupH$.}

\doFigure{cartoon3}{~\vskip-0.4in\begin{psfrags}
\psfrag{n}[bl][]{nullspace}
\psfrag{r}[tl][]{row space}
\psfrag{c}[tl][]{column space}
\psfrag{l}[bl][]{left nullspace}
\psfrag{1}[bl][]{$0$}
\psfrag{2a}[bl][]{$\x_{\scriptscriptstyle \Rss}$}
\psfrag{2}[bl][]{$\y_{\scriptscriptstyle \Css}$}
\psfrag{3}[bl][]{$\y_{\scriptscriptstyle \Lss}$}
\psfrag{4}[bl][]{$\x_{\scriptscriptstyle \Nss}$}
\psfrag{x}[tl][]{\begin{parbox}{2.7in}{{\color{white} co} forward: $A \x_{\scriptscriptstyle \Rss}=\y_{\scriptscriptstyle \Css}$\\
                                       inverse: $A^+ \y_{\scriptscriptstyle \Css}=A^+(A\x_{\scriptscriptstyle \Rss})=\x_{\scriptscriptstyle \Rss}$}\end{parbox}}
\psfrag{y}[bl][]{$A^+ \y_{\scriptscriptstyle \Lss}=0$}
\psfrag{z}[bl][]{$A \x_{\scriptscriptstyle \Nss}=0$}
\psfrag{9}[bl][]{$0$}

\centerline{~\hskip0.3in\includegraphics[width=5.0in]{figs/subspacesplus.eps}}
\end{psfrags}\vskip-0.17in
}{As in Figure \ref{cartoon1}, illustrating the forward and inverse mapping due to $A$ and $\protect\AsupP$
(see \protect\S \ref{sec.A.E}).  If $A$ is nonsingular (i.e., if $r=m=n$),
then the nullspace and left nullspace are given by $\protect\{0\protect\}$, and thus $\protect\AsupP=\protect\AsupI$.}
\clearpage

\begin{sidewaystable}
%\begin{landscape}\begin{table}
\centerline{%~\hskip0.2in
$\mbox{${}$\begin{tabular}{c|c|c|c|c|c|c}
matrix & column space & left nullspace & row space   & nullspace   & 
\multirow{2}{0.6in}{$A$ is \ldots} & \multirow{2}{0.8in}{$A\x=\b$ is \ldots} \\
$A$    & $\Css=\Ran(A)$ & $\Lss=\Null(A^{H})$   & $\Rss=\Ran(A^H)$ & $\Nss=\Null(A)$ & & \\ \hline
$\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ &
$\begin{gathered} \Span\left\{ \begin{pmatrix} 1 \\ 3 \end{pmatrix}, \begin{pmatrix} 2 \\ 4 \end{pmatrix} \right\} \\ = \Complex^{2} \end{gathered}$ & $\{0\}$ &
$\begin{gathered} \Span\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 3 \\ 4 \end{pmatrix} \right\} \\ = \Complex^{2} \end{gathered}$ &
$\{0\}$ &
\begin{parbox}{1.1in}{~\\{\bf  square} ($m=n$),\\
{\bf  invertible} /\\
{\bf  nonsingular} /\\
{\bf full rank} \\
($r=m$, $r=n$)\\[-0.08in]}\end{parbox} &
\begin{parbox}{0.9in}{{\bf  uniquely \\  determined} \\ {\small (as $\Nss=\Lss=\{0\}$)} \\ \small $\Rightarrow$ 1 solution}\end{parbox} \\ \hline
$\begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} \right\}$ 
& $\Span\left\{ \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$ &
$\begin{gathered} \Span\left\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \right\} \\ = \Complex^{2} \end{gathered}$ &
$\{0\}$ &
\begin{parbox}{1.1in}{~\\{\bf  tall} ($m>n$),\\
{\bf full (column) rank} \\
($r=n$)\\[-0.08in]}\end{parbox} &
\begin{parbox}{0.9in}{~\\  {\bf potentially \\ inconsistent} \\ {\small (since $\Lss \ne \{0\}$)} \\
\small $\Rightarrow$ 1 sol.~if $\b\in \Css$, \\ otherwise none\\[-0.08in]}\end{parbox} \\ \hline
$\begin{pmatrix} 1 & 0 & 0 \\ 1 & 2 & 3 \end{pmatrix}$ &
$\begin{gathered} \Span\left\{ \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 0 \\ 2 \end{pmatrix} \right\} \\ = \Complex^{2} \end{gathered}$ & $\{0\}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} \right\}$ &
$\Span\left\{ \begin{pmatrix} 0 \\ 3 \\ -2 \end{pmatrix} \right\}$ &
\begin{parbox}{1.1in}{~\\{\bf  fat} ($m<n$),\\
{\bf  full (row) rank} \\
($r=m$)\\[-0.08in]}\end{parbox} &
\begin{parbox}{0.9in}{{\bf  underdetermined} \\ {\small (since $\Nss \ne \{0\}$)} \\ \small $\Rightarrow$ $\infty$ solutions }\end{parbox} \\ \hline
$\begin{pmatrix} 1 & 2 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix} \right\}$ &
$\Span\left\{ \begin{pmatrix} 0 \\ 1 \end{pmatrix} \right\}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix} \right\}$ &
$\Span\left\{ \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$ &
\begin{parbox}{1.1in}{{\bf  fat} ($m<n$),\\
{\bf  rank deficient} \\
($r<m$, $r<n$)}\end{parbox} &
\begin{parbox}{0.9in}{~\\ {\bf  underdetermined} \\ {\small (since $\Nss \ne \{0\}$)} \\
and {\bf potentially \\ inconsistent} \\ {\small (since $\Lss \ne \{0\}$)} \\
\small $\Rightarrow$ $\infty$ sol.~if $\b\in \Css$, \\ otherwise none\\[-0.08in]}\end{parbox} \\ \hline
$\begin{pmatrix} 1 & 3 \\ 2 & 6 \end{pmatrix}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix}  \right\}$ &
$\Span\left\{ \begin{pmatrix} 2 \\ -1 \end{pmatrix} \right\}$ &
$\Span\left\{ \begin{pmatrix} 1 \\ 3 \end{pmatrix}  \right\}$ &
$\Span\left\{ \begin{pmatrix} 3 \\ -1 \end{pmatrix} \right\}$ &
\begin{parbox}{1.1in}{~\\
{\bf  square} ($m=n$),\\
{\bf  noninvertible} /\\
{\bf  singular} /\\
{\bf  rank deficient} \\
($r<m$, $r<n$)\\[-0.08in]
}\end{parbox} &
\begin{parbox}{0.9in}{~\\ {\bf  underdetermined} \\ {\small (since $\Nss \ne \{0\}$)} \\
and {\bf potentially \\ inconsistent} \\ {\small (since $\Lss \ne \{0\}$)} \\
\small $\Rightarrow$ $\infty$ sol.~if $\b\in \Css$, \\ otherwise none\\[-0.08in]}\end{parbox} \\ \hline
\end{tabular}}$ % \hskip0.5in~
}
\caption{\protect{Five simple examples indicating the column space, row space, nullspace, left nullspace, and the applicable names for some
representative matrices $A$ and their corresponding systems of linear equations $A\x=\b$.}}\label{tab:sidewaystable}
\end{sidewaystable}
%\end{table}\end{landscape}
\clearpage

Taking Properties 4a and 4b together, it is seen that $|A|=0$ iff $A$
is singular.  Properties 1 and 3 establish (by taking $A=I_{m\times
m}$ and $D=I_{n\times n}$) that $|I-CB|=|I-BC|$.  Properties 1 and 3
also establish the following.
 
\begin{fact} \label{fact.A.B.A.A}
Let $M=\begin{bmatrix} A & B \\ C & D
\end{bmatrix}$.  Then: $\begin{cases} \textrm{If $|M|\ne 0$ and $|A|\ne 0$, then
$|D-C\,A^{-1}\,B|\ne 0$.}\\ \textrm{If $|M|\ne 0$ and $|D|\ne 0$, then
$|A-B\,D^{-1}\,C|\ne 0$.} \end{cases}$
\end{fact}

\noindent This fact allows us to establish a block form of the Matrix Inversion
Lemma (again, easily verified simply by multiplying the original
matrix by the formulae given for its inverse).

\begin{fact}[The Matrix Inversion Lemma, part 2] \label{fact.A.B.A.B}
Let $\tilde A=\begin{bmatrix} A & B \\ C & D \end{bmatrix}$
and assume that $|\tilde A|\ne 0$.  Then:

\noindent a) If $|A|\ne 0$, then define a {\bf RC_Schur complement} of
$\tilde A$ as $G=D-CA^{-1}B$.  By Fact \ref{fact.A.B.A.A}, $|G|\ne 0$, and thus
\begin{equation*}
    \tilde A^{-1} = \begin{bmatrix} A & B \\ C & D \end{bmatrix}^{-1} =
    \begin{bmatrix} A^{-1}+A^{-1} B G^{-1} C A^{-1} & -A^{-1} B G^{-1} \\ -G^{-1} C A^{-1} & G^{-1} \end{bmatrix}.
\end{equation*}

\noindent b) If $|D|\ne 0$, define the other {\bf RC_Schur
complement} of $\tilde A$ as $H=A-BD^{-1}C$.  By Fact \ref{fact.A.B.A.A}, $|H|\ne 0$, and thus
\begin{equation*}
    \tilde A^{-1} = \begin{bmatrix} A & B \\ C & D \end{bmatrix}^{-1} =
    \begin{bmatrix} H^{-1} & -H^{-1} B D^{-1} \\ - D^{-1} C H^{-1} & D^{-1}+ D^{-1} C H^{-1} B D^{-1}  \end{bmatrix}.
\end{equation*}

\noindent c) If both $|A|\ne 0$ and $|D|\ne 0$, then by (a) and (b) above and the uniqueness of the matrix inverse (Fact \ref{fact.A.A.J.B}),
using both RC_Schur complements $G=D-CA^{-1}B$ and $H=A-BD^{-1}C$, we may write
\begin{equation*}
    \tilde A^{-1} = \begin{bmatrix} A & B \\ C & D \end{bmatrix}^{-1} =
    \begin{bmatrix} H^{-1} & -H^{-1}BD^{-1} \\  -G^{-1}CA^{-1} & G^{-1} \end{bmatrix}.
\end{equation*}
\end{fact}

\noindent Parts (a) and (b) of Fact \ref{fact.A.B.A.B} demonstrate how a given matrix inverse ($A^{-1}$ or $D^{-1}$, respectively)
may be updated via a computationally inexpensive algorithm when a (block) row and column are appended to the original matrix ($A$ or $D$).
It follows from part (c) of Fact \ref{fact.A.B.A.B} that
\begin{subequations}
\begin{align}
  \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} \x_o \\ \x_e \end{bmatrix} = \begin{bmatrix} \b_o \\ \b_e \end{bmatrix}
  \quad &\Rightarrow \quad
  \begin{bmatrix} \x_o \\ \x_e \end{bmatrix} = \begin{bmatrix} H^{-1} & -H^{-1}BD^{-1} \\  -G^{-1}CA^{-1} & G^{-1} \end{bmatrix} \begin{bmatrix} \b_o \\ \b_e \end{bmatrix} \\
  &\Rightarrow \quad
  \begin{bmatrix} A-BD^{-1}C & 0 \\  0 & D-CA^{-1}B \end{bmatrix} \begin{bmatrix} \x_o \\ \x_e \end{bmatrix} =
  \begin{bmatrix} I & -BD^{-1} \\  -CA^{-1} & I \end{bmatrix} \begin{bmatrix} \b_o \\ \b_e \end{bmatrix};
\end{align}
\end{subequations}
this relation forms the basis for the {\bf cyclic reduction} algorithm for the parallel solution of tridiagonal systems described in Exercise \ref{ex.4.cyclicreduction}.

Note also that, if $\alpha\ne \beta$, we may write
\begin{equation}
a_{\alpha 1}\,A_{\beta 1} + a_{\alpha 2}\,A_{\beta 2} + \cdots + a_{\alpha n}\,A_{\beta n} =0.
 \label{eq:methofcofactors;c}
\end{equation}
The above expression is valid because, as easily verified, it expresses the determinant of a new matrix $B$ which is identical to matrix $A$ except in row 
$\beta$, which has its former elements replaced by a copy of row $\alpha$; note that this modification to row $\beta$ leaves the cofactors $A_{\beta j}$ unchanged for all $j$, but (by Property 4b above)
makes the determinant of the new matrix zero. We may thus assemble \eqref{eq:methofcofactors;a} (for all $\beta$) together with \eqref{eq:methofcofactors;c} (for all $\{\alpha,\beta\}$ such that $\alpha\ne\beta$)
in the following convenient matrix form
\begin{equation*}
  \underbrace{\begin{pmatrix} a_{11} & a_{12} & \ldots & a_{1n} \\ a_{21} & a_{22} & \ldots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nn} \\ \end{pmatrix}}_A
  \underbrace{\begin{pmatrix} A_{11} & A_{21} & \ldots & A_{n1} \\ A_{12} & A_{22} & \ldots & A_{n2} \\
  \vdots & \vdots & \ddots & \vdots \\ A_{1n} & A_{2n} & \ldots & A_{nn} \\ \end{pmatrix}}_{A_\textrm{cof}} =
  \begin{pmatrix} |A| & & & 0 \\ & |A| & & \\ & & \ddots & \\ 0 & & & |A| \end{pmatrix}.
\end{equation*}
\eject

\noindent Note that the $\{i,j\}$ element of the {\bf cofactor matrix} $A_{\textrm{cof}}$ is the cofactor $A_{ji}$; in other words, the cofactors of $A$ are assembled into the cofactor matrix $A_{\textrm{cof}}$ in a transposed fashion.
The following fact follows immediately:
\begin{fact}[Cramer's rule] \label{fact:inversefromcofactors} $A^{-1}=A_{\textrm{\rm cof}} / |A|$.
\end{fact}
Fact \ref{fact:inversefromcofactors} is an extra"ordinarily expensive formula for the inverse when $A$ is large;
it is thus used only once (in \S \ref{sec:cttfandres}) in the remainder of this entire text.
Note that Fact \ref{fact.A.A.J.D} is a special case of Fact \ref{fact:inversefromcofactors} for $n=2$.

\subsection{Computing the determinant}\label{sec.A.B.B}

For a large matrix $A$, the determinant is most easily computed by
performing the row operations mentioned in Properties 1 and 2
of \S \ref{sec.A.B.A} to reduce $A$ to an upper triangular
matrix $U$.  (In fact, this is the heart of RC_Gaussian elimination
procedure, described in \S \ref{chap02}.)  Taking Properties 1, 2, and
3 of \S \ref{sec.A.B.A} together, it follows that
\begin{equation*}
|A| = (-1)^r\,|U| = (-1)^r\,u_{11}\,u_{22}\cdots u_{nn},
\end{equation*}
where $r$ is the number of row exchanges performed, and the
$u_{\kappa\kappa}$ are the elements of $U$ on the main
diagonal.  

\section{An introduction to eigenvalues and eigenvectors}\label{sec.A.C}

Consider the equation
\begin{equation}
A\s = \lambda \s.
\label{emodedef}
\end{equation}
For most values of $\lambda$, the only solution of this problem is the
{\bf trivial} solution $\s=0$.  However, for certain special values of
$\lambda$, this equation admits other {\bf nontrivial} solutions $\s\ne
0$.  These special values of $\lambda$ are called the {\bf
eigenvalues}, and the corresponding vectors $\s$ are called the {\bf
right eigenvectors}, or more commonly simply as the {\bf
eigenvectors}.  For these special values of $\lambda$, premultiplying
$\s$ by the matrix $A$ is equivalent to simply scaling $\s$ by the
factor $\lambda$.  Such a situation has the important physical
interpretation as a natural mode of a system when $A$ represents the
system matrix for a given dynamic system, as illustrated in \S
\ref{sec.A.C.B}.  Note also that those vectors $\r$ that satisfy the
equation $\r^{H} A = \lambda \r^{H}$ (equivalently, $A^{H}\r=\lambda^{H} \r$) are referred to as the {\bf left eigenvectors};
note that the alternative definition $\q^{T} A = \lambda \q^{T}$  (equivalently, $A^{T}\q=\lambda^{T} \q$)
of the left eigenvector is sometimes more convenient both algebraically and computationally,
so which definition is being used must always be specified when considering complex systems.

\subsection{Computing the eigenvalues and eigenvectors of small matrices}\label{sec.A.C.A}

The most direct way to determine for which $\lambda$ it is possible to
solve the equation $A\s = \lambda \s$ for $\s\ne 0$ is to rewrite
this equation as
\begin{equation*}
(\lambda I - A) \s= 0.
\end{equation*}
If $(\lambda I - A)$ is a nonsingular matrix, then this equation has a
unique solution, and since the RHS is zero, that solution
must be $\s=0$.  However, for those values of $\lambda$ for which $(\lambda I - A)$ is singular, this equation admits other solutions with
$\s\ne 0$.  The values of $\lambda$ for which $(\lambda I - A)$ is
singular are the eigenvalues of the matrix $A$, and the corresponding
vectors $\s$ are the eigenvectors.  Making use of Property 4 of the
determinant (see \S \ref{sec.A.B.A}), we see that the eigenvalues must therefore
be exactly those values of $\lambda$ for which
\begin{subequations}
  \label{chareqn}
\begin{equation}
|\lambda I - A| = 0 \qquad \Rightarrow \qquad p(\lambda) \triangleq \lambda^n + a_{n-1} \lambda^{n-1} + \ldots + a_{1} \lambda + a_{0} = 0.
  \label{chareqnunfactored}
\end{equation}
The determinant on the LHS of the first equation in \eqref{chareqnunfactored}, when multiplied out, is seen to be a polynomial in $\lambda$ of degree $n$, and is known as the {\bf characteristic polynomial} of $A$ and is usually denoted $p(\lambda)$;
the equation $p(\lambda)=0$ is referred to as the {\bf characteristic equation} of $A$.
  
\begin{fact}[The Fundamental Theorem of Algebra] \label{fact.A.C.A.A} 
Any $n$'th-order polynomial of the form \eqref{chareqnunfactored} has exactly $n$ complex roots, $\lambda_{1}$ through $\lambda_{n}$, and
may thus be written in the form
\begin{equation}
  (\lambda-\lambda_1) (\lambda-\lambda_2) \cdots (\lambda - \lambda_n) = 0.
  \label{chareqnfactored}
\end{equation}
\end{fact}
\end{subequations}
\enlargethispage{10pt}

\noindent Proof of the Fundamental Theorem of Algebra is given in Appendix \ref{chapAA}.
Note that the roots $\lambda_{1}$ through $\lambda_{n}$ are not necessarily {\bf distinct} (that is, they are not necessarily all
different); the number of times a particular eigenvalue is repeated is referred to as its {\bf multiplicity} (a.k.a.~{\bf algebraic multiplicity}; cf.~geometric multiplicity
as defined in footnote \ref{foot:geometricmult} on page \pageref{foot:geometricmult}).  An eigenvalue with multiplicity 1 is referred to as a {\bf simple eigenvalue}.

The set of all eigenvalues of the matrix $A$ is denoted
$\lambda(A)$.  To calculate the eigenvalues and eigenvectors of a
small matrix by hand, one may simply calculate the roots of its characteristic polynomial.  Once the eigenvalues $\lambda$ are known, the
corresponding eigenvectors $\s$ may be found by determining a nontrivial solution to the singular
system of equations $(\lambda I - A) \s=0$.  Efficient techniques to
solve a singular system of this sort are discussed in detail
in \S \ref{sec.A.A.H}.  Note that the eigenvectors $\s$ are defined only to within
an arbitrary multiplicative constant, which cannot be specified
uniquely because $(\lambda I - A)$ is singular.  In other words, if
$\s$ is an eigenvector corresponding to a particular eigenvalue
$\lambda$, then $c \s$ is also an eigenvector for any complex scalar
$c$.  It follows that

\begin{fact} \label{fact.A.C.A.B}
If $\dis A=A_{2\times 2}=\begin{pmatrix} a & b\\ c& d\end{pmatrix}$, then
$\lambda_{\pm}=\frac{1}{2} \big[(a+d) \pm \sqrt{4bc+(a-d)^2}\big]$.\\
If $A=A_{2\times 2}$ is Hermitian, then $a$ and $d$ are real and $b=\bar c$, and thus the eigenvalues of $A$ are real.\\
If $A=A_{2\times 2}$ is real, then the eigenvalues, if they are complex, are a complex conjugate pair.\\
If $b\ne 0$, the eigenvectors may be defined as
$\s_\pm = \begin{pmatrix} b \\ \lambda_{\pm} - a \end{pmatrix}$; if $c\ne 0$, they may be defined as
$\s_\pm = \begin{pmatrix} \lambda_{\pm} - d \\ c \end{pmatrix}$.\\
If $b=c=0$, the eigenvalues are $\lambda_1=a$ and $\lambda_2=d$ and the eigenvectors are
$\s_1=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ and $\s_2=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$.
\end{fact}

In general, for a matrix $A_{n \times n}$ with $n>4$, it is
impossible to compute the roots of the characteristic polynomial, and thus determine the eigenvalues of $A$, with a finite
sequence of calculations (see Facts \ref{fact.abel1} and \ref{fact.abel2}).
However, any of several efficient iterative
algorithms (akin to those introduced in \S \ref{chap03}) may be used for approximating eigenvalues and eigenvectors
numerically.  An introduction to such iterative techniques is
presented in \S \ref{sec.A.D}, along with the matrix decompositions upon which these techniques are based.  Note
that robust and efficient eigenvalue solvers have already been
developed in most computer languages used today for scientific
computing.  For the purpose of many typical applications, these prepackaged {\bf black box} algorithms may often be
called effectively without knowing exactly how they work.  In order to use such algorithms reliably, however,
it is valuable to understand the fundamental algorithms upon which they are built.
\enlargethispage{4pt}

\subsection{RC_Eigenmode analysis of an oscillating string}\label{sec.A.C.B}

To demonstrate the significance of eigenvalues and eigenvectors for characterizing physical systems, and to foreshadow
some of the developments in later chapters, it is enlightening at this
point to diverge for a bit from the main line of development in this chapter, and to discuss two physical problems motivating eigenvalue/eigenvector analysis.
Consider first the time evolution of a taut string which has just been struck or plucked.
Neglecting damping, the deflection of the string, $f(x,t)$, obeys the linear
partial differential equation (PDE) known as the {\bf 1D wave equation}:
\begin{subequations} \label{wire}
\begin{equation}
\label{wirePDE}
\frac{\partial^2 f}{\partial t^2} = c^2 \frac{\partial^2 f}{\partial x^2},
\end{equation}
where $c$ is the speed of wave propogation in the string, subject to
\begin{align}
\textrm{boundary\ conditions:}\quad &\begin{cases} \ f=0 & \textrm{at}\ x=0\ \ \textrm{and}\ \ x=L, \\
				  \end{cases}  \label{wire_bcs}\\
\textrm{and\ initial\ conditions:}\quad   &\begin{cases}\   f=a(x)\ \ \textrm{and}\ \ \displaystyle \Dpartial{\!f}{t}=b(x)   & \textrm{at}\ t=0.
			      \end{cases}  \label{wire_ics}
\end{align}
\end{subequations}

\noindent For a gently plucked guitar string, $b(x)\approx 0$ and $a(x)$
describes the initial displacement of the string.  For an
impulsively struck piano string, $a(x)\approx 0$ and $b(x)$ describes
the initial velocity of the string.

We will solve this system using the {\bf separation of variables}
({\bf SOV}) approach.  With this approach, an assumed {\bf separable} (that is, decoupled) form is imposed on the individual {\bf modes} of the
solution (this is often called the {\bf SOV Ansatz}).  We then attempt to construct a general solution to the full
system \eqref{wire} built from such modes.  We thus first seek modes, $f^{\,m}$, which satisfy the PDE
\eqref{wirePDE} {\it and} the boundary conditions \eqref{wire_bcs} of the separable form
\begin{equation}
\label{Ansatz}
f^{\,m}(x,t) = X(x)\, T(t).
\end{equation}
[Note that  $X=X(x)$ and $T=T(t)$ in this section are simply scalar functions of
their arguments, not matrices.]  If we can find enough nontrivial
(that is, nonzero) solutions of \eqref{wirePDE} which fit this form, we will be
able to reconstruct a solution which also satisfies the initial
conditions \eqref{wire_ics} as a superposition of these modes.
Inserting \eqref{Ansatz} into \eqref{wirePDE} and assuming $c=$constant, we find that
\begin{equation*}
X T''  = c^2 X'' T \hskip0.2in \Rightarrow \hskip0.2in
\frac{T''}{T} = c^2 \frac{X''}{X} \triangleq -\omega^2  \hskip0.2in \Rightarrow \hskip0.2in
{\displaystyle T'' = - \omega^2\,T, \atop
 \displaystyle X'' = - {k_x}^2 X, \strut }
 \quad \textrm{where} \quad \omega = c {k_x}
\end{equation*}
and where the constant $\omega$ (and, thus, $k_x$) must be independent of both $x$ and
$t$ due to the second equation combined with the facts that
$X=X(x)$ and $T=T(t)$.  The two systems at right are solved with:
\begin{subequations}
\begin{alignat}{2}
T&=A \,\cos (\omega\, t)  &&+ B \,\sin (\omega\, t), \label{T_i} \\
X&=C \,\cos ({k_x} x )    &&+ D \,\sin ({k_x} x ).  \label{X_i}
\end{alignat}
\end{subequations}
Due to the BC at $x=0$, it follows that $C=0$.  Due to the BC at $x=L$, it follows for most $k_x$ that $D=0$ as well, and thus $f^{\,m}(x,t)=0$ for all
$\{x,t\}$.  However, for certain values of $k_x$ (specifically, for $k_{x_{\,i}} L = i \pi$ for integer values of $i$), $X$ satisfies the
homogeneous boundary condition at $x=L$ even for nonzero values of
$D$.  These special values of $k_{x_{\,i}}$ are the {\bf eigenvalues of the PDE system} in SOV form\footnote{The \label{foot:Sturm-Liouville} problem
of determining the admissible values $k_{x_{\,i}}$, and corresponding functions $X_i(x)$, such that each nontrivial function $X_i(x)$ satisfies both
the ODE $X'' = - {k_x}^2 X$ and homogeneous BCs
at both $x=0$ and $x=L$ is a special case of the general problem of determining the {\bf eigenvalues} $\lambda_i$,
and corresponding {\bf eigenfunctions} $u_i(x)$, of the homogeneous {\bf Sturm-Liouville eigenvalue problem}
\begin{equation*}
  \Big[\Dnorm{}{x} p(x) \Dnorm{}{x} + q(x) + \lambda r(x) \Big] u(x) = 0 \quad \textrm{with} \quad
  \left\{ \begin{aligned} c_1 u(a) + c_2 u'(a) = 0, \\ c_3 u(b) + c_4 u'(b) = 0, \end{aligned}
  \right.
\end{equation*}
where $p(x)>0$, $p'(x)$, $q(x)$, and $r(x)>0$ are continuous on $x\in (a,b)$, and $\{c_1,c_2,c_3,c_4\}$ are constants.
Another example of a problem of this class, related to {\bf Bessel functions}, is given in \eqref{eq.11sl}.}.
Defining (for convenience) the combined constants $\hat a^s = A D$ and $\hat b^s = \omega B D$ and forming a {\bf linear superposition} of the nontrivial modes $f^{\,m}_i$
[each of which satisfying the SOV Ansatz \eqref{Ansatz} for each value of $k_{x_{\,i}}$ and
corresponding value of $\omega_i$] in an attempt to additionally satisfy \eqref{wire_ics}, we now write
\begin{equation}
f=\sum_{i=1}^{\infty} f^{\,m}_i
 =\sum_{i=1}^{\infty} \left[\hat a_i^s \cos(\omega_i t) +
				\frac{\hat b_i^s}{\omega_i} \sin(\omega_i t) \right]
				\sin (k_{x_{\,i}} x).
\label{linearcomb}
\end{equation}
The coefficients $\hat a_i^s$ and $\hat b_i^s$ are now determined by enforcing the initial conditions:
\begin{subequations}
\begin{equation}
			    f(x,t=0)=a(x)
=\sum_{i=1}^{\infty} \hat a_i^s \sin (k_{x_{\,i}} x),
\qquad
\frac{\partial f}{\partial t}(x,t=0)=b(x)
=\sum_{i=1}^{\infty} \hat b_i^s \sin (k_{x_{\,i}} x).
\label{coefSOVPDE}
\end{equation}
Noting the {\bf orthogonality of the sine and cosine functions}\footnote{This \label{othogfootnote} orthogonality principle states that,
for $i$, $k$ integers, 
\quad $\displaystyle\int_0^L \sin\left(\frac{i \pi x}{L}\right) \cos\left(\frac{k \pi x}{L}\right) dx = 0$,
\begin{equation*}
\int_0^L \sin\left(\frac{i \pi x}{L}\right) \sin\left(\frac{k \pi x}{L}\right) dx = \begin{cases} L/2 & i=k\ne 0\\ 0 & \textrm{otherwise,}\end{cases}
\qquad \textrm{and} \qquad 
\int_0^L \cos\left(\frac{i \pi x}{L}\right) \cos\left(\frac{k \pi x}{L}\right) dx = \begin{cases} L   & i=k= 0\\ L/2 & i=k\ne 0\\ 0 & \textrm{otherwise.}\end{cases}
\end{equation*}}, we multiply both of the above equations by $\sin(k_{x_{p}} x)=\sin(p \pi x / L)$ and
integrate over the domain $x\in [0,L]$, which results in:
\begin{equation}
\left.
\begin{alignedat}{2}
\int_0^L a(x) \sin(k_{x_{p}} x) dx &= \hat a^s_p\,\frac{L}{2}
\hskip0.2in &&\Rightarrow \hskip0.2in
\hat a^s_p = \frac{2}{L} \int_0^L a(x) \sin(k_{x_{p}} x) dx \\
\int_0^L b(x) \sin(k_{x_{p}} x) dx &= \hat b^s_p\,\frac{L}{2}
\hskip0.2in &&\Rightarrow \hskip0.2in
\hat b^s_p = \frac{2}{L} \int_0^L b(x) \sin(k_{x_{p}} x) dx
\end{alignedat}
\right\} \hskip0.2in \textrm{for\ } p=1, 2, 3, \ldots
\label{chatdhat}
\end{equation}
\end{subequations}
Thus, the $\hat a^s_p$ and $\hat b^s_p$ may be calculated
directly.  As discussed further in \S \ref{sec.G.B}, these representations are
referred to as the {\bf infinite sine transforms} of $a(x)$ and $b(x)$ on
the interval $x\in[0,L]$.

An analytic solution of the PDE \eqref{wirePDE} satisfying both the
boundary conditions \eqref{wire_bcs} and the initial conditions
\eqref{wire_ics} may thus be constructed as a linear combination of
separable modes, as shown in \eqref{linearcomb}, the coefficients of which may easily
be determined, as shown in \eqref{chatdhat}.

Now consider the PDE\footnote{Note: this is {\it not} the PDE governing waves in a nonuniform bar;
treatment of this physical problem is considered in Exercise \ref{ex.nonuniformbar}.}
in \eqref{wirePDE} in the case in which $c$ is a function of $x$.  We can
no longer represent the mode shapes in $x$ analytically with sines and
cosines.  In this case, we can still seek decoupled modes of the form
$f = X(x) T(t)$, but we now must determine the $X(x)$ numerically.
The differential equation for $X(x)$ is
\begin{subeqnA}
\begin{alignat}{2}
X'' &= -\frac{\omega^2}{c^2} X  \qquad &&\textrm{for}\ x\in(0,L), \ \ \textrm{with} \label{odeX} \\
X&=0 && \textrm{at\ } x=0 \textrm{\ \ and\ \ } x=L. \label{bcX} 
\end{alignat}
\end{subeqnA}
Consider now the values of $X$ only at $N+1$ discrete locations (a.k.a.~{\bf gridpoints}) located at $x=x_{j}=j\,\Delta x$ for $j=0\ldots N$, where $\Delta
x=L/N$.  Note that, at these gridpoints, derivatives may be approximated as follows:
\begin{equation*}
\left. \frac{\partial X}{\partial x}\right|_{x_j} \approx
\frac{ X_{j+1} -  X_{j-1}}{2\,\Delta x}, \qquad
\left. \frac{\partial^2 X}{\partial x^2}\right|_{x_j} \approx
\left( \frac{ X_{j+1} - X_{j}}{\Delta x} - \frac{ X_{j} - X_{j-1}}{\Delta x} \right) / \Delta x =
\frac{ X_{j+1} - 2 X_{j} + X_{j-1}}{(\Delta x)^2},
\end{equation*}
where we denote $X_j\triangleq X(x_j)$.  By the boundary conditions \eqref{bcX}, $X_0=X_N=0$.  The
differential equation \eqref{odeX} at each of the $N-1$ grid points on the interior
may be approximated by the relation
\begin{equation*}
c^2_j \,\frac{ X_{j+1} - 2 X_{j} + X_{j-1}}{(\Delta x)^2} = - \omega^2 X_{j}.
\end{equation*}
Thus, the differential equation \eqref{odeX} and boundary conditions \eqref{bcX}
may be assembled in matrix form as
\begin{equation*}
\frac{1}{(\Delta x)^2}
\begin{pmatrix}
-2c^2_1 &  c^2_1  &                &                  &         0        \\
 c^2_2  & -2c^2_2 &   c^2_2   &                  &                  \\
	     & \ddots       &  \ddots        &    \ddots        &                  \\
	     &              & c^2_{N-2} & -2c^2_{N-2} & c^2_{N-2}   \\
     0       &              &                &  c^2_{N-1}  & -2c^2_{N-1}
\end{pmatrix}
\begin{pmatrix} X_1 \\ X_2 \\ \vdots \\ X_{N-2} \\ X_{N-1} \end{pmatrix}
=  \Big[ - \omega^2  \Big]
\begin{pmatrix} X_1 \\ X_2 \\ \vdots \\ X_{N-2} \\ X_{N-1} \end{pmatrix},
\end{equation*}
or, more simply, as
\begin{equation}
A \s = \lambda \s,
\label{evalproblemSOV}
\end{equation}
where $\lambda \triangleq -\omega^2$.  This is exactly the matrix
eigenvalue problem discussed at the beginning of this section, and can
be solved numerically for the eigenvalues $\lambda_i$ and the
corresponding mode shapes $\s_i$, as illustrated (for the case with constant $c$) in the
code {\tt Wire.m}, given in Algorithm \ref{alg.3.1}.  Note that, for
constant $c$ and a sufficiently large number of gridpoints, the
first several eigenvalues returned by {\tt Wire.m} closely match the
analytic solution $\omega_i=i \pi c / L$, and the first
several eigenvectors $\s^i$ are of the same shape as the analytic
mode shapes $X_{i}(x)=\sin(\omega_i x / c)$.  Note that this
numerical code is easily modified to handle the situation in which
$c$ varies with $x$, though this case cannot be solved
analytically.
\clearpage

Thus, the eigenvalues and eigenvectors that satisfy
\eqref{evalproblemSOV} may be used to approximate the $\omega_{i}$ and
$X_{i}(x)$ of the SOV modes satisfying the PDE \eqref{wirePDE} and the
boundary conditions \eqref{wire_bcs} even in the case that the
eigenvalues of the PDE system cannot be determined analytically.  The
equation for $T_{i}(t)$ is the same as before; its solution is
given by \eqref{T_i}.  Forming a superposition of the nontrivial modes
$\f^{\,i}$ in this spatially discretized setting in an attempt to
additionally satisfy the initial conditions \eqref{wire_ics}, we
express [cf.  \eqref{linearcomb}]
\begin{equation}
    \f=\sum_{i=1}^{N} \f^{\,i}
     =\sum_{i=1}^{N} \left[\hat a^s_i \cos(\omega_i t) +
				    \frac{\hat b^s_i}{\omega_i} \sin(\omega_i t) \right] \s^{i}.
    \label{linearcombdisc}
\end{equation}
Enforcing the initial conditions at the grid points, denoting
$a_i=a(x_{i})$ and $b_i=b(x_i)$, we have [cf. \eqref{coefSOVPDE}]
\begin{subequations}\label{chatdhatdisc}
\begin{equation}
  \f(t=0)=\a=S\hat\a^s,
\quad
\frac{d \f}{d t}(t=0)=\b=S\hat\b^s
\quad \textrm{where} \quad
S=\begin{bmatrix} | & | &  & | \\ \s^1 & \s^2 & \ldots  & \s^N \\ 
                    | & | &  & | \end{bmatrix}
\label{chatdhatdisc;a}
\end{equation}
Assuming $S$ is invertible, we may calculate the appropriate
values of $\hat\a^s$ and $\hat\b^s$ directly [cf. \eqref{chatdhat}]
\begin{equation}
  \hat\a^s=S^{-1}\a,
\hskip0.3in
  \hat\b^s=S^{-1}\b.
  \label{chatdhatdisc;b}
\end{equation}
\end{subequations}
As discussed further in \S \ref{sec.G.Ba},
$\hat\a^s$ and $\hat\b^s$ are referred to as the {\bf discrete sine transforms}\footnote{As $S$ and $S^{-1}$ are full, it appears at first glance that the transformations
from $\a$ to $\hat\a^s$ and from $\hat\a^s$ back to $\a$ in \eqref{chatdhatdisc} involve full matrix/vector multiplication, and thus
each requires $\sim 2N^2$ real flops if $\a$ is real.  We will see in
\S \ref{sec.G.B} that we can in fact explot the several symmetries within $S$ via a remarkable algorithm called the {\bf fast fourier transform} ({\bf FFT}; see \S \ref{sec.G.D}) to solve
this problem in only $\sim 5M \log_2 M$ real flops, where $M=N/2$.  If $N=256$, the latter algorithm is almost 30 times cheaper than the former. (!)} of $\a$ and $\b$.

Thus, a numerical approximation of the solution of the PDE \eqref{wirePDE} satisfying both the
boundary conditions \eqref{wire_bcs} and the initial conditions
\eqref{wire_ics} may be constructed as a linear combination of
separable modes, as shown in \eqref{linearcombdisc}, the coefficients
of which may easily be determined, as shown in \eqref{chatdhatdisc}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\lstinputlistingplain[frame=single,label=alg.3.1,caption=Matlab codes to compute and animate the leading natural modes of vibration of a string.]{RCC1.0/chap04/Wire.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/WireAnimate.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The eigenvalue problem formulated above is implemented in Algorithm \ref{alg.3.1}.
Note that the script {\tt Wire.m} calls the auxiliary function {\tt WireAnimate.m};
{\tt WireAnimate.m} is isolated as a function because it is ancillary to the purpose of the main function {\tt Wire.m},
which one might want to digest without sifting through (or, in Fortran or C, compiling) the clumsy animation commands.

Note the important distinction between a {\bf Matlab script} (like {\tt Wire.m}), which inherits the {\bf global
workspace} (the currently defined set of variables) of Matlab, and a
{\bf Matlab function} (like {\tt WireAnimate.m}), which may only reference variables which are explicitly
passed in and only returns variables that are explicitly passed out.
Functions are much more easily debugged and extended to other problems than scripts, as functions make clear via their argument list what information from
the calling function is used in, and returned by, the called function\footnote{That is, in addition to those variables defined as global,
as seen in Algorithm \ref{alg.mg2d.matlab} and discussed in the last footnote on page \pageref{sec.C.C.comparison}.}.
In complicated codes, unintentionally assigning a minor variables (like the index {\tt i}) with different meanings in different scripts that call each other
can lead to a bug that is very difficult to find, and thus should be avoided.
The proper use of functions, and the associated passing of only the relevant data back and forth (known as {\bf handshaking}), prevents such bugs from appearing.

\enlargethispage{10pt}

There is extra overhead associated with function calls, because they often
{\bf allocate} (that is, assign) new memory locations for variables passed in when called, then {\bf deallocate} (that is, release) this memory
upon exit.  This behavior is referred to as {\bf passing by value}.  For small variables, the overhead associated with passing by value is minimal.
For large arrays, however such overhead can be substantial.  To avoid this overhead, one may either use arrays defined globally,
as done in Algorithms \ref{alg.mg2d.matlab}-\ref{alg.mg2d.c}, or {\bf pass by reference}, which means to pass a \href{http://www.cs.stanford.edu/cslibrary/PointerFunCBig.avi}{\bf pointer}
to the original memory location of the array rather than copy the values in the array into a new memory location.
This is the default for array passing in Fortran and is common in C.  Matlab's default until the 2007a release was essentially a pass-by-value approach (with memory allocation deferred
to the first time the array is modified within the subprogram).  With the 2007a release of Matlab, a pass-by-reference capability was finally introduced:
when a function calls another function with an identical argument in both the input and output lists (in both the calling function and the called function),
a proper pass by reference is now initiated in Matlab, which leads to dramatic speed improvements for large problems.

\subsection{RC_Eigenmode analysis of surface waves in a shallow rectangular pool}\label{sec.A.C.B.2d}
\enlargethispage{10pt}

The analysis considered in \S \ref{sec.A.C.B} extends directly to multidimensional systems.  To illustrate, consider
the evolution of a low-amplitude wave in a shallow pool which is
$L_x=5$m wide $\times$ $L_y=10$m long $\times$ $b=1$m deep.
Neglecting surface tension and viscosity, the height of the water, $h(x,y,t)$, obeys the {\bf 2D wave equation}:
\begin{subequations} \label{4wave}
\begin{equation}
\label{4wavePDE}
\frac{\partial^2 h}{\partial t^2} = c^2 \Big(\frac{\partial^2 h}{\partial x^2} + \frac{\partial^2 h}{\partial y^2} \Big)
\end{equation}
where $g=9.8\textrm{m/s}^2$ and $c=\sqrt{b g}$ is the speed of wave propogation in the system, subject to
\begin{align}
\textrm{boundary\ conditions:}\quad &\begin{cases} \ \displaystyle \Dpartial{h}{x}=0 & \textrm{at}\ x=0\ \ \textrm{and}\ \  x=L_x \\[0.1in]
						  \ \displaystyle \Dpartial{h}{y}=0 & \textrm{at}\ y=0\ \ \textrm{and}\ \  y=L_y,
			      \end{cases}  \label{4wave_bcs}\\
\textrm{and\ initial\ conditions:}\quad   &\begin{cases}\   h=a(x,y)  \ \ \textrm{and}\ \  \displaystyle \Dpartial{h}{t}=b(x,y)   & \textrm{at}\ t=0.
			      \end{cases}  \label{4wave_ics}
\end{align}
\end{subequations}
\clearpage

\noindent [The waves on a rectangular membrane may be analyzed in an analogous manner, simply by replacing
the {\bf homogeneous Neumann} BCs given above with {\bf homogeneous Dirichlet} BCs (that is, $h=0$) on the boundaries, and by retaining the sine expansions
in $x$ and $y$ below instead of the cosine expansions.]

Inspired by \eqref{Ansatz}, we seek the {eigenmodes} of the solution, $f^{\,m}$, which separate into the form
\begin{equation}
f^{\,m}(x,y,t) =  X(x)\, Y(y)\, T(t).  \label{wave_Ansatz}
\end{equation}
Following the analysis given previously, we may write 
\begin{equation*}
\frac{T''}{T} = c^2 \Big(\frac{X''}{X} + \frac{Y''}{Y}\Big) \triangleq -\omega^2,\qquad 
  \frac{X''}{X} = \frac{1}{c^2} \frac{T''}{T} - \frac{Y''}{Y} \triangleq -k_x^2, \qquad
  \frac{Y''}{Y} = \frac{1}{c^2} \frac{T''}{T} - \frac{X''}{X} \triangleq -k_y^2, 
\end{equation*}
from which it follows that the constants $\omega$, ${k_x}$, and ${k_y}$, are independent of $x$, $y$, and $t$, and are related such that
$\omega^2=c^2(k_x^2+k_y^2)$.  It also follows that
\begin{equation*}
T(t) = A \cos(\omega\, t) + B \sin(\omega\, t), \quad 
X(x) = C \cos({k_x}  x) + D \sin({k_x} x), \quad 
Y(y) = E \cos({k_y}  y) + F \sin({k_y} y).
\end{equation*}
Applying the BCs at $x=0$ and $y=0$ for all $t$ implies that $D=0$ and $F=0$; applying the BCs at $x=L_x$ and $y=L_y$ for all $t$ implies that, for most values of $k_x$ and $k_y$,
the coefficients $C$ and $E$ must equal zero as well, and thus the mode $f^{\,m}$ is trivial (that is, zero).  However, for certain values of $k_x$ and $k_y$
(specifically, for $k_{x_{\,i}} L_x = i \pi$ for $i=0,1,2,\ldots$ and $k_{y_{\!j}} L_y = j \pi$ for $j=0,1,2,\ldots$), $X(x)$ and $Y(y)$ satisfy the
homogeneous Neumann BCs at $x=L_x$ and $y=L_y$ even for nonzero values of $C$ and $E$.
Defining $\hat a^c = A C E$ and $\hat b^c = \omega B C E$ and assembling a superposition of these nontrivial modes,
defining $\omega_{ij}=c \sqrt{k_{x_{\,i}}^2 + k_{y_{\!j}}^2}$, we may write
\begin{subequations}
\begin{equation}
f=\sum_{i=0}^{\infty} \sum_{j=0}^{\infty}  f^{\,m}_{ij}
 =\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \left[\hat a_{ij}^c \cos(\omega_{ij} t) +
				\frac{\hat b_{ij}^c}{\omega_{ij}} \sin(\omega_{ij} t) \right]
				\cos(k_{x_{\,i}} x) \,\cos(k_{y_{\!j}} y),
\label{wavelinearcomb}
\end{equation}
where the coefficients $\hat a_{ij}^c$ and $\hat b_{ij}^c$ are determined by enforcing the initial conditions:
\begin{equation*}
f(x,y,t=0)=a(x)=\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \hat a_{ij}^c \cos(k_{x_{\,i}} x) \,\cos(k_{y_{\!j}} y),
\quad
\frac{\partial f}{\partial t}(x,y,t=0)=b(x)
=\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \hat b_{ij}^c \cos(k_{x_{\,i}} x) \,\cos(k_{y_{\!j}} y).
\end{equation*}
Noting the orthogonality of the sine and cosine functions mentioned previously results in 
\begin{equation}
\left.
\begin{aligned}
\hat a^c_{pq} = \frac{4}{L_x L_y} \int_0^{L_x}\int_0^{L_y} a(x,y) \cos(k_{x_p} x) \cos(k_{y_q} x) \,dx\,dy \\
\hat b^c_{pq} = \frac{4}{L_x L_y} \int_0^{L_x}\int_0^{L_y} b(x,y) \cos(k_{x_p} x) \cos(k_{y_q} x) \,dx\,dy
\end{aligned}
\right\} \hskip0.2in \textrm{for\ } p=0, 1, 2, \ldots \ \ \textrm{and}\ \  q=0, 1, 2, \ldots
\label{wavechatdhat}
\end{equation}
\end{subequations}
An analytic solution of the PDE \eqref{4wavePDE} satisfying both the
boundary conditions \eqref{4wave_bcs} and the initial conditions
\eqref{4wave_ics} may thus be constructed as a linear combination of
separable modes, as shown in \eqref{wavelinearcomb}, the coefficients of which may easily
be determined, as shown in \eqref{wavechatdhat}.

The problem of the simulation of wave evolution in a shallow pool is considered much further in \S \ref{sec:PDEhyp}, where we extend
our analysis to handle both variable depth of the pool and finite amplitude of the waves (and the associated nonlinear interactions).
In the limiting case that the depth is constant and the waves are small in amplitude, the analytic solution given above may be used to quantify
the accuracy of this more involved numerical simulation.

\clearpage
\section{Fundamental matrix decompositions}\label{sec.A.D}

This section presents a variety of useful ways to {\bf decompose} or
{\bf factor} a given matrix $A$ as the product of other matrices with
special properties.  The resulting {\bf decompositions} (a.k.a.~{\bf
factorizations}) form the foundation for many powerful and efficient
numerical tools that are useful for a variety of tasks, including
\beginmylistb
\item solving $A\x=\b$ in the case of square, nonsingular $A$,
\item solving $A\x=\b$ in the inconsistent and/or underdetermined case,
\item computing eigenvalues and eigenvectors,
\item understanding linear dynamic systems, and
\item solving linear feedback control problems.
\endmylist
Five of the decompositions we will consider,
\beginmylistb
\item the {\bf RC_Hessenberg decomposition} $A=V T_{0} V^{H}$ where $V$ is unitary and $T_0$ is upper RC_Hessenberg (\S \ref{sec.A.D.A}),
\item the {\bf RC_Schur decomposition} $A=UTU^{H}$ where $U$ is unitary and $T$ is upper triangular (\S \ref{sec.A.D.C}),
\item the {\bf real RC_Schur decomposition} $A=U{\hat T}U^{T}$, for real $A$, where $U$ is real and orthogonal and ${\hat T}$ is real and
{\it block} upper triangular with $1\times 1$ and $2\times 2$ blocks on the main diagonal (\S \ref{sec.A.D.Ca}),
\item the {\bf eigen decomposition} $A=S\Lambda S^{-1}$ where $\Lambda$ is diagonal (\S \ref{sec.A.D.D}), and
\item the {\bf Jordan decomposition} $A=MJM^{-1}$ where $J$ is in so-called Jordan form (\S \ref{sec.A.D.H}),
\endmylist
are referred to as {\bf similarity transformations}.  Such
decompositions play a particularly important role in the analysis of a
variety of problems in numerical linear algebra because of the
following two facts.\medskip

\begin{fact} \label{fact.A.D.A}
If $A=CBC^{-1}$ for some $C$ (that is, if $A$
and $B$ are {\bf similar}), then $A$ and $B$ have the same
eigenvalues, and if $\s$ is an eigenvector of $A$, then $C^{-1}\s$ is
an eigenvector of $B$.
\end{fact}

\noindent {\it Proof}\/: Follows immediately from:
$\quad A\s=\lambda\s \quad \Rightarrow \quad CBC^{-1}\s=\lambda\s \quad \Rightarrow \quad B(C^{-1}\s)=\lambda(C^{-1}\s)$. \endproof

\begin{fact} \label{fact.A.D.B}
If $A=CBC^{H}$ for some unitary $C$ (that is,
if $A$ and $B$ are {\bf unitarily similar} or {\bf congruent}) and $A$ is Hermitian (or
symmetric), then $B$ is also Hermitian (or symmetric).
\end{fact}

\noindent {\it Proof}\/: Follows immediately from:
$B=C^{H}A C=C^{H}A^{H} C=(C^{H}A C)^{H}=B^{H}$. \endproof \medskip

{\it The subsections that follow are intended to be read sequentially}: the RC_Hessenberg
decomposition forms a valuable preparatory step for applying the QR
method to compute the useful RC_Schur decomposition, from which the immensely useful
eigen decomposition may readily be determined (if it exists).  The Jordan decomposition
represents the matrix that is, in a sense, as close as you can get to a diagonalization of $A$
via a similarity transformation when an eigen decomposition does not exist; unfortunately, as we will
show, it is numerically ill-behaved.  Rather, the final decomposition we introduce, 
\beginmylistb
\item the {\bf singular value decomposition} $A=U \Sigma V^H$ where $U$ and $V$ are unitary and $\Sigma$ is diagonal (\S \ref{sec.A.D.G}),
\endmylist
though it is {\it not} a similarity transformations (as, in general, $U\ne V$), is the most useful
generalization of the eigen decomposition that may be used even
if an eigen decomposition of $A$ does not exist or, indeed, even if $A$ is nonsquare;
it is shown in \S \ref{sec.A.E} that the singular value decomposition leads directly
to an efficient construction of the Moore-Penrose pseudo"inverse $A^+$, as introduced in Figure \ref{cartoon3}.
We also review the decompositions related to RC_Gaussian elimination, as introduced in \S \ref{chap02}, as they may
be understood a bit more deeply once the properties of positive definite and positive semidefinite
matrices are established (as done in \S \ref{sec.posdef}).  

Recalling the caution at the beginning of NRchap\ref{chap04} related to its difficulty and importance, it is no exaggeration to say that
{\it \S \ref{sec.A.D} is by far the most difficult and important section of NRchap\ref{chap04}.}  Note that the definitive references on this material
are Wilkenson (1965) and Golub \& Van Loan (1996), which significantly extend the present discussion.

\clearpage

\subsection{The RC_Hessenberg decomposition}\label{sec.A.D.A}

Every $n\times n$ matrix $A$ has a {\bf RC_Hessenberg decomposition} $A=V
T_0 V^{H}$, where $V$ is unitary and $T_0$ is upper RC_Hessenberg.  The
RC_Hessenberg decomposition, which is a unitary similarity transformation (see Fact \ref{fact.A.D.B}), is useful primarily as a preparatory
step in the computation of the RC_Schur and eigen decompositions of
a square matrix $A$.  Recall from \S \ref{sec.A.A.K} that a
RC_Hessenberg form $T_0$ is an upper triangular matrix with extra
nonzero elements populating its first subdiagonal.  The RC_Hessenberg
decomposition is useful because it introduces many zero elements while
preserving the eigenvalues (Fact \ref{fact.A.D.A}) and, if it exists,
the Hermitian structure of the matrix $A$ (Fact \ref{fact.A.D.B}).
Thus, if $A$ happens to be Hermitian, then $T_0$ is both RC_Hessenberg
and Hermitian---that is, $T_0$ is {\it tridiagonal\/}!  Further, the
RC_Hessenberg decomposition can be completed with a finite
sequence of calculations, as described below.  Note that, by
presenting this construction, we also effectively establish that the
RC_Hessenberg decomposition itself exists.  As the RC_Hessenberg
decomposition reduces a general square matrix $A$ to RC_Hessenberg form while preserving its eigenvalues, and it reduces a Hermitian
matrix $A$ to a tridiagonal form, this decomposition makes the problem of determining eigenvalues significantly easier.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingT[frame=single,label=alg.3.RC_Hessenberg,caption=Computation of the RC_Hessenberg decomposition.]{RCC1.0/chap04/RC_Hessenberg}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The algorithm to construct a RC_Hessenberg decomposition of an $n\times
n$ matrix $A$ is comprised of $n-2$ steps, each of which is based
on an appropriately configured Householder reflector matrix (as
described in \S \ref{sec.A.A.M}), embedded within an appropriately-sized identity matrix, to introduce zeros into the
transformed matrix.  Specifically, at the first step, define
\begin{align*}
    &\x^{1}=\begin{pmatrix} a_{21} \\ a_{31} \\ \vdots \\ a_{n1} \end{pmatrix}, \quad
    \textrm{$\{\sigma_{1},\w^{1},\nu_{1}\}$ according to \eqref{sigmawdefb} and $H(\sigma_{1},\w^{1})$
    according to \eqref{Householder},}\\
    &V_{1}=\begin{bmatrix} 1 & 0 \\ 0 & H(\sigma_{1},\w^{1})_{(n-1)\times (n-1)} \end{bmatrix}
    \quad \Rightarrow \quad
    V_{1}^{H}A V_{1}=\begin{bmatrix} a_{11} & * \\ H^{H}(\sigma_{1},\w^{1})\x^{1} & * \end{bmatrix} =
    \begin{pmatrix} a_{11}     & * & * & * & \hdots \\
				      -\nu_{1} & * & * & * & \hdots \\
				      0          & * & * & * & \hdots \\
				      0          & * & * & * & \hdots \\
				      \vdots     & \vdots & \vdots & \vdots & \ddots \end{pmatrix}.
\end{align*}
Thus, via a unitary similarity transformation, we have reduced the first
column of $A$ to upper RC_Hessenberg form.  Referring to the elements of the
transformed matrix as $a_{ij}$ (that is, performing this
transformation {\it in place} in the computer memory), we proceed by
working next on the second column: define
\clearpage
\begin{align*}
    &\x^{2}=\begin{pmatrix} a_{32} \\ a_{42} \\ \vdots \\ a_{n2} \end{pmatrix}, \quad
    \textrm{$\{\sigma_{2},\w^{2},\nu_{2}\}$ according to \eqref{sigmawdefb} and $H(\sigma_{2},\w^{2})$
    according to \eqref{Householder},}\\
    &V_{2}=\begin{bmatrix} I_{2\times 2} & 0 \\ 0 & H(\sigma_{2},\w^{2})_{(n-2)\times (n-2)} \end{bmatrix}
    \quad \Rightarrow \quad
    V_{2}^{H}(V_{1}^{H}A V_{1})V_{2}=\begin{pmatrix} a_{11} & a_{12} & * & * & \hdots \\
				      -\nu_{1}   & a_{22} & * & * & \hdots \\
				      0          & -\nu_{2} & * & * & \hdots \\
				      0          & 0 & * & * & \hdots \\
				      \vdots     & \vdots & \vdots & \vdots & \ddots \end{pmatrix}.
\end{align*}
We continue this way for $n-2$ steps, after which the transformed
matrix, which we denote by $T_0$, is in RC_Hessenberg form.  The
sequence of operations that are performed in the process may be
summarized in matrix notation as
\begin{equation*}
    T_0=V^{H} A V \quad \Leftrightarrow \quad A=V T_0 V^{H}, \quad
    \textrm{where} \quad V=V_{1}V_{2}\hdots V_{n-2}.
\end{equation*}
In the algorithm above, we never actually even need to compute the
matrices $V_{i}$.  In order to compute both $T_0$ and $V$, it is
sufficient to compute the $\sigma_{k}$ and $\w^{k}$, noting
\eqref{householdercheap}.

Significantly, {\it this is the best we can do in terms of introducing zeros into $A$ exactly}\footnote{That is, assuming infinite-precision
arithmetic.} {\it via a similarity transformation using a finite sequence of simple reflections and rotations}.  The difficulty that prevents
us from taking this approach any further is that any transformation matrix, such as the Householder, Givens, and fast Givens matrices introduced in
\S \ref{chap01}, must be applied from both the left {\it and} the right when building a similarity transformation.  Thus, if two or more rows are combined
via premultiplication by some convenient transformation matrix, then the corresponding columns must immediately also be combined in the subsequent
postmultiplication by the 
conjugate transpose of that transformation matrix.  Therefore, if we attempt, e.g., to apply a series of premultiplications by Givens rotation matricies in
order to zero out the first subdiagonal of a RC_Hessenberg matrix $T_0$ in order to reduce it to a triangular form via a similarity transformation, the required
postmultiplications by the conjugate transpose of these rotation matrices immediately destroys certain subdiagonal zeros which we have worked so hard
to create.  This is true no matter how much rotation we apply at each step or what sequence these steps are applied (try it!).  Thus,

\begin{fact} \label{fact.abel1}
In general, the RC_Schur and eigen decompositions must be computed iteratively.
\end{fact}

\noindent This statement, which makes concrete the fundamental difficulty of the problem of determining eigenvalues,
is in fact quite deep, and follows as a direct consequence of the following classical result: 

\begin{fact}[The Abel-Ruffini Theorem] \label{fact.abel2} 
A polynomial equation of order higher than four is incapable of general algebraic solution by radicals
(that is, in terms of a finite number of additions, subtractions, multiplications, divisions, and root extractions). 
\end{fact}

\noindent Note, of course, that there are some polynomial equations of order higher than four that in fact are solvable
by radicals ($x^5-x^4-x+1=0$ is an example, with roots $\{1,1,-1,\imath,-\imath\}$).
However, there are other polynomial equations of order higher than four that are {\it not} solvable by radicals ($x^5-x+1=0$ is an example).
For a description of Abel's proof of Fact \ref{fact.abel2}, the reader is referred to Pesic (2004).
The criterion that ultimately distinguishes between those polynomial equations that can be solved by radicals and those that cannot is made precise
by {\bf Galois theory} (see, e.g., Postnikov 2004).

The connection between the Abel-Ruffini Theorem and Fact \ref{fact.abel1} may be seen immediately by, for example, considering the
eigenvalues of the following matrix in top companion form (see \S \ref{sec.blockmatrices}):
\begin{equation*}
   A=\begin{pmatrix} -a_{n-1} &\ldots &  -a_{1} &  -a_0 \\
                     1        &       & 0         & 0 \\ 
                              &\ddots &           & \vdots \\ 
                     0        &       & 1         & 0 \end{pmatrix}, \quad
  |\lambda I - A|=0 \quad \Leftrightarrow \quad p(\lambda) = \lambda^n + a_{n-1} \lambda^{n-1} + \ldots + a_1 \lambda + a_0 = 0.
\end{equation*}
It follows that any polynomial equation $p(\lambda) = 0$ is the characteristic equation of a matrix $A$ in companion form,
thus allowing one to convert the problem of finding the roots of a polynomial into the problem of computing the eigenvalues of $A$,
as illustrated in Algorithm \ref{alg.RC_Roots}.
This relation also establishes that, if there are some polynomial equations that are not solvable
via a finite number of additions, subtractions, multiplications, divisions, and root extractions,
then there are some eigenvalue problems that are not solvable via a finite sequence of such operations as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingT[frame=single,label=alg.RC_Roots,caption=Code to convert the polynomial root finding problem into an equivalent eigenvalue problem.]{RCC1.0/chap04/RC_Roots}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the following section, we thus step back from eigenvalue-preserving similarity transformations for a moment,
and investigate how to triangularize a matrix when (only) premultiplying by a finite sequence of unitary transformation matrices,
which we assemble into a unitary matrix $Q$, thereby constructing the decomposition $A=QR$.  It is then shown how the
resulting $QR$ decomposition algorithms may be used as the core of efficient {\it iterative} algorithms for determining
the RC_Schur and eigen decompositions.

\subsection{The $QR$ decomposition}\label{sec.A.D.B}\enlargethispage{3pt}

Every $m\times n$ matrix $A$ has a {\bf $QR$ decomposition} $A=QR$,
where $Q=Q_{m\times m}$ is unitary and $R=R_{m\times n}$ is upper
triangular.  The $QR$ decomposition is a tool of fundamental
importance in linear algebra.  We thus present {\it five} different algorithms to construct various forms
of this decomposition in the subsections that follow.  Note that, by presenting these constructions, we also establish that the
$QR$ decomposition itself exists.

Further, if $r=\Rank(A)=n$ (which is often the case when using the $QR$ decomposition),
or $r<n$ but column pivoting (via a permutation matrix $\Pi$)
is applied to ensure the $QR$ decomposition is ordered correctly (such that, e.g., $|r_{ii}|$ decreases with $i$),
then the $QR$ decomposition can be partitioned such that:
\begin{equation}
  A_{m\times n} \Pi_{n\times n} =Q_{m\times m} R_{m\times n} = 
  \begin{bmatrix} {\underline{Q}}_{\,m \times r} & {\overline Q}_{m \times (m-r)} \end{bmatrix}
  \begin{bmatrix} {\underline{R}}_{\,r \times n} \\ {0} \end{bmatrix} =
      {\underline{Q}}_{\,m \times r} {\underline{R}}_{\,r \times n},
      \label{partitionedQR}
\end{equation}
where ${\underline{R}}$ is upper triangular (and possibly fat), $\underline{Q}$ is an orthogonal basis for the column space of $A$,
$\overline Q$ is an orthogonal basis for the left nullspace of $A$ (see \S \ref{sec.A.A.I}), and $\Pi$ is a permutation matrix.
We thus refer to
\beginmylistb
\item a {\bf complete form}, $A_{m\times n}=Q_{m\times m} R_{m\times n}$ (taking $\Pi=I$), as a $QR$ {\bf decomposition} of $A$, 
\item a {\bf pivoted complete form}, $A_{m\times n} \Pi_{n\times n}=Q_{m\times m} R_{m\times n}$ with a permutation matrix $\Pi$ selected so that
$|r_{ii}|$ decreases with $i$ [thus revealing the block partitioning of \eqref{partitionedQR}], as a {\bf pivoted $QR$ decomposition} of $A$, 
\item a {\bf reduced form}, $A_{m\times n}={\underline{Q}}_{\,m \times r} {\underline{R}}_{\,r \times n}$ (with\footnote{In the important
special case in which $r=\Rank(A)=n$, all $n$ elements on the main diagonal of $R$ are nonzero; in this special case, we may always determine a
$\underline{QR}$ decomposition of $A$ without pivoting} $\Pi=I$), as a {\bf $\underline{QR}$ decomposition} of $A$, and
\item a {\bf pivoted reduced form}, $A_{m\times n} \Pi_{n\times n}={\underline{Q}}_{\,m \times r} {\underline{R}}_{\,r \times n}$, as a {\bf pivoted $\underline{QR}$ decomposition} of $A$.  
\endmylist
\clearpage

\noindent Beware that other texts are not as strict notationally, so you sometimes need to look carefully to see whether
the pivoted or nonpivoted and the complete or reduced form of this decomposition is being used. 
Note also that a $QL$  decomposition may also be developed via
procedures similar to those outlined below, where $Q$ is unitary
and $L$ is {\it lower} triangular.

The first two algorithms presented below (Classical Gram-Schmidt and
Modified Gram-Schmidt) only determine (reduced)
$\underline{QR}$ decompositions, whereas the last three algorithms
presented (those based on Householder reflections, Givens
rotations, and fast Givens transforms) determine (complete) $QR$ decompositions.
When column pivoting is applied [if necessary; that is, if $\Rank(A)<n$], the latter three algorithms thus reveal
both $\underline{Q}$ and $\overline{Q}$.  As mentioned above,
$\underline{Q}$ is an orthogonal basis of the column space of $A$, and
$\overline Q$ is an orthogonal basis of the left nullspace of $A$. 

\subsubsubsection{Determining the \protect{\underline{$QR$}} decomposition via Classical Gram-Schmidt}

\noindent Perhaps the simplest method to determine an orthonormal basis of
the space spanned by a set of $n$ linearly independent vectors
$\a^{i}$ (for $i=1,\ldots n$) of order $m$ [that is, in this subsection (only), we assume the special case that $r=\Rank(A)=n$, which
implies that $m\ge n$] is
known as {\bf Classical Gram-Schmidt} orthogonalization, or simply as
{\bf Gram-Schmidt}.  This method (see Algorithm \ref{alg.3.QRcgs}) is initialized by taking the first
vector of this orthonormal basis, $\q^{1}$, as the first vector of the
original set, $\a^{1}$, scaled to be of unit norm, i.e.,
\begin{equation*}
  \q^{1} =\a^{1}/r_{11} \quad \textrm{where} \quad r_{11}=\Vert \a^{1} \Vert.
\end{equation*}
Thereafter (for $i=2,3,\ldots,n$), $\q^{i}$ is taken as the original vector
$\a^{i}$ minus its projections in the directions of the
previously-computed orthonormal basis vectors ($\q^{k}$ for
$k=1,\ldots,i-1$) and scaled to be of unit norm,
\begin{equation*}
  \q^{i} = \z^{i}/r_{ii} \quad \textrm{where} \quad 
  \z^{i} = \a^{i} - \sum_{k=1}^{i-1} r_{ki}\q^{k}
  \quad \textrm{with} \quad 
  r_{ki} = \begin{cases} (\q^{k})^{H}\a^{i}\ \  & k<i\\
			      \Vert \z^{i} \Vert & k=i\\
			      0 & k>i. \end{cases}  \quad \textrm{Combining,} \quad
			      \a^{i}=\sum_{k=1}^{n} r_{k i} \q^{k}.
\end{equation*}
Multiplying the equation above for $\q^{i}$ from the left by
$(\q^{j})^{H}$ for values of $j$ ranging from $1$ to $i$, it is
easily verified that $(\q^{j},\q^{i})=\delta_{ji}$.  The
combined expression at right is easily recognized as simply
\begin{equation*}
    \begin{pmatrix} | & | &  & | \\ \a^1 & \a^2 & \ldots  & \a^n \\ 
		    | & | &  & | \end{pmatrix} =
    \begin{pmatrix} | & | &  & | \\ \q^1 & \q^2 & \ldots  & \q^n \\ 
		    | & | &  & | \end{pmatrix}
    \begin{pmatrix}
       r_{11} & r_{12} & \hdots & r_{1n} \\
	      & r_{22} & \hdots & r_{2n} \\
	      &        & \ddots & \vdots \\
	0     &        &        & r_{nn}
    \end{pmatrix},
\end{equation*}
that is, as $A=\underline{QR}$, where $A$ is the original matrix (with
the $n$ linearly independent vectors $\a^{k}$ as columns), $\underline{Q}$ is an $m\times n$ unitary
matrix (with the $n$ orthonormal vectors $\q^{i}$ as columns), and $\underline{R}$
is an $n\times n$ upper triangular matrix.  Note that the Classical Gram Schmidt procedure
builds up the matrix $\underline{R}$ one {\it column} at a time; that is, for
each $i$, the $r_{ki}$ are selected so that
$\q^{i}$ is orthogonal to the previously-computed $\q^{k}$
(for $k=1,\ldots,i-1$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.QRcgs,caption=Compute a $\underline{QR}$ decomposition of a matrix $A$ of full column rank via Classical Gram-Schmidt.]{RCC1.0/chap04/QRcgs.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.QRmgs,caption=Compute a pivoted $\underline{QR}$ decomposition of a matrix $A$ via Modified Gram-Schmidt.]{RCC1.0/chap04/QRmgs.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsubsection{Determining the pivoted \protect{\underline{$QR$}} decomposition via Modified Gram-Schmidt}

\noindent The {\bf Modified Gram-Schmidt} algorithm is simply a reordering of the
Classical Gram Schmidt algorithm that is better behaved numerically in
terms of the orthogonality of the resulting columns of $\underline{Q}$.
In constrast with the Classical Gram-Schmidt procedure, the Modified
Gram-Schmidt procedure builds up the matrix $\underline{R}$ one {\it
row} at a time; that is, for each $i$, the $r_{ik}$ are
selected so that $\q^{i}$ is orthogonal to the columns of the
matrix from which the yet-to-be-determined $\q^{k}$ (for
$k=i+1,\ldots,n$) will be determined.  This is done by
subtracting off from the remaining columns of $A$ the appropriate
projections on $\q^{i}$ as soon as $\q^{i}$ is determined.
By so doing, future projections may be calculated more accurately, as
they are based on shorter vectors (the modified columns of $A$) that
already have several other projections subtracted off, thereby
reducing round-off error.  Note that, in this subsection and the three that follow,
we relax the assumption made previously that $r=\Rank(A)=n$.

The resulting procedure, at step $i$, may be described as
follows: assume we are construcing $A=\underline{QR}$ (where $\underline{Q}$ is unitary and
$\underline{R}$ is upper triangular) and we have already determined the first
$i-1$ columns of $\underline{Q}$ and the first $i-1$ rows of $\underline{R}$ (and
therefore, since $\underline{R}$ is upper triangular, the first $i-1$ columns
of $\underline{R}$).  Writing $A-\underline{QR}=0$ in index notation as
$a_{ij}-\sum_{i'=1}^{n}q_{ii'}r_{i' j}=0$ and moving
the component of this sum corresponding to all yet-to-be-determined
columns of $\underline{Q}$ and rows of $\underline{R}$ at step $i$ to the RHS, we 
define $A_{i}$ as the modified $A$ matrix comprised of the following elements:
\begin{equation*}
    [A_{i}]_{st}\triangleq a_{st}-\sum_{i'=1}^{i-1}q_{si'}r_{i't}
	  =\sum_{i'=i}^{n}q_{si'}r_{i' t}.
\end{equation*}
Noting that the first $i-1$ columns of the matrix $A_{i}$
are zero, we name the $i$'th column of the $A_{i}$ matrix $\z^{i}$.  As in the Classical
Gram-Schmidt procedure, we now scale $\z^{i}$ appropriately to determine
$\q^{i}$, taking
\begin{equation*}
	 \q^{i} = \begin{cases} \z^{i}/r_{ii} & \textrm{if\ } r_{ii}>0,\\
                                  0  & \textrm{otherwise}, \end{cases} \quad \textrm{where}
	 \quad r_{ii}=\Vert \z^{i} \Vert.
\end{equation*}
We then subtract off from $A_{i}$ the projections of the
remaining columns of $A_{i}$ on $\q^{i}$, taking
\begin{equation*}
	 A_{i+1}=A_{i} - \q^{i}\begin{bmatrix}
	 0_{1\times(i-1)} & r_{i,i+1} & r_{i,i+2} & \ldots & r_{i n}
	 \end{bmatrix}, \quad \textrm{where} \quad 
	 r_{ik} = (\q^{i})^{H}\a^{k}\ \ \textrm{for}\ \ k>i,
\end{equation*}	 
where $\a^{k}$ refers to the $k$'th column of $A_{i}$.  Note that the first $i$ columns of the matrix $A_{i+1}$ are zero.  We
then increment $i$ and go to the next step.  Though no more
expensive than Classical Gram Schmidt, as exactly the same number of
projections, vector subtractions, and normalizations are performed (that is, if pivoting isn't applied),
the Modified Gram-Schmidt algorithm is usually a bit more accurate in terms of
the resulting orthogonality of the columns of $\underline{Q}$.

The procedure of column pivoting developed in \S \ref{sec.B.B.C} (to determine the $A=PLU\!Q^T$ decomposition)
is incorporated in Algorithm \ref{alg.3.QRmgs} in an almost identical fashion: before each step $i$,
the columns of the modified $A$ matrix are swapped to move the column with the largest
norm into the $i$'th column, keeping track of this column swap in the permutation matrix $\Pi$ (or, to reduce storage,
in a permutation vector $\pi$). This ensures that $|r_{ii}|$ decreases with $i$,
and thus that a partitioning of the form \eqref{partitionedQR} exists even if $\Rank(A)<n$.  The necessary modifications to {\tt QRmgs.m}
to implement this are addressed in Exercise \ref{ex:04.QRmgspivot}.  
Note also that, for efficiency, immediately after the $i$'th step of the Modified Gram-Schmidt procedure when implementing
this column pivoting, the (previously-calculated)
length of the $t$'th column of $A_{i}$, for $t> i$, may be updated to determine the length of the $t$'th
column of $A_{i+1}$ simply by subtracting off $r_{i t}$; that is, $r_{tt} \leftarrow \sqrt{r_{tt}^2 - r_{it}^2}$ 

\subsubsubsection{Determining the $QR$ decomposition via Householder reflections} 

\noindent An approach for determining the (complete) $QR$ decomposition which is
better than modified Gram-Schmidt in terms of the orthogonality of the
resulting columns of $Q$, but is about twice as expensive, is to use a
sequence of Householder reflections in a manner very similar to that
used to compute the RC_Hessenberg decomposition in \S \ref{sec.A.D.A}.

The determination of the $QR$ decomposition of the matrix $A$ via this
approach is comprised of $p=\min(n,m-1)$ steps.  The $k$'th step of this
procedure may be described as follows: assume that the first $k-1$
columns of $A$ have already been transformed to upper triangular form,
that is,
\begin{equation*}
	 (Q_{k-1}^{H}Q_{k-2}^{H}\hdots Q_{1}^{H})A=\begin{bmatrix} \underline{R}_{\,(k-1)\times (k-1)} & *_{(k-1)\times (n-k+1)} \\
	 0 & *_{(m-k+1)\times (n-k+1)} \end{bmatrix},
\end{equation*}
where the $Q_{i}$ are unitary and $\underline{R}_{\,(k-1)\times (k-1)}$
is upper triangular (as its notation implies,
$\underline{R}_{\,(k-1)\times (k-1)}$ turns out to be the upper-left
corner of $\underline{R}_{\,n\times n}$).  Referring to the elements of
the transformed matrix as $a_{ij}$ (performing the transformation in
place in the computer memory), the $k$'th step focuses on reducing the
$k$'th column to upper triangular form.  Define
\begin{align*}
	 &\x^{k}=\begin{pmatrix} a_{k,k} \\ a_{k+1,k} \\ \vdots \\ a_{m,k} \end{pmatrix}, \quad
    \textrm{$\{\sigma_{k},\w^{k},\nu_{k}\}$ according to \eqref{sigmawdefb} and $H(\sigma_{k},\w^{k})$
    according to \eqref{Householder},}\\
	 &Q_{k}=\begin{bmatrix} I_{(k-1)\times (k-1)} & 0 \\
						     0 & H(\sigma_{k},\w^{k})_{(m-k+1)\times (m-k+1)} \end{bmatrix}
	 \quad \Rightarrow \quad
	 (Q_{k}^{H}Q_{k-1}^{H}\hdots Q_{1}^{H})A=\begin{pmatrix} \underline{R}_{\,k\times k} & *_{k\times (n-k)} \\
	 0 & *_{(m-k)\times (n-k)} \end{pmatrix}.
\end{align*}
After $p=\min(n,m-1)$ steps, defining $Q=Q_{1}Q_{2}\cdots Q_{p}$ and thus
$Q^{-1}=Q_{p}^{H}\cdots Q_{2}^{H}Q_{1}^{H}$, we have $Q^{-1}A=R$ and
thus $A=QR$, where $Q=Q_{m\times m}$ is unitary and $R=R_{m\times n}$
is upper triangular.  As in \S \ref{sec.A.D.A}, the individual $Q_{i}$
need never be computed.  In order to compute both $Q$ and $R$, it is
sufficient to compute the $\sigma_{k}$ and $\w^{k}$, noting
\eqref{householdercheap}, as implemented in Algorithm \ref{alg.3.QRHouseholder}.

% ???????????????????????????
% In fact, redefining $\w^{k}\leftarrow \sqrt{\sigma_{k}} \w^{k}$ and
% $\sigma_{k}\leftarrow 1$ and storing the modified $\w^{k}$ vectors in the lower
% triangular elements of the modified $A$ matrix as the corresponding
% elements are set to zero during the computation of $R$, this algorithm
% again reflects the conservation of information property seen
% previously with the $LU$ decomposition presented in \S \ref{sec.B.Ab};
% that is, the information necessary to describe the $QR$ decomposition
% of $A$ following this approach takes precisely the same number of
% elements as it takes to describe $A$ itself.
% ???????????????????????????

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.QRHouseholder,caption=Compute a $QR$ decomposition of a general matrix $A$ via Householder reflections.]{RCC1.0/chap04/QRHouseholder.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Note that column pivoting may easily be applied to this algorithm in the same manner as implemented in Exercise \ref{ex:04.QRmgspivot} following the Modified Gram-Schmidt approach.  By so doing, one may easily distinguish
the columns of $\underline{Q}$ from the columns of $\overline{Q}$ in the pivoted $QR$ decomposition [of the form
given in \eqref{partitionedQR}] even when $\Rank(A)<n$. 

\subsubsubsection{Determining the $QR$ decomposition via Givens rotations}

\noindent The $QR$ decomposition may also be built up from several,
appropriately configured Givens rotation matrices $G$, as defined in
\S \ref{sec.A.A.L}.  The determination of the $QR$ decomposition of the matrix
$A$ via a series of such Givens rotations simply steps through every
subdiagonal element of $A$, multiplying (if necessary) the entire
matrix $A$ by the appropriate Givens rotation matrix such that the
transformed matrix is zero in that element, with the ordering of the
loops arranged in such a manner that the subdiagonal elements of $A$
that have already been transformed to zero by this procedure stay that
way.  For example, assume that after five steps of the procedure
we have transformed $A$ to the following form:
\begin{equation*}
(G^{H}_{5}G^{H}_{4}\cdots G^{H}_{1}) A = \begin{pmatrix} x & x & x & x \\
					     0 & x & x & x \\
					     0 & x & x & x \\
					     0 & x & x & x \\
					     0 & 0 & x & x \end{pmatrix},  
\end{equation*}
where the $x$'s denote the nonzero elements.  For the next step, we define
\begin{equation*}
    G_{6}=G(3,4;x_{3,2},x_{4,2}) \quad \Rightarrow \quad
    (G^{H}_{6}G^{H}_{5}G^{H}_{4}\cdots G^{H}_{1}) A = \begin{pmatrix} x & x & x & x \\
						 0 & x & x & x \\
						 0 & * & * & * \\
						 0 & 0 & * & * \\
						 0 & 0 & x & x \end{pmatrix},  
\end{equation*}
where the $*$'s denote the nonzero elements that have most recently been modified.
We continue in this manner, working on each subdiagonal element in
turn, until the transformed matrix reaches upper triangular form, at which point we have
$Q^{H}A=R$, and thus $A=QR$, where $Q=G_{1}G_{2}\cdots G_{p}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.RC_QRGivensHessenberg,caption=Compute a $QR$ decomposition of an upper RC_Hessenberg matrix $A$ via Givens rotations.]{RCC1.0/chap04/RC_QRGivensHessenberg.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As in the Householder approach described previously, the individual
$G_{i}$ need never be calculated; initializing $Q=I$ and noting the
simple algorithmn to compute $G^{H}X$ in \eqref{GTxa},
both $Q$ and $R$ can be determined efficiently by working in
place in the computer memory on $Q$ and $A$, applying the Givens
rotations directly from the $\{i,k;c,s\}$ parameters defining each
one, as illustrated in Algorithm \ref{alg.3.RC_QRGivensHessenberg}.
At the end of the procedure, $A$ is replaced by $R$.  Further, it is important to note that, in most algorithms
that use the $QR$ decomposition, we do not even need the matrix
$Q=G_{1}G_{2}\cdots G_{p}$ explicitly; we only need to be able to
apply $Q$ to vectors or matrices (that is, collections of vectors).
Thus, for efficiency, we may store the information defining each
rotation matrix that combine to make up $Q$ instead of storing $Q$ itself.  This may be done by storing the $c$ and $s$ associated with each rotation or,
more compactly, by storing $\gamma$ [from which the $c$ and $s$ associated with each rotation may be recomputed,
as seen in \eqref{GTxc}] into the element of the transformed matrix that has
just been set to zero.  The resulting matrix $A$ then contains $R$ in
its upper triangular part and the various values of $\gamma$ in each
of its subdiagonal elements.  Noting that $Q=G_{1}G_{2}\cdots G_{p}$, the product $XQ$ for some matrix
$X$ may later be computed leveraging \eqref{GTxc} and \eqref{GTxa2}.
Thus, as with the $LU$ decomposition presented in \S \ref{sec.B.Ab},
this algorithm also demonstrates the conservation of information
property evident in many efficient numerical algorithms; that is, the
information necessary to describe the $QR$ decomposition of $A$
following the minimum storage version of this algorithm takes precisely
the same number of elements as it takes to describe $A$ itself.

The primary advantage of the Givens rotations approach to computing
the $QR$ decomposition is that it can take advantage of any subdiagonal zeros $A$ already possesses by reducing the number of Givens
rotations required to transform $A$ to an upper triangular form.  In particular,
if $A$ is either RC_Hessenberg or tridiagonal, only $n-1$ Givens
rotations need to be performed to compute the $QR$ decomposition of
$A$ using this approach.

\enlargethispage{3pt}
\subsubsubsection{Determining the $QR$ decomposition via fast Givens transforms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.RC_QRFastGivensHessenberg,caption=Compute a $QR$ decomposition of an upper RC_Hessenberg matrix $A$ via fast Givens transforms.]{RCC1.0/chap04/RC_QRFastGivensHessenberg.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent The previous section demonstrated how to build a $QR$ decomposition of $A$ via a series of $p$ (unitary) Givens rotations
$(G^H_p \cdots G^H_2 G^H_1)A=Q^H A = R$.  The approach is particularly attractive because, unlike a Householder-based approach, it is
significantly accelerated if there are already some zeros in the strictly lower-triangular part of $A$.
Leveraging the following two facts, we now illustrate how to do the same thing via a series of
$p$ (nonunitary) fast Givens transforms, which, as discussed in \S \ref{sec.A.A.P}, are each 33\%
cheaper to apply than Givens rotations due to their even simpler structure.

\begin{fact} \label{fact.A.D.B.Ea}
If $\overline Q^H A=\overline R$ is upper triangular and $\overline Q^H \overline Q=D$ is diagonal and nonsingular, then $Q=\overline Q D^{-1/2}$ and
$R=D^{-1/2} \overline R$ form a $QR$ decomposition of $A$ (that is, $Q$ is unitary, $R$ is upper triangular, and $A=QR$).
\end{fact}

\noindent {\it Proof}\/: If $\overline R$ is upper triangular, $R=D^{-1/2} \overline R$ is upper triangular by inspection.
To see that $Q=\overline Q D^{-1/2}$ is unitary, note that
\begin{equation*}
  Q^H Q = (\overline Q D^{-1/2})^H (\overline Q D^{-1/2}) =  D^{-1/2} D D^{-1/2} = I.
\end{equation*}
Finally, to see that $A=QR$, note that
\begin{equation}
  Q^H A = D^{-1/2} \overline Q^H A = D^{-1/2} \overline R = R.  \tag*{$\Box$}
\end{equation}

\begin{fact} \label{fact.A.D.B.Eb}
Initializing $\overline Q_0=I$ and $D_0=I$ and defining $\overline Q_j$ and $D_j$ via a series of fast Givens transforms
$\overline Q_j=F_1 F_2 \cdots F_j$ and $D_j=F_j^H D_{j-1} F_j$, it follows that $\overline Q_j^H \overline Q_j = D_j$ is diagonal.
\end{fact}

\noindent {\it Proof (by induction)}\/: The base case $j=0$ is clearly true.  Assuming $\overline Q_j^H \overline Q_j=D_j$ is diagonal, it
follows that 
\begin{equation*}
 \overline Q_{j+1}^H \overline Q_{j+1} = (\overline Q_j F_{j+1})^H (\overline Q_j F_{j+1}) = F^H_{j+1} D_j  F_{j+1} = D_{j+1}.
\end{equation*}
Noting the definition of $F$ (see the last paragraph of \S \ref{sec.A.A.P}), $F^H_{j+1} D_j  F_{j+1} = D_{j+1}$ is diagonal. \endproof
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.QRFastGivensTridiagonal,caption=Compute a $QR$ decomposition of a tridiagonal matrix $A$ via fast Givens transforms.]{RCC1.0/chap04/QRFastGivensTridiagonal.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent 
Fact \ref{fact.A.D.B.Eb} establishes a simple iterative procedure which we may use
to construct a nonunitary matrix $\overline Q_k$ via a series of $k$ fast Givens transforms, $F_1$ through $F_k$, such that
$\overline Q_k^H A= \overline R_k$ is upper triangular, where $\overline Q_k=F_1 F_2 \cdots F_k$ and $\overline Q_k^H \overline Q_k= D_k$ is diagonal. 
Note (see \S \ref{sec.A.A.P}) that the iterative procedure to determine $D_{k}$ is computationally inexpensive. 
By Fact \ref{fact.A.D.B.Ea}, it thus follows that $Q=\overline Q_k D_k^{-1/2}$ and $R_k=D_k^{-1/2} \overline R$ form a $QR$ decomposition of $A$,
as implemented for RC_Hessenberg and tridiagonal $A$ in
Algorithms \ref{alg.3.RC_QRFastGivensHessenberg} and \ref{alg.3.QRFastGivensTridiagonal}.

\subsection{Characterizing the RC_Schur decomposition}\label{sec.A.D.C}

Every $n\times n$ matrix $A$ has a {\bf RC_Schur decomposition} $A=U T U^{H}$,
where $T$ is upper triangular and $U$ is unitary with columns $\u^k$, sometimes referred to as
{\bf RC_Schur vectors}.  Proof of this
important statement builds up from two preliminary facts that are easy
to verify:\medskip

\begin{fact} \label{fact.A.D.C.A}
If an $n\times n$ matrix $T$ may be partitioned
as $T=\begin{bmatrix} L_{p\times p} & C_{p\times q} \\ 0 & S_{q \times q} \end{bmatrix}$, then $\lambda(T)=\lambda(L) \cup \lambda(S)$.
Further, the eigenvalues of a triangular matrix appear on its main diagonal,
and the eigenvalues of a block triangular matrix, with square blocks on the main diagonal, are given by the union of the eigenvalues of the blocks
on the main diagonal.
\end{fact}

\noindent {\it Proof}\/: If $\Lambda$ is an eigenvalue of $T$, then by
Property 3b of the determinant in \S \ref{sec.A.B.A}, $|\lambda I-T|=|\lambda I-L|\cdot |\lambda I-S|=0$, where
$|\lambda I-L|$ is a polynomial whose $p$ roots are the eigenvalues of $L$, and
$|\lambda I-S|$ is a polynomial whose $q$ roots are the eigenvalues of $S$.  Thus, the
$p+q=n$ roots of $T$ are given by the union of the roots of $L$ and
the roots of $S$. The statements about triangular and block triangular matrices follow via repeated application of this result.\endproof

\begin{fact} \label{fact.A.D.C.B}
If
\begin{equation}
    AX=XB,
    \label{AX=XB}
\end{equation}
where $A=A_{n\times n}$, $B=B_{p\times p}$, and $X=X_{n\times p}$ with
$\Rank(X)=p$, then there exists a unitary $Q$ such that
\begin{equation}
Q^{H} A Q =\begin{bmatrix} L_{p\times p} & C_{p\times q} \\ 0 & S_{q
\times q} \end{bmatrix},
\label{QHAQ=L0CS}
\end{equation}
where $\lambda(L)=\lambda(B)$.
\end{fact}

\noindent {\it Proof}\/: By the constructions given in \S \ref{sec.A.D.B}, we
know a $QR$ decomposition of $X$ exists, which may be written
\begin{equation*} X_{n\times p}=Q_{n\times n}\begin{bmatrix} \underline{R}_{p\times p} \\ 0 \end{bmatrix},
\end{equation*}
where $Q$ is unitary and $\underline{R}$ is triangular; note also
that, as $\Rank(X)=p$, it follows that $\Rank(\underline{R})=p$ as
well.  Substituting this decomposition into \eqref{AX=XB} and
multiplying by $Q^{H}$, we have
\begin{equation*}
Q^{H} A Q \begin{bmatrix} \underline{R} \\ 0 \end{bmatrix}
=\begin{bmatrix} L_{p\times p} & C_{p\times q} \\ D_{q\times p} & S_{q \times q} \end{bmatrix} \begin{bmatrix} \underline{R} \\ 0 \end{bmatrix}
=\begin{bmatrix} \underline{R} \\ 0 \end{bmatrix} B.
\end{equation*}
As $\underline{R}$ is nonsingular, the first block row of this
equation may be written $\underline{R}^{-1}L\underline{R}=B$, and
thus, by Fact \ref{fact.A.D.A}, $\lambda(L)=\lambda(B)$.  The second
block row of this equation gives $D \underline{R}=0$; as
$\underline{R}$ is nonsingular, the unique solution of this equation
is $D=0$, thus establishing the existence of the decomposition
\eqref{QHAQ=L0CS}.   \endproof

\begin{fact}[The RC_Schur Decomposition Theorem] \label{fact.A.D.C.Bb} Every square
matrix $A=A_{n\times n}$ has a RC_Schur decomposition $A=U T U^{H}$,
where $U$ is unitary and $T$ is upper triangular.  Further, the matrix
$T$ in the RC_Schur decomposition has the eigenvalues of $A$ listed (in
any desired order) on its main diagonal.
\end{fact}

\noindent {\it Proof (by induction)}\/: Note first that the theorem holds
(trivially) for the base case of order $n=1$.  Assume the theorem
holds for order $n-1$.  Now consider the case of order $n$: for any
desired eigenvalue $\lambda$ of $A=A_{n\times n}$, we can write
$A\s=\lambda \s$ with $\s\ne 0$, as established in \S \ref{sec.A.C.A}.
By Fact \ref{fact.A.D.C.B} (with $B=\lambda$ and $X=\s$), there exists
a unitary $Q_{n}$ such that
\begin{equation*}
Q_{n}^{H} A Q_{n} =\begin{bmatrix} \lambda & * \\ 0 & S \end{bmatrix},
\end{equation*}
where $S=S_{(n-1)\times (n-1)}$.  By the induction hypothesis, a RC_Schur
decomposition exists for $S$, which we denote $S=U_{n-1} T_{n-1}
U^{H}_{n-1}$, where $U_{n-1}$ is unitary and $T_{n-1}$ is upper
triangular.  Thus, we may write
\begin{equation*}
    \begin{bmatrix} 1 & 0 \\ 0 & U^{H}_{n-1} \end{bmatrix} Q_{n}^{H} A Q_{n}
		\begin{bmatrix} 1 & 0 \\ 0 & U_{n-1}     \end{bmatrix} \triangleq U^{H} A U =
		\begin{bmatrix} \lambda & * \\ 0 & U^{H}_{n-1} S U_{n-1} \end{bmatrix} =
		\begin{bmatrix} \lambda & * \\ 0 & T_{n-1} \end{bmatrix} \triangleq
		    T \quad \Rightarrow \quad A = U T U^{H},
\end{equation*}
where $U$ is unitary and $T$ is upper triangular.  By Property 3a of
the determinant (see \S \ref{sec.A.B.A}), as $T$ is a triangular
matrix, the equation $|\lambda I-T|=0$ gives simply
$(\lambda-t_{11})(\lambda-t_{22})\cdots(\lambda-t_{nn})=0$, where
$t_{11}$ to $t_{nn}$ are the diagonal elements of $T$.  As $T$ and $A$
share the same eigenvalues (Fact \ref{fact.A.D.A}), it follows that
the eigenvalues of $A$ are listed on the main diagonal of $T$.  Note
further that any desired eigenvalue may be selected to be in the
upper-left corner of $T$ (and of $T_{n-1}$, etc.); thus, once the
eigenvalues of $A$ are known, the above relations may be used to
construct a RC_Schur decomposition with the eigenvalues appearing on the
main diagonal of $T$ in any desired ordering.   \endproof \vskip0.1in

As noted in Fact \ref{fact.abel1}, the RC_Schur decomposition cannot, in general, be
computed exactly with a finite sequence of calculations, as could the
closely-related RC_Hessenberg decomposition discussed in \S \ref{sec.A.D.A}.
A family of efficient iterative algorithms to determine
the RC_Schur decomposition is presented in \S \ref{sec.A.D.E}.

Note that the first column of the equation $AU=UT$ is $A\u^1=\lambda_1\u^1$, and thus the
first RC_Schur vector is, in fact, an eigenvector of $A$ corresponding to $\lambda_1$ (that is, $\u^1=\s^1$).

Finally, note that, if $A$ is Hermitian, then $T$ is triangular and,
by Fact \ref{fact.A.D.B}, $T$ is also Hermitian---that is, $T$ is diagonal.  In
this case, we denote $T$ by $\Lambda$ and $U$ by $S$, and the RC_Schur
decomposition reduces to the {\bf eigen decomposition}, as described
in \S \ref{sec.A.D.D}, with a unitary eigenvector matrix $S$.

\subsubsubsection{Consequences of the existence of the RC_Schur decomposition}

\noindent The existence of the RC_Schur decomposition allows us to prove some
important results, such as the following.

\begin{fact}[The Cayley-Hamilton Theorem] \label{fact.A.D.C.Bc} Any square
matrix $A$ satisfies its own characteristic equation.
\end{fact}

\noindent {\it Proof}\/: By the Fundamental Theorem of Algebra (Fact \ref{fact.A.C.A.A}), the characteristic equation of $A$ may be written
\begin{equation*}
    \lambda^{n}+a_{n-1}\lambda^{n-1}+\ldots + a_{1}\lambda +a_{0} \lambda^0=
    (\lambda - \lambda_{1})(\lambda-\lambda_{2})\cdots (\lambda-\lambda_{n})=0.
\end{equation*}
Replacing $\lambda$ in this equation with the matrix $A$ itself, and inserting $I$ where appropriate,
consider the quantity
\begin{equation}
    A^{n}+a_{n-1}A^{n-1}+\ldots + a_{1}A+a_{0} I =
    (A - \lambda_{1}I)(A -\lambda_{2}I)\cdots (A -\lambda_{n}I) = C.
    \label{intermediateCH}
\end{equation}
By the RC_Schur decomposition theorem, we know that $A=UTU^{H}$, that is,
$A$ is unitarily similar to an upper triangular matrix $T$ with the eigenvalues of $A$
on its main diagonal.  Thus \eqref{intermediateCH} may be written
\begin{align*}
    (UTU^{H} - \lambda_{1}UU^{H})(UTU^{H} -\lambda_{2}UU^{H})\cdots (UTU^{H} -\lambda_{n}UU^{H}) &= C \\
    U(T - \lambda_{1}I)U^{H}\,U(T -\lambda_{2}I)U^{H}\cdots U(T -\lambda_{n}I) U^{H} &= C \\
    (T - \lambda_{1}I) (T -\lambda_{2}I) \cdots (T -\lambda_{n}I)  &= U^{H} C U.
\end{align*}
The upper triangular matrix factor $(T-\lambda_{i}I)$ has a zero on its $i$'th diagonal
element.  Noting this fact and multiplying the upper triangular matrix factors together one by one, it is
easily verified that the first column of $(T - \lambda_{1}I)$ is zero, then that
the first two columns of $(T - \lambda_{1}I) (T -\lambda_{2}I)$ are
zero, etc.  Thus, $U^{H}CU=0\ \ \Rightarrow\ \ C=0$, and therefore, by
\eqref{intermediateCH}, $A$ satisfies its own characteristic equation.  \endproof 

\begin{fact} \label{fact.A.D.C.Bd} The eigenvalues of $A^{-1}$ are the reciprocal of the eigenvalues of $A$.
\end{fact}

\noindent {\it Proof}\/: By Fact \ref{fact.A.D.C.Bb}, $A=UTU^{H}$ where $T$ is triangular.  By
Fact \ref{fact.A.A.J.C}, it follows that $A^{-1}=UT^{-1}U^{H}$ where, by Fact \ref{fact.inverse.triangular}, the
diagonal elements of $T^{-1}$ are the reciprocal of the diagonal
elements of $T$.  Thus, by the RC_Schur decomposition theorem, the eigenvalues of $A^{-1}$ are the
reciprocal of the eigenvalues of $A$. \endproof

\begin{fact} \label{fact.A.D.C.Be} The determinant of $A$ is equal to the product of its eigenvalues:
$|A|=\lambda_{1}\lambda_{2}\cdots\lambda_{n}$.  In particular, $|A|=0$
iff $\lambda_{i}=0$ for at least one value of $i$.
\end{fact}

\noindent {\it Proof}\/: By Fact \ref{fact.A.D.C.Bb}, $A=UTU^{H}$ with
$U^{H}=U^{-1}$ and $T$ upper triangular with the eigenvalues of $A$
listed on its main diagonal.  By Properties 3 and 5 of the determinant
(see \S \ref{sec.A.B.A}), it follows that $|A|=|UTU^{H}|=|U|\cdot
|T|\cdot |U^{H}|= |T| =
\lambda_{1}\lambda_{2}\cdots\lambda_{n}$.  \endproof \vskip0.1in

\begin{fact} \label{fact.A.D.C.Bf} The eigenvalues of a real matrix $A$ come in complex conjugate pairs.
That is, if $\lambda\in \lambda(A)$, then $\bar\lambda \in \lambda(A)$ with the same multiplicity.
\end{fact}

\noindent {\it Proof}\/: By Fact \ref{fact.A.D.C.Bb}, $A=UTU^{H}$ with $T$ upper triangular and the eigenvalues of $A$
listed on its main diagonal.  Since $A$ is real, it follows that $A=\bar A=(\bar U) \bar T (\bar U)^{H}$.  Since $\bar T$ is also triangular,
the eigenvalues of $A$ are listed on its main diagonal.  \endproof

\subsubsection{The real RC_Schur decomposition} \label{sec.A.D.Ca}

Complex arithmetic is significantly more expensive than real arithmetic.  If a real $n\times n$ matrix $A$ is known
to have real eigenvalues (e.g., as established in Fact \ref{fact.A.D.D.B} for symmetric matrices), then the matrices forming its
RC_Schur decomposition $A=UTU^H$ happen to be real, and may be computed using real arithmetic.  

Unfortunately (from a computational perspective), the RC_Schur decomposition of a general real matrix is complex.
For such matrices, it is often desirable to compute a convenient eigenvalue-revealing form without resorting to
complex arithmetic.  This may be accomplished by performing the {\bf real RC_Schur decomposition} $A=U \hat T U^T$, 
where $U$ and $\hat T$ are real and $\hat T$ is in block upper triangular form with $1\times 1$ and $2\times 2$
blocks (with complex conjugate eigenvalues) on the main diagonal, referred to as a {\bf real RC_Schur} form.  
Essentially, the real RC_Schur decomposition is as close as you can get to a RC_Schur decomposition of a
general real matrix without resorting to complex arithmetic.
By Facts \ref{fact.A.D.A} and Fact \ref{fact.A.D.C.A}, the eigenvalues of $A$ are the
union of the eigenvalues of the blocks on the main diagonal of $\hat T$.  By Fact \ref{fact.A.C.A.B}, the eigenvalues of
the $2\times 2$ blocks are easy to compute.
In particular, if the $2\times 2$ blocks on the main diagonal are rotated into the {\bf $2\times 2$ standard form}
\begin{equation}
A=\begin{pmatrix} \alpha & \omega/k \\ -\omega k & \alpha \end{pmatrix},
\label{2by2standardform}
\end{equation}
then $\lambda_{\pm}=\alpha\pm \imath \omega$.  Most of the algorithms that follow which leverage the RC_Schur decomposition may
in fact be written to leverage the real RC_Schur decomposition in the case that the matrices setting up the problem are real, thereby
avoiding complex arithmetic for most of the computation.

\subsection{Characterizing the eigen decomposition}\label{sec.A.D.D}

If $A_{n\times n}$ has $n$ linearly independent eigenvectors
$\{\s^{1},\s^{2},\ldots \s^{n}\}$, then any vector $\x$ of order $n$
may be uniquely decomposed in terms of contributions parallel to each
eigenvector such that
\begin{equation*}
\x=S \chib = \chi_{1}\s^{1}+\chi_{2}\s^{2}+\ldots+\chi_{n}\s^{n}, \hskip0.3in \textrm{where} \hskip0.15in
S=\displaystyle \begin{pmatrix} | & | & & | \\ \s^1 & \s^2 & \ldots & \s^n \\  | & | & & | \end{pmatrix}, \quad
\Lambda=
       \begin{pmatrix} \lambda_1 &   &       & 0 \\
			 & \lambda_2 &       &   \\
			 &   &\ddots &   \\
		       0 &   &       & \lambda_n
		       \end{pmatrix}.
\end{equation*}
The columns of $S$ are, for convenience, often scaled to have unit
norm.  The several relations $A \s^\iota = \lambda_\iota \s^\iota$ for
$\iota = 1, 2, \ldots, n$ may be assembled in matrix form as $A S
= S \Lambda$.  If the columns of $S$ are linearly independent, then
$S$ is invertible, and thus we may write the result as $A = S \Lambda
S^{-1}$ (or, alternatively, as $\Lambda = S^{-1} A S$), where, as
illustrated above, $\Lambda$ is diagonal; this is known as the {\bf eigen} (a.k.a.~{\bf spectral}) {\bf decomposition} of $A$.  

An eigen decomposition of a matrix $A$,
though expensive to calculate if the matrix is large, is very revealing.
As just one example, an eigen decomposition completely decouples
a linear system of ODEs into its independent modes, as will be shown in \S
\ref{sec.ModalCoordinateForm}. As with the RC_Schur decomposition
described in \S \ref{sec.A.D.C}, the eigen decomposition cannot, in
general, be computed exactly with a finite sequence of calculations;
some efficient iterative algorithms to compute the eigen decomposition
are presented in \S \ref{sec.A.D.E}.

{\it Note that an eigen decomposition of $A=A_{n\times n}$
exists if and only if $A$ has $n$ linearly independent eigenvectors}, in which case $A$ is said to be {\bf nondefective}.
This is often, but not always, the case.  For instance, $A$ is
guaranteed to have $n$ linearly independent eigenvectors if $A$ has
$n$ distinct eigenvalues, if $A$ is Hermitian, or if $A$ is skew
Hermitian, as established by the following facts.  On the other hand,
\S \ref{sec.A.D.G} presents an example [\eqref{A1example} with
$\epsilon=0$] that does {\it not} have $n$ linearly independent
eigenvectors; such a matrix is said to be {\bf defective}.

\begin{fact} \label{fact.A.D.D.A}
The set of eigenvectors $\{\s^{1},\ldots,\s^{k}\}$ corresponding to
distinct eigenvalues $\{\lambda_{1},\ldots\lambda_{k}\}$ is linearly
independent.
\end{fact}

\noindent {\it Proof (by induction)}\/: We first prove the statement
is true for the set $\{\s^{1},\s^{2}\}$: Take
$\y=c_{1}\s^{1}+c_{2}\s^{2}=0$.  Multiplying this equation by $A$ and
subtracting the result from $\lambda_{2}$ times this equation gives
\begin{equation*}
    \lambda_{2}[c_{1}\s^{1}+c_{2}\s^{2}] - A[c_{1}\s^{1}+c_{2}\s^{2}] =0 \quad \Rightarrow \quad
    c_{1} (\lambda_{2}-\lambda_{1}) \s^{1} = 0 \quad \Rightarrow \quad c_{1}=0 \quad \Rightarrow \quad c_{2}=0.
\end{equation*}
Thus, the set $\{\s^{1},\s^{2}\}$ is linearly independent.  Now,
assuming the statement is true for the set
$\{\s^{1},\ldots,\s^{k-1}\}$, we prove it must also be true for
$\{\s^{1},\ldots,\s^{k}\}$: Take
$\y=c_{1}\s^{1}+\ldots+c_{k}\s^{k}=0$.  Multiplying this equation by
$A$ and subtracting the result from $\lambda_{k}$ times this equation
gives
\begin{align*}
    \lambda_{k}[c_{1}\s^{1}+\ldots+c_{k}\s^{k}] -
    A[c_{1}\s^{1}+\ldots+c_{k}\s^{k}] =0 \quad &\Rightarrow \quad c_{1}
    (\lambda_{k}-\lambda_{1}) \s^{1} + \ldots + c_{k-1}
    (\lambda_{k}-\lambda_{k-1}) \s^{k-1} = 0 \\ &\Rightarrow \quad
    c_{1}=c_{2}=\ldots=c_{k-1}=0 \quad \Rightarrow \quad c_{k}=0.
\end{align*}
Thus, the set $\{\s^{1},\ldots,\s^{k}\}$ is linearly independent. \endproof

\begin{fact} \label{fact.A.D.D.B}
If the matrix $A$ is Hermitian (in the real case,
symmetric), then its eigenvalues are all real and its eigenvectors may be
chosen to be orthonormal.  Thus, $A$ may be written as a linear combination of rank one
factors such that
\begin{equation*}
A=S\Lambda S^H = \lambda_1 \s^1 (\s^1)^H + \lambda_2 \s^2 (\s^2)^H + \ldots + \lambda_N \s^N (\s^N)^H.
\end{equation*}
\end{fact}

\noindent {\it Proof}\/: By the RC_Schur decomposition theorem, there
exists a unitary matrix $U$ and an upper triangular matrix $T$ such
that $U^{H}AU=T$.  If $A=A^{H}$, then $(U^{H}AU)^{H}=U^{H}AU$, and
thus $T^{H}=T$, and therefore $T$ must be diagonal and its elements
must be real.  Identifying $T=\Lambda$ and $U=S$, we may write
$S^{H}A S=\Lambda$, where the diagonal elements of $\Lambda$ are the
(real) eigenvalues and the columns of $S$ are the (orthonormal)
eigenvectors.   \endproof

\begin{fact} \label{fact.A.D.D.C}
If the matrix $A$ is skew-Hermitian (in the real case,
skew-symmetric), then its eigenvalues are all imaginary and its
eigenvectors may be chosen to be orthonormal.
\end{fact}

\noindent {\it Proof}\/: The proof follows as in the proof of Fact
\ref{fact.A.D.D.B}, where we now find that $T=-T^{H}$, and thus the
eigenvalues must be imaginary.   \endproof

\begin{fact} \label{fact.A.D.D.D}
For any matrix $A$, the matrix $B=A^{H} A$ is Hermitian with real,
non-negative eigenvalues and eigenvectors which may be chosen to be
orthonormal.
\end{fact}

\noindent {\it Proof}\/: It follows by its definition
that $B=B^{H}$ and thus, by Fact \ref{fact.A.D.D.B}, that its
eigenvalues are real and its eigenvectors may be chosen to be
orthonormal.  Suppose $\x$ is an eigenvector of $(A^{H}A)$, and thus $\Vert \x\Vert^{2}>0$.  Then
$\x^{H}[(A^{H}A)\x=\lambda \x]\ \Rightarrow\ \lambda (\x^{H}\x) = (\x^{H}A^{H})(A\x) = \Vert A\x\Vert^{2} \ge 0$.
Since $\x^{H}\x>0$, it follows that $\lambda\ge 0$.   \endproof

\begin{fact} \label{fact.eigenvaluecontinuity} The eigenvalues of a matrix $A$ vary continuously as the elements of $A$ are varied.
\end{fact}

\noindent {\it Proof}\/: Follows as an immediate consequence of Fact \ref{fact.zerocontinuity} applied to the characteristic polynomial of $A$,
the coefficients of which are linear combinations of various products of the elements of $A$. \endproof

\begin{fact}[The Gershgorin Circle Theorem] \label{fact.Gershgorin}
Define the {\bf Gershgorin discs} of $A_{n\times n}$, denoted here $G(a_{\kappa\kappa},R_\kappa)$ for $\kappa=1,\ldots,n$,
as the $n$ closed disks in the complex plane centered at $a_{\kappa\kappa}$ with radius $R_\kappa=\sum_{i\ne\kappa} |a_{\kappa i}|$. Then:\break
{\bf (a)} Every eigenvalue of $A$ lies within at least one Gershgorin disc.
{\bf (b)} Denote by $E$ the union of $k$ Gershgorin discs, and by $F$ the union of the other $n-k$ Gershgorin discs; if $E$ is disjoint from $F$, then
$E$ contains exactly $k$ of the eigenvalues of $A$, and $F$ contains $n-k$ of the eigenvalues of $A$.
\end{fact}

\noindent {\it Proof}\/: (a) Take $\lambda$ as an eigenvalue of $A$ and $\s$ as a corresponding eigenvector (that is, $A\s=\lambda\s$ with $\s\ne 0$).
Define $\kappa=\operatorname{arg\,max}_\kappa |s_\kappa|$ (that is, select $\kappa$ as the index of the element of $\s$ that is largest in magnitude).   Then:
\begin{equation*}
\sum_{j} a_{\kappa j} s_j = \lambda s_\kappa \quad \Rightarrow \quad
\sum_{j\ne \kappa} a_{\kappa j} s_j = \lambda s_\kappa - a_{\kappa \kappa} s_\kappa \quad \Rightarrow \quad
|\lambda - a_{\kappa \kappa}| = \frac{|\sum_{j\ne \kappa} a_{\kappa j} s_j|}{|s_\kappa|} \le
\sum_{j\ne \kappa} | a_{\kappa j}| \frac{|s_j|}{|s_\kappa|} \le \sum_{j\ne \kappa} | a_{\kappa j}| = R_\kappa.
\end{equation*}
(b) Take $D$ as a diagonal matrix with $d_{\kappa\kappa}=a_{\kappa\kappa}$ for $\kappa=1,\ldots,n$, and define $B(t)=(1-t)D+tA$.
By Fact \ref{fact.A.D.C.A}, the eigenvalues of $B(0)=D$ are thus simply the diagonal elements of $A$.
Note that, for small $t$, the Gershgorin discs of $B(t)$ are correspondingly small disks centered at each diagonal element of $A$.
By part (a), the eigenvalues of $B(t)$ lie within the union of all of its Gershgorin discs.
As $t$ is increased smoothly from zero to one (that is, as $B(t)$ is converted smoothly from $D$ into $A$),
the eigenvalues of $B(t)$ vary continuously (by Fact \ref{fact.eigenvaluecontinuity}); thus, as $t$ is increased,
the eigenvalues of $B(t)$ can only move from one Gershgorin disk into another if two Gershgorin disks intersect.\endproof

\begin{fact} \label{fact.A.D.D.Aa}
If $A = S \Lambda S^{-1}$ is an eigen decomposition of $A$, then
the columns of $(S^{-1})^{H}$ are the left eigenvectors of $A$.
Further, if the eigenvectors of $A$ are orthonormal, the left and
right eigenvectors are identical.
\end{fact}

\noindent {\it Proof}\/: Follows immediately from $S^{-1}[A=S\Lambda
S^{-1}]\ \Rightarrow\ S^{-1}A=\Lambda S^{-1}\ \Rightarrow\
A^{H}(S^{-1})^{H}=(S^{-1})^{H}\Lambda^{H}$, noting the definition of a
left eigenvector, and the fact that $(S^{H})^{H}=S$.  \endproof

\begin{fact}[The Rayleigh-Ritz Theorem] \label{fact.Rayleigh.Ritz}
For Hermitian $B$ with maximum and minimum (real) eigenvalues given by $\lambda_{max}$ and $\lambda_{min}$, respectively,
\begin{equation*}
  \max_{\x^H\,\x=1} {\x^H B\, \x} = \max_{\x\ne 0} \frac{\x^H B\, \x}{\x^H\,\x} = \lambda_{\max}  \quad \textrm{and} \quad
  \min_{\x^H\,\x=1} {\x^H B\, \x} = \min_{\x\ne 0} \frac{\x^H B\, \x}{\x^H\,\x} = \lambda_{\min},
\end{equation*}
\end{fact}

\noindent {\it Proof}\/:   The fact that the max of the unnormalized expression over all $\x$ of unit norm is equal to the
max of the normalized expression over all $\x\ne 0$ follows immediately by straightforward scaling arguments.
To see their relation to $\lambda_{\max}$, decompose $\x=\sum_{i=1}^{n} \chi_{i} \s^{i}$ where
$\lambda_{i}$ and $\s^{i}$ denote the (real) eigenvalues and (orthonormal) eigenvectors of $B$ (see Fact \ref{fact.A.D.D.B}).
Then $B\x=\sum_{k=1}^{n}\chi_{k}\lambda_{k}\s^{k}$ and
\begin{equation*}
    \max_{\x^H\,\x=1} {\x^H B\, \x} = \max_{\chib^H\,\chib=1} \Big(\sum_{i=1}^{n}\chi_{i}\s^{i}\Big)^{H}
    \Big(\sum_{k=1}^{n}\chi_{k}\lambda_{k}\s^{k}\Big) = \max_{\chib^H\,\chib=1} \sum_{i=1}^{n} |\chi_{i}|^{2} \lambda_{i} = \lambda_{\max}.
\end{equation*}
An analogous proof follows for the statement of the minimum. \endproof \medskip

The final step in the equation shown above is easily understood algebraically.  
The sum may be interpreted as a linear combination of the several eigenvalues $\lambda_i$ of $B$, each with
a non-negative coefficient $c_i=|\chi_{i}|^{2}\ge 0$, where the sum of these coefficients $c_i$ is unity.
To maximize this sum, the coefficient corresponding to $\lambda_{\max}$ must be one and the other coefficients zero, whereas
to minimize the sum, the coefficient corresponding to $\lambda_{\min}$ should be one and the other coefficients zero.

\subsubsection{Hermitian positive definite and Hermitian positive semidefinite matrices}\label{sec.posdef}

\noindent A Hermitian matrix $A$ with all positive eigenvalues is said to be {\bf Hermitian positive definite}\footnote{The shortened name {\bf positive definite} is sometimes
used synonymously with {Hermitian positive definite} in the complex case and {symmetric positive definite} in the real case.  However, a matrix
with all positive eigenvalues is not necessarily Hermitian or symmetric, and non-Hermitian / non-symmetric positive definite matrices, though rare,
are occasionally encountered.  Thus, this text will avoid the use of this shortened name.}, which
is commonly denoted with the abbreviated notation $A>0$.  A Hermitian
matrix $A$ with non-negative eigenvalues is said to be {\bf Hermitian positive semidefinite},
and is commonly denoted $A\ge 0$.  {\bf Hermitian negative definite} and {\bf Hermitian negative semidefinite} matrices are defined in an
analogous fashion.  A Hermitian matrix which might have both positive and negative eigenvalues is said to be {\bf indefinite}.
Note that the names {\bf symmetric positive definite}, etc., are commonly used in the special case of real matrices.
\begin{fact} \label{fact.spd}
If $A\ge 0$, then $\x^{H}A\x \ge 0$ for all $\x$.
If $A>0$, then $\x^{H}A\x>0$ for all $\x \ne 0$.
\end{fact}

\noindent {\it Proof}\/: As $A$ is Hermitian, its eigenvector matrix $S$
is unitary and eigenvalues $\lambda_{i}$ are real (Fact \ref{fact.A.D.D.B}).
Thus, for any $\x$, we may write $\x=S\chib$ (where $\chib\ne 0$ if
$\x\ne 0$).  It follows that
\begin{equation*}
    \x^{H} A \x= \chib^{H} S^{H} A S \chib =
\chib^{H} S^{H} S \Lambda \chib = \chib^{H} \Lambda \chib =
\lambda_{1} |\chi_{1}|^{2} + \lambda_{2} |\chi_{2}|^{2} + \ldots +
\lambda_{n} |\chi_{n}|^{2}.
\end{equation*}
For the case with $A\ge 0$, as all $\lambda_{i}\ge 0$, it follows that
$\x^{H} A \x \ge 0$.\hfill\break For the case with $A> 0$ with $\Vert
\x \Vert>0$, since $\chib\ne 0$ and all $\lambda_{i}>0$, it follows
that $\x^{H} A \x >0$.   \endproof \medskip

\begin{fact} \label{fact.A.D.D.Ea}
If $A\ge 0$, then $\x^{H} B^H A B\x \ge 0$ for all $\x$ and thus $B^H A B \ge 0$.  If $A>0$ and $|B|\ne 0$, then $\x^{H} B^H A B\x > 0$ for all $\x$
and thus $B^H A B > 0$.
\end{fact}

\noindent {\it Proof}\/: The case with $A\ge 0$ follows as in the proof of Fact \ref{fact.spd},
taking $B\x=S\chib$.  The case with $A>0$ follows directly from the case with $A\ge 0$ together
with Property 5 of the determinant and Fact \ref{fact.A.D.C.Be}.\endproof \medskip

Geometrically, as seen in the proof of Fact \ref{fact.spd},
the matrix $A>0$ defines a family of concentric ellipsoids with $J=\x^H A \x = \chib^{H} \Lambda \chib = \textrm{constant}$, where $\x=S\chib$.
The axes of these ellipsoids (in the space of $\x$) are given by the eigenvectors $\s^k$, whereas the extent of these ellipsoids in the direction
of each of these eigenvectors is given by $\sqrt{J/\lambda_k}$.
For any $B>0$, it follows that the matrix $(A+B)$ defines a family of concentric ellipsoids with $J=\x^H (A+B) \x = \textrm{constant}$
that are bigger.  Stated precisely,

\begin{fact} \label{fact.A.D.D.Eb}
If $A> 0$ and $B>0$, then $(A+B)>0$ with $\x^H (A+B) \x > \x^H A \x$ for any $\x\ne 0$.\hfill\break
If $A\ge 0$ and $B\ge 0$, then $(A+B)\ge 0$ with $\x^H (A+B) \x \ge \x^H A \x$ for any $\x$.
\end{fact}

Selecting $\x$ to be zero in some elements and arbitrary in the other elements (denoted $z_1$ through $z_k$), it follows directly from
Fact \ref{fact.spd} that, for any principle submatrix $B$ of a Hermitian positive definite matrix $A$, we may write $\z^H B \z > 0$ for $\z\ne 0$, and thus

\begin{fact} \label{fact.A.D.D.Ef}
Any principle submatrix of a Hermitian positive definite matrix, including its leading and trailing principle submatrices as well as its
individual diagonal elements, is itself Hermitian positive definite.
\end{fact}

Note that the first RC_Gauss transformation $M_1 A$ of the RC_Gaussian elimination procedure (see \S \ref{sec.B.Ab})
applied to a Hermitian positive definite matrix $A$ may be written in the form
\begin{equation}
  A=\begin{bmatrix} \alpha_1 & \v_1^H \\ \v_1 & B_1 \end{bmatrix} =
  \begin{bmatrix} 1 & 0 \\ \v_1/ \alpha_1 & I \end{bmatrix}
  \begin{bmatrix} \alpha_1 & \v_1^H \\ 0 & B_1-\v_1 \v_1^H/ \alpha_1 \end{bmatrix}
  \label{CholeskyIdentitya}
\end{equation}
where, by Fact \ref{fact.A.D.D.Ef}, $\alpha_1>0$ and $B_1>0$.  That is, the RC_Gaussian elimination procedure without pivoting, when applied to a Hermitian positive definite matrix,
will not encounter a zero pivot during the first RC_Gauss transformation.  Defining $\beta_1=\sqrt{\alpha_1}$, this relation may be further decomposed as
\begin{equation}
  A= \begin{bmatrix} \beta_1 & 0 \\ \v_1/ \beta_1 & I \end{bmatrix}
  \begin{bmatrix} 1 & 0 \\ 0 & B_1-\v_1 \v_1^H/ \alpha_1 \end{bmatrix}
  \begin{bmatrix} \beta_1 & \v_1^H/ \beta_1 \\ 0 & I \end{bmatrix} = \tilde G_1\tilde A \tilde G_1^H \quad \Rightarrow \quad
  \tilde G_1^{-1} = \begin{bmatrix} 1/\beta_1 & 0 \\ -\v_1/\alpha_1 & I \end{bmatrix}.
  \label{CholeskyIdentity}
\end{equation}
The above decomposition may be written in the form\footnote{Note that $\tilde G_1 \tilde G_1^H\ne I$, and thus this is {\it not} a similarity transformation
(that is, the eigenvalues of $A$ and $\tilde A$ are different).} $\tilde A=\tilde G_1^{-1} A \tilde G_1^{-H}$ where $A>0$, and thus $\tilde A>0$
by Fact \ref{fact.A.D.D.Ea}.  Since $\tilde A>0$, $\tilde A$ has all positive eigenvalues; thus, since $\tilde A$ has a block diagonal decomposition,
it follows that $A_1=B_1-\v_1 \v_1^H/ \alpha_1$, which is Hermitian, also has all positive eigenvalues, and therefore $A_1>0$.  Noting that \eqref{CholeskyIdentitya}
represents the first step of RC_Gaussian elimination without pivoting, and that the matrix $A_1=B_1-\v_1 \v_1^H/ \alpha_1$ may be decomposed in an identical fashion (representing
the second step of RC_Gaussian elimination without pivoting), we conclude the following:

\begin{fact} \label{fact.A.D.D.Eg}
The RC_Gaussian elimination procedure without pivoting, when applied to a Hermitian positive definite matrix, will not encounter a zero pivot.
\end{fact}

\subsubsection{Hamiltonian and symplectic matrices} \label{sec.A.D.D.B}

Partition $Z$ and $M$ and define $J=J_{2n\times 2n}$ such that
\begin{equation*}
Z = \begin{bmatrix} Z_{11} & Z_{12} \\ Z_{21} & Z_{22} \end{bmatrix}; \qquad
M = \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix}; \qquad
J = \begin{bmatrix} 0 & I \\ -I & 0 \end{bmatrix} \quad \Rightarrow \quad J^{-1} = - J = J^T.
\end{equation*}
The following facts follow immediately from Facts \ref{fact.A.D.A}, \ref{fact.A.D.C.Bd}, and \ref{fact.A.B.A.B}:

\begin{fact}[The Symmetric Root Property] \label{fact.A.D.D.Ec}
If $J^{-1} Z J = -Z^H$ (that is, if $Z$ is {\bf Hamiltonian}),
then for each eigenvalue $\lambda= \lambda_{R}+\imath\,\lambda_{I} \in \lambda(H)$
there is a corresponding eigenvalue $-\bar\lambda = -\lambda_{R}+\imath\,\lambda_{I} \in \lambda(H)$
with the same multiplicity.  Further, if $Z$ is Hamiltonian, then $Z_{22}=-Z_{11}^H$, $Z_{12}=Z_{12}^H$, and $Z_{21}=Z_{21}^H$.
\end{fact}

\begin{fact}[The Reciprocal Root Property] \label{fact.A.D.D.Ed}
If $J^{-1} M J = M^{-H}$ (that is, if $M$ is {\bf symplectic}), then for each eigenvalue $\lambda=R e^{\imath \theta}  \in \lambda(M)$
there is a corresponding eigenvalue $1/\bar\lambda = (1/R) e^{\imath \theta} \in \lambda(M)$
with the same multiplicity.  Further, if $M$ is symplectic, then $M_{11}=M_{22}^{-H}+M_{12} M_{22}^{-1} M_{21}$ and $M_{22}=M_{11}^{-H}+M_{21} M_{11}^{-1} M_{12}$.
\end{fact}

The symmetric root property implies that any Hamiltonian $Z$ has as many eigenvalues in the LHP as it has in the RHP, whereas
the reciprocal root property implies that any symplectic $M$ has as many eigenvalues inside the unit circle as it has outside the unit circle, as illustrated in Figure \ref{fig:symroot}.
\clearpage

\begin{figure}[h!]
\centerline{\psfig{figure=figs/root.sym.eps,width=2.2in}\qquad \qquad \psfig{figure=figs/root.recip.eps,width=2.2in}}
\caption{(left) The symmetric root property: if $H$ is Hamiltonian, then for every eigenvalue in the LHP, there is a
corrresponding eigenvalue in the RHP.
(right) The reciprocal root property: if $M$ is symplectic, then for every eigenvalue inside the unit circle, there is a
corrresponding eigenvalue outside the unit circle.}\label{fig:symroot}
\end{figure}

\subsection{Computing the eigen and RC_Schur decompositions}\label{sec.A.D.E}

\subsubsubsection{Power methods} \label{sec.A.D.E.A}

The simplest iterative technique available to determine the
largest eigenvalue and corresponding eigenvector of a matrix $A$ is the {\bf power method}.
This method initializes $\x^{(0)}\ne 0$ arbitrarily
and repeatedly calculates
\begin{equation}
\x^{(k+1)}=A\x^{(k)}.\label{eq.powermethod}
\end{equation}
For problems in which $A$ is known to have
a complete set of linearly independent eigenvectors (e.g., if $A$ is Hermitian, skew-Hermitian, or the eigenvalues of $A$ are distinct; see
Facts \ref{fact.A.D.D.B}, \ref{fact.A.D.D.C}, and \ref{fact.A.D.D.A}), we may write $\x^{(0)}=S\chib$, where $S$ is the matrix
of eigenvectors of $A$ and $\chib$ is a vector of coefficients; it thus follows from the relation $AS=S\Lambda$ that
\begin{equation*}
\x^{(k)}=A^k \x^{(0)} = A^k S\chib=S\Lambda^k\chib = \chi_1 \lambda_1^k \s^1 + \chi_2 \lambda_2^k \s^2 + \ldots + \chi_n \lambda_n^k \s^n. 
\end{equation*}
If $|\lambda_1|>|\lambda_2|$ and $\chi_1\ne 0$, 
assuming that the remaining eigenvalues are ordered such that $|\lambda_2|\ge|\lambda_3|\ge|\lambda_4|\ge\ldots$,
the magnitude of the term in the direction of $\s^1$
eventually dominates the magnitude of the other terms, and thus $\x^{(k)}$ is, eventually, approximately aligned with $\s^1$.
The rate of convergence is ultimately dominated by the factor by which the 
first term grows faster than the second, that is, by $|\lambda_1|/|\lambda_2|$.
If this factor is relatively small (e.g., if $|\lambda_1|/|\lambda_2|=1.1$), convergence of the algorithm is relatively slow. 

For problems in which $A$ might not have a complete set of linearly independent eigenvectors, the conclusion is the same (again, so long as
$|\lambda_1|>|\lambda_j|$ for $j\ge 2$), but the analysis is slightly more involved.  We know by the RC_Schur decomposition theorem that
the decomposition $A=UTU^H$ always exists, where $U$ is unitary, $T$ is triangular, and the eigenvalues of $A$ appear on the main diagonal
of $T$ in any desired order.
For the sake of analysis, we again select the ordering $|\lambda_1|>|\lambda_2|\ge|\lambda_3|\ge\ldots$  It follows immediately that
\begin{equation*}
\x^{(k+1)}=A\x^{(k)} = UTU^H \x^{(k)} \quad \Rightarrow \quad \y^{(k+1)}=\frac{1}{\lambda_1} T \y^{(k)} =
\begin{pmatrix} 1 & c_{1,2} & \ldots & c_{1,n-1} & c_{1,n} \\ 
                  & \lambda_2/\lambda_1 & \ldots & c_{2,n-1} & c_{2,n} \\
                  &  & \ddots & \vdots & \vdots \\ 
                  &  &  & \lambda_{n-1}/\lambda_1 & c_{n-1,n} \\ 
                0 &  &  &  & \lambda_n/\lambda_1 \\ \end{pmatrix} \y^{(k)}
\end{equation*} 
where $\y^{(k)}=U^H\x^{(k)}/\lambda^k_1$.  We now examine the convergence of the components of $\y^{(k)}$ as $k$ is increased:
\begin{alignat*}{2}
y_n^{(k)} &= \left(\frac{\lambda_n}{\lambda_1}\right) y_n^{(k-1)}=\left(\frac{\lambda_n}{\lambda_1}\right)^k y_n^{(0)} \xrightarrow[k\rightarrow \infty]{} 0 \quad
&&\textrm{since } \left|\frac{\lambda_n}{\lambda_1}\right|<1, \\
y_{n-1}^{(k)} &= \left(\frac{\lambda_{n-1}}{\lambda_1}\right) y_{n-1}^{(k-1)} + c_{n-1,n} y_{n}^{(k-1)} \xrightarrow[k\rightarrow \infty]{} 0 \quad
&&\textrm{since } \left|\frac{\lambda_{n-1}}{\lambda_1}\right|<1 \textrm{ and } y_n^{(k)} \xrightarrow[k\rightarrow \infty]{} 0,\\
\vdots \\
y_{2}^{(k)} &= \left(\frac{\lambda_{2}}{\lambda_1}\right) y_{2}^{(k-1)} + c_{2,3} y_{3}^{(k-1)}+\ldots+c_{2,n} y_{n}^{(k-1)} \xrightarrow[k\rightarrow \infty]{} 0 \quad
&&\textrm{since } \left|\frac{\lambda_{2}}{\lambda_1}\right|<1 \textrm{ and } y_j^{(k)} \xrightarrow[k\rightarrow \infty]{} 0 \textrm{ for } j=3,\ldots,n,\\
y_{1}^{(k)} &= y_{1}^{(k-1)} + c_{1,2} y_{2}^{(k-1)}+\ldots+c_{1,n} y_{n}^{(k-1)} \xrightarrow[k\rightarrow \infty]{} C \quad
&&\textrm{since } y_j^{(k)} \xrightarrow[k\rightarrow \infty]{} 0 \textrm{ for } j=2,\ldots,n.
\end{alignat*}
Thus, $\y^{(k)}\rightarrow C\e^1$ as $k\rightarrow\infty$, and therefore $\x^{(k)}/\lambda_1^k\rightarrow C\u^1$  as $k\rightarrow\infty$.
In other words, unless $C=0$ (it usually is not; for more discussion on this point, see Footnote \ref{footnoteC} on page \pageref{footnoteC}),
$\x^{(k)}$ eventually converges towards the direction of the first RC_Schur vector of $A$ (that is, the eigenvector of $A$ corresponding to $\lambda_1$).
\vskip0.1in

The {\bf inverse power method} is similar to the power method, but instead of marching the iterative equation $\x^{(k+1)}=A\x^{(k)}$, it marches the equation
\begin{equation}
\x^{(k+1)}=A^{-1}\x^{(k)} \quad \textrm{or, equivalently,} \quad A\x^{(k+1)}=\x^{(k)}.\label{eq.inversepowermethod}
\end{equation}
The latter form may be solved efficiently using the RC_Gaussian elimination techniques discussed in \S \ref{chap02}.
Following a similar analysis as for the power method,
noting that the eigenvalues of $A^{-1}$ are the reciprocal of the eigenvalues of $A$ (Fact \ref{fact.A.D.C.Bd}),
in the case of the inverse power method the magnitude of the term in the direction of $\s^n$
eventually dominates the magnitude of the other terms, and thus $\x^{(k)}$ is, for sufficiently large $k$, approximately aligned with $\s^n$.
The rate of convergence is ultimately dominated by the factor by which the 
$n$'th term grows faster than the $(n-1)$'th, that is, by $|\lambda_{n-1}|/|\lambda_{n}|$.
If this factor is relatively small (e.g., $|\lambda_{n-1}|/|\lambda_{n}|=1.1$), convergence of the algorithm is relatively slow.
\vskip0.1in

Finally, the {\bf shifted inverse power method} enormously accelerates this procedure.
With this method, if at the $k$'th iteration an approximation $\mu_k$ of the eigenvalue $\lambda_{j}$ is available, we march the equation
\begin{equation}
\x^{(k+1)}=(A-\mu_k I)^{-1}\x^{(k)} \quad \textrm{or, equivalently,} \quad (A-\mu_k I)\x^{(k+1)}=\x^{(k)}.\label{eq.shiftedinversepowermethod}
\end{equation}
The eigenvalues of $(A-\mu_k I)$ are just shifts of the eigenvalues of $A$ [specifically, they are given by
$(\lambda_i-\mu_k)$ for $i=1,\ldots,n$], and the eigenvectors of $(A-\mu_k I)$ are the same as the eigenvectors of $A$.
Thus, the factor of convergence of shifted inverse power method at each step is given by $|\lambda_{i}-\mu_k|/|\lambda_{j}-\mu_k|$, where $\lambda_{i}$
is the next closest eigenvalue to $\mu_k$ (that is $\lambda_i$ is the eigenvalue which minimizes $|\lambda_{i}-\mu_k|$ for all $i\ne j$).
If $\mu_k$ is a good approximation of $\lambda_{j}$, then this factor is large, and convergence of the scheme to $\s^j$ is quite rapid.
In fact\footnote{In such a problem, a consistently good choice for $\x^{(0)}$ is given by solving $U\w=\e$ for $\w$, then taking $\x^{(0)}=\w/\Vert \w \Vert$, where
$U$ is found via RC_Gaussian elimination with partial pivoting applied to the matrix $(A-\mu_0 I)$
such that $P(A-\mu_0 I)=LU$, and
$\e=\begin{pmatrix} 1 & 1 & \ldots & 1\end{pmatrix}^T$. While other (perhaps, simpler) choices are often just as good,
the reason that this choice is consistently good is that
it is generally found not to be deficient in its component in the $\s^j$ direction.
For further discussion, see Wilkinson (1965).}${}^{,}$\footnote{Note also that, if $\mu_k$ is an accurate approximation of an eigenvalue $\lambda_j$
of the matrix $A$, then the shifted inverse power method solves a problem that is nearly singular.  However,
$\mu_k$ is only an approximation of $\lambda_j$, and finite-precision arithmetic is used in the computations;
thus, it is in fact found that the shifted inverse power method rarely fails in practice.
In those isolated pathological examples in which it might fail, the techniques of \S \ref{sec.A.A.H} (see in particular the second
bullet point of Fact \ref{fact.A.A.H.Aa}) could be used instead, thereby determining the eigenvector $\s^j$ from the exact value of the eigenvalue
$\lambda_j$.},
\beginmylistb
\item if $\mu_0$ is an accurate approximation of an eigenvalue $\lambda_{j}$ obtained by some other method (e.g., via computation of a RC_Schur
decomposition $A=UTU^H$ using the $QR$ method, as developed below), convergence of the shifted inverse power method
to the eigenvector $\s^j$ is obtained in just one or two steps.
\endmylist
Thus, {\it the primary difficulty lies with the computation of the eigenvalues}; once they are obtained, determination of the eigenvectors
via the shifted inverse power method is relatively easy (see Algorithm \ref{alg.RC_ShiftedInversePower}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.RC_ShiftedInversePower,caption=Computation of the eigenvectors and RC_Schur vectors using the shifted inverse power method.]{RCC1.0/chap04/RC_ShiftedInversePower.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Note that $\x^{(k)}$ in \eqref{eq.powermethod}, \eqref{eq.inversepowermethod}, and \eqref{eq.shiftedinversepowermethod} may be normalized at each step in any desired or convenient manner without
altering its rate convergence towards the direction of the desired eigenvector.  To clarify the analysis that follows, we will thus normalize
$\x^{(k)}$ to be of unit norm at each iteration [$(\x^{(k)})^H \x^{(k)}=1$].

Finally, if an accurate approximation $\mu$ of the eigenvalue $\lambda_{n}$ is not available, the remaining question to be addressed is,
at each step $k$ of the shifted inverse power method, how should we select the approximation $\mu_k$ to the eigenvalue $\lambda_{n}$?
Inspired by Fact \ref{fact.Rayleigh.Ritz}, one good choice turns out to be the {\bf Rayleigh quotient shift} $\mu_k = (\x^{(k)})^H A \x^{(k)}$.
The reason this is an appropriate choice is that, as $\x^{(k)}$ aligns closer and closer to the direction of the eigenvector $\s^n$, the expression $A \x^{(k)}$
approaches $\lambda_n \x^{(k)}$, and thus $\mu_k = (\x^{(k)})^H A \x^{(k)}\rightarrow \lambda_n (\x^{(k)})^H \x^{(k)} =  \lambda_n$; that is, $\mu_k$ approaches
the eigenvalue $\lambda_n$.  

By using this expression for $\mu_k$, the shifted inverse power method converges extremely quickly.
Putting it all together, starting from an arbitrary initial vector $\x^{(0)}$ of unit norm,
the shifted inverse power method with Rayleigh quotient shifts is written
\begin{equation}
\begin{aligned}
&\mu_k = (\x^{(k)})^H A \x^{(k)}  \\
&(A-\mu_k I)\w=\x^{(k)} \\
&\x^{(k+1)}=\w/\Vert \w \Vert.
\end{aligned}
\label{shiftedinversepower}
\end{equation}
\clearpage

\subsubsubsection{The block power method and its equivalence to the unshifted $QR$ method} % \label{sec.A.D.E.B}

\noindent Starting from some vector $\x^{(0)}$ and assuming $|\lambda_1|>|\lambda_j|$ for $j\ge 2$,
the power method $\x^{(k+1)}=A\x^{(k)}$ presented above converges to the eigenvector corresponding
to the eigenvalue of $A$ with maximum amplitude. 

Instead of iterating on a single vector, we now extend the power method by iterating on a {\it set} of vectors $\{\u^{1},\u^{2},\ldots,\u^{n}\}^{(k)}$,
lined up as columns in a matrix ${U_k}$.  This extended method scales each vector in the set by $A$, assembling the resulting vectors in the columns of
a matrix $Z$, then orthogonalizes the resulting set of vectors (using, e.g., Modified Gram Schmidt in Algorithm \ref{alg.3.QRmgs}), and repeats.  
Exactly as in the power method, the first vector $(\u^{1})^{(k)}$ of this extended method converges to the eigenvector $\s^1$
corresponding to the eigenvalue $\lambda_1$ of $A$ with
maximum amplitude; this eigenvector is in fact the same as the first RC_Schur vector $\u^1$ of $A$.  By a similar argument\footnote{In the notation
of the discussion in the second paragraph of \S \ref{sec.A.D.E}, defining $\y^{(k)}$ in this case as $\y^{(k)}=U^H\x^{(k)}/\lambda^k_2$,
it is found that $\y^{(k)}\rightarrow C\e^2$ as $k\rightarrow\infty$, and therefore $\x^{(k)}/\lambda_2^k\rightarrow C\u^2$  as
$k\rightarrow\infty$.} to that given in the second paragraph of \S \ref{sec.A.D.E}, assuming now that $|\lambda_2|>|\lambda_3|$,
the next vector in the set $(\u^{2})^{(k)}$, which is constrained at each iteration to be orthogonal to the first vector in the set $(\u^{1})^{(k)}$,
converges to the second RC_Schur vector $\u^2$ of $A$, corresponding to the eigenvalue of $A$ with the second largest amplitude, etc.
We now consider the case in which we work with $n$ vectors, which, for simplicity, we initialize as the Cartesian unit vectors.  
Subject to only mild assumptions\footnote{The first assumption on the strict separation of the magnitudes of the eigenvalues
is in fact somewhat restrictive (especially in the case of real matrices with eigenvalues in complex conjugate pairs),
and will thus be relaxed when shifting is applied in later sections.  Note that, in fact, {\it repeated} eigenvalues
$\lambda_i=\lambda_{i+1}=\ldots=\lambda_{i+m}$
present no obstacle to this method, as the method will simply converge to $m$ orthogonal vectors spanned by $\{\u^i,\u^{i+1},\ldots,\u^{i+m}\}$;
in this setting, any such $m$ vectors will work.}$^{,}$\footnote{The \label{footnoteC} second assumption is generally not as strict as it may first appear, as numerical errors
usually work in our favor as the algorithm proceeds to introduce
small components in these directions even if they are initially lacking.  Convergence of this method, once the acceleration techniques
presented later in this section are applied, is so rapid that such initialization of the necessary components
via small numerical errors is, in fact, entirely adequate.} (specifically, that $|\lambda_1|>|\lambda_2|>|\lambda_3|>\ldots$ and
also that the initialization chosen
for each column does not happen to be lacking in the necessary components in each of the corresponding RC_Schur vector
directions),
this method converges to a RC_Schur decomposition of $A$.  In equations, this method,
called the {\bf block power method}, is defined by
\begin{subeqnA}
\label{blockpower}
\begin{align}
& {U}_{0}=I, \label{blockpower;a}\\
& Z=A{U}_{k-1}, \label{blockpower;b}\\
& Z={U}_{k} {R}_{k} \quad \textrm{(that is, determine a $QR$ decomposition of $Z$)}.\label{blockpower;c}
\end{align}
Following the analysis of Trefethen \& Bau (1997), we consider this method together with the supplemental matrices
\begin{align}
T_{k}&={U}_{k}^H A {U}_{k},\label{blockpower;d}\\
{\widetilde R}_{k}&=R_{k} R_{k-1} \cdots R_{1}.  \label{blockpower;e}
\end{align}
\end{subeqnA}

\begin{fact} \label{fact.blockpower}
The ${U}_{k}$ and ${\widetilde R}_{k}$ defined by \eqref{blockpower}
form a $QR$ decomposition of the $k$'th power of $A$, that is,
\begin{subeqnA}
\label{result}
\begin{align}
A^k &= {U}_{k} {\widetilde R}_{k}, \label{result;a} \\
\intertext{and the matrix $T_{k}$ defined by \eqref{blockpower} is
unitarily similar to $A$ via the transformation matrix ${U}_{k}$, that is,}
A &= {U}_{k} T_{k} {U}_{k}^H. \label{result;b}
\end{align}
\end{subeqnA}
\end{fact}
\enlargethispage{3pt}

\noindent {\it Proof}\/: 
First note that \eqref{result;b} is identical to \eqref{blockpower;d}, so \eqref{result;b} is verified immediately.  Note also
that \eqref{blockpower} implies that $A^0={U}_{0}={\widetilde R}_{0}=I$ and $T_{0}=A$, and thus \eqref{result;a}
is verified immediately for the base case $k=0$.  Now, assuming \eqref{result;a} is true for the case $k-1$,
it follows directly from the relations in \eqref{blockpower} that it must also be true for the case $k$:
\begin{equation*}
   A^k = A A^{k-1} = A {U}_{k-1} {\widetilde R}_{k-1} = Z {\widetilde R}_{k-1}
   = {U}_{k} {R}_{k} {\widetilde R}_{k-1} = {U}_{k} {\widetilde R}_{k},
\end{equation*}
thereby proving Fact \ref{fact.blockpower} by induction. \endproof

Now consider the {\bf unshifted $QR$ method} defined by
\begin{subeqnA}
\label{QR}
\begin{align}
&T_{0}  =A, \label{QR;a}\\
&T_{k-1}={Q}_{k} {R}_{k} \quad \textrm{(that is, determine a $QR$ decomposition of $T_{k-1}$)},\label{QR;b}\\
&T_{k}  = {R}_{k} {Q}_{k},\label{QR;c}
\end{align}
which we analyze here together with the supplemental matrices
\begin{align}
{U}_{k}&= Q_{1} Q_{2} \cdots Q_{k} \label{QR;d}\\ 
{\widetilde R}_{k}&=R_{k} R_{k-1} \cdots R_{1}.  \label{QR;e}
\end{align}
\end{subeqnA}

\begin{fact} \label{fact.unshiftedQR}
The ${U}_{k}$, ${\widetilde R}_{k}$, and $T_{k}$ defined by \eqref{QR}
also satisfy the relations given in \eqref{result;a} and \eqref{result;b}.
\end{fact}

\noindent {\it Proof}\/: 
First note that \eqref{QR} implies that $A^0={U}_{0}={\widetilde R}_{0}=I$ and $T_{0}=A$, and thus \eqref{result;a} and  \eqref{result;b}
are verified immediately for the case $k=0$.  Now, assuming \eqref{result;a} and \eqref{result;b} are true for the case $k-1$,
it follows directly from the relations in \eqref{QR} that they must also be true  for the case $k$:
\begin{equation*}
  A^k = A A^{k-1} = A {U}_{k-1} {\widetilde R}_{k-1} =  {U}_{k-1} T_{k-1} {\widetilde R}_{k-1} 
      = {U}_{k-1} {Q}_{k} {R}_{k} {\widetilde R}_{k-1} = {U}_{k} {\widetilde R}_{k}
\end{equation*}
and
\begin{equation*}
  T_{k-1} = {U}_{k-1}^H A {U}_{k-1} \quad \Rightarrow \quad
  {Q}_{k}^H [ {Q}_{k} {R}_{k} = {U}_{k-1}^H A {U}_{k-1} ] {Q}_{k} \quad \Rightarrow \quad
  {R}_{k} {Q}_{k} = T_{k} = {U}_{k}^H A {U}_{k},
\end{equation*}
thereby proving Fact \ref{fact.unshiftedQR} by induction. \endproof \medskip

Together, Facts \ref{fact.blockpower} and \ref{fact.unshiftedQR} establish that the two iteration schemes \eqref{blockpower}
and \eqref{QR} are equivalent\footnote{That is, the block power method \eqref{blockpower;a}-\eqref{blockpower;c} determines a $U_k$ (an orthogonalization of the columns of the $k$'th power of $A$),
from which the corresponding $T_k$ may be extracted according to $T_k=U_k^H A U_k$, whereas the unshifted $QR$ method \eqref{QR;a}-\eqref{QR;c} determines an essentially equivalent
value of $T_k$ directly.}. \vskip0.1in

\subsubsubsection{Accelerating the $QR$ method: preliminary comments} %\label{sec.A.D.E.E}

\noindent As shown above, the block power method \eqref{blockpower;a}-\eqref{blockpower;c} ultimately converges to a RC_Schur decomposition of $A$; thus, the unshifted
$QR$ method \eqref{QR;a}-\eqref{QR;c}, which is equivalent (as established in Facts \ref{fact.blockpower} and \ref{fact.unshiftedQR}), converges in an identical manner.
Both methods, though elegant in their simplicity, are very slow to converge.
However, leveraging various facts established above, we are now in an exceptional position to accelerate tremendously the $QR$ method in particular via the following
four steps:\vskip0.1in

\noindent (i)
Note first that, before starting the $QR$ iterations to approximate the RC_Schur decomposition $A=UTU^H$, thereby determining (in the diagonal elements of $T$)
the eigenvalues of $A$, we actually don't have to start this iteration from ``scratch'' (that is, with the full matrix $A$), as indicated in
\eqref{QR;a}.  Indeed, we have already identified an efficient technique to introduce {\it many} zeros into $A$ by reducing it all the way
to RC_Hessenberg form $T_0$ via a unitary similarity tranformation in a finite number of steps (see \S \ref{sec.A.D.A}).
By Fact \ref{fact.A.D.A}, the $T_0$ so computed has the same eigenvalues as $A$.
Thus, {the RC_Hessenberg form $T_0$ provided by Algorithm \ref{alg.3.RC_Hessenberg} is the preferred starting point for the $QR$ method.}
Further, as easily verified, if $T_{k-1}$ is upper RC_Hessenberg, then, calculating its $QR$ decomposition $T_{k-1}={Q}_{k} {R}_{k}$
via Algorithm \ref{alg.3.RC_QRGivensHessenberg} or \ref{alg.3.RC_QRFastGivensHessenberg},
${Q}_{k}$ is also upper RC_Hessenberg.  As ${R}_{k}$ is upper triangular by construction, the product $T_{k} = {R}_{k} {Q}_{k}$
is also upper Hessenenberg (Fact \ref{fact.matrixproducts}).  Thus, the upper RC_Hessenberg structure of $T_{k}$ may be leveraged at each step $k$, both when computing the
decomposition $T_{k-1}={Q}_{k} {R}_{k}$ and when calculating the product $T_{k} = {R}_{k} {Q}_{k}$.  It is thus seen that
{\it the role of the $QR$ iterations is simply to diminish the elements in the first subdiagonal of the upper RC_Hessenberg matrix $T_{k}$ to (nearly) zero},
thereby reducing it (iteratively) towards an eigenvalue-revealing RC_Schur form.\vskip0.1in

\noindent (ii)
The second step in the refinement of the unshifted $QR$ method \eqref{QR} is to note that the shifting idea discussed previously may be applied to
accelerate the convergence of one of the RC_Schur vectors being calculated (typically, the last in the set).  
Note that, in the case of the unshifted $QR$ method, we may write
\begin{equation*}
  T_{k-1} = (Q_k R_k) Q_k Q_k^H = Q_k (R_k Q_k) Q_k^H = Q_k T_k Q_k^H.
\end{equation*}
Thus, $T_k$ is unitarily similar to $T_{k-1}$.  In a similar fashion, if we consider the {\bf shifted QR method} defined by
\begin{subeqnA}
\label{shQR}
\begin{align}
& T_{0}= \textrm{RC_Hessenberg form derived from a unitary similarity decomposition of $A$ via Algorithm \ref{alg.3.RC_Hessenberg}}, \label{shQR;a}\\
& (T_{k-1}-\mu_k I) = {Q}_{k} {R}_{k} \quad \textrm{[that is, determine a $QR$ decomposition of $(T_{k-1}-\mu_k I)$]},\label{shQR;b}\\
& T_{k} = {R}_{k} {Q}_{k} + \mu_k I,\label{shQR;c}
\end{align}
\end{subeqnA}
then it follows similarly that
\begin{equation}
  T_{k-1} = (Q_k R_k + \mu_k I) Q_k Q_k^H = Q_k (R_k Q_k + \mu_k I) Q_k^H = Q_k T_k Q_k^H.
\label{eq:shQRsingle}
\end{equation}
Thus, even when such shifts are applied, $T_k$ is unitarily similar to $T_{k-1}$.  It follows in turn that $T_k$ is unitarily similar to the original matrix $A$.
If good shifts $\mu_k$ are selected (a topic that is deferred to the following three subsections),
then the convergence of one of the eigenvalues [on the main diagonal of $T_k$] and the corresponding RC_Schur vector
[which may be reconstructed via \eqref{QR;d}, if desired] is immensely accelerated,
as explained in the case of the shifted inverse power method above\footnote{Recall that the convergence of the $QR$ method is identical to the convergence of the
corresponding block power method.}.\vskip0.1in

\noindent (iii)
The third step in the refinement of the $QR$ method is to apply Fact \ref{fact.A.D.C.A} regarding the eigenvalues of a block upper triangular matrix.
When one or more of the elements in the first subdiagonal of the upper RC_Hessenberg matrix $T_{k}$ is reduced to (nearly) zero, then $T_{k}$ may be partitioned in
block upper triangular form, and the eigenvalues of the upper RC_Hessenberg blocks on the main diagonal of $T_{k}$ may be determined separately; the eigenvalues
of $T_{k}$ are then given by the union of the eigenvalues of these upper RC_Hessenberg blocks.  This process of splitting the eigenvalue computation
into smaller subproblems is referred to as {\bf deflation}.

To achieve deflation efficiently in all of the cases we will consider, avoiding recursion and its associated overhead,
consider a $3\times 3$ block upper triangular partitioning of $T$ at each iteration.
Denote by $T_{11}$, $T_{22}$, and $T_{33}$ the three square blocks on the main diagonal in
this partitioning.  Select $T_{33}$ to be as large as possible
while being upper triangular; that is, $T_{33}$ contains all eigenvalues already determined
in the lower-right corner on the main diagonal of $T$.  Then, select $T_{22}$
to be as large as possible while still being {\bf unreduced} (that is, with
no zero elements on its subdiagonal); $T_{11}$ contains the remaining elements in the upper-left corner of $T$.
Then, simply apply a shifted $QR$ iteration to $T_{22}$, and repeat the entire process until $T_{22}$ is empty.\label{par:T22isolation}\vskip0.1in

\noindent (iv)
Finally, note that, to save computational effort, we do not bother accumulating the transformation matrices $U_k$ that
complete the (nearly) triangularizing unitary similarity transformation $A=U_k T_k U_k^H$, as the computations to determine this transformation matrix
during the iteration are relatively expensive.  
Once $T_k$ converges to an essentially triangular form (and, thus, the $\lambda_i$ are determined), computing the corresponding eigenvectors (that is, the columns of $S$) via the shifted inverse power method discussed previously is straightforward.
If it is the corresponding RC_Schur vectors (that is, the columns of $U$) that are desired, a straightfoward modification of the shifted inverse power method discussed previously may be used,
subtracting off the components of $\x^{k}$ at each iteration $k$ in the directions of each the previously computed RC_Schur vectors (see Algorithm \ref{alg.RC_ShiftedInversePower}). \vskip0.08in

\noindent To summarize, to calculate the eigenvalues of a matrix $A$:
\beginmylistb
\item[(a)] Start by taking the RC_Hessenberg decomposition using Algorithm \ref{alg.3.RC_Hessenberg}.
\item[(b)] Identify the unreduced block $T_{22}$ on the main diagonal of a block upper triangular partitioning of $T$.
\item[(c)] Apply the shifted $QR$ method to $T_{22}$, and repeat from step (b) until $T_{22}$ is empty. 
\item[(d)] Compute the RC_Schur vectors or eigenvectors, as necessary, using Algorithm \ref{alg.RC_ShiftedInversePower}.
\endmylist \vskip0.08in

\clearpage
\noindent To clarify the details of the acceleration of the $QR$ algorithm, we isolate below three special cases:
\beginmylistb
\item general (non-Hermitian complex) matrices,
\item Hermitian (complex) and symmetric (real) matrices, and
\item non-symmetric real matrices.
\endmylist

\subsubsubsection{Accelerating the $QR$ method: the general (non-Hermitian complex) case} %\label{sec.A.D.E.T}

\noindent In this case, we may follow the $QR$ method with the four acceleration steps enumerated above without further embellishment,
as illustrated in Algorithm \ref{alg.3.RC_EigGeneral}.
At each iteration $k$, we may shift simply by the lower-right element of $T_k$, which is usually the first eigenvalue to converge.

Following the analysis of Golub \& van Loan (1996), if at one step of the shifted $QR$ method the matrix $T_{k-1}$ has the form
\begin{subequations}
  \label{eq:convshiftquad}
\begin{equation}
  T_{k-1} = \begin{pmatrix} x & x & x & x & x \\ x & x & x & x & x \\ 0 & x & x & x & x \\ 0 & 0 & x & x & x \\ 0 & 0 & 0 & \epsilon & \mu_k \end{pmatrix},
  \label{eq:convshiftquad;a}
\end{equation}
where $x$ denotes nonzero entries, $\epsilon\ll 1$, and the subsequent shift is denoted $\mu_k$, then it follows from \eqref{shQR;b}-\eqref{shQR;c} (see Exercise \ref{ex:04.convshiftquad}) that
\begin{equation}
  T_{k} = \begin{pmatrix} x & x & x & x & x \\ x & x & x & x & x \\ 0 & x & x & x & x \\ 0 & 0 & x & x & x \\ 0 & 0 & 0 & \delta & \mu_{k+1} \end{pmatrix} 
    \label{eq:convshiftquad;b}
\end{equation} 
\end{subequations}
where $\delta \propto \epsilon^2$; that is, {\it convergence of the shifted $QR$ method is quadratic}.
\vskip0.1in

\subsubsubsection{Accelerating the $QR$ method: the Hermitian/symmetric case} %\label{sec.A.D.E.U}

\noindent We now consider the case in which $A$ is Hermitian (or, if it happens to be real, symmetric).  Note first that, in this case, $A$ has
real eigenvalues.  The first step of \eqref{shQR} is to compute the RC_Hessenberg decomposition of $A$.
Note that, since the RC_Hessenberg form $T_0$ is unitarily similar to $A$ (see Fact \ref{fact.A.D.B}), $T_0$ is also Hermitian.
Thus, as $T_0$ is both RC_Hessenberg and Hermitian, it must be {\it tridiagonal}.

Further, as easily verified, if $T_{k-1}$ is tridiagonal then, calculating its shifted $QR$ decomposition $(T_{k-1}-\mu_k I)={Q}_{k} {R}_{k}$
via Algorithm \ref{alg.3.QRFastGivensTridiagonal}, it turns out that ${R}_{k}$ is upper tridiagonal.
By Fact \ref{fact.matrixproducts}, $T_k$ is upper RC_Hessenberg.
Additionally, since $T_k$ is unitarily similar to the Hermitian matrix $T_0$ [see \eqref{result;b}],
$T_{k}$ in this case is also Hermitian.  Thus, as $T_{k}$ is both RC_Hessenberg and Hermitian, it is also tridiagonal.
Thus, the tridiagonal structure of $T_{k}$ may be leveraged at each step $k$, both when computing the
decomposition $(T_{k-1}-\mu_k I)={Q}_{k} {R}_{k}$ and when calculating $T_{k} = {R}_{k} {Q}_{k} + \mu_k I$.  Again,
the role of the $QR$ iterations is simply to diminish the elements in the subdiagonal of the Hermitian tridiagonal matrix
$T_{k}$ to (nearly) zero, thereby reducing it to an eigenvalue-revealing diagonal form.

We could again shift by the lower-right element of $T$ at each step.  A better choice for the shift is given by the eigenvalue of the $2\times 2$ submatrix in the
lower-right corner of $T$ which is closest to the element in its lower-right corner.  In the present notation, this $2\times 2$ submatrix is denoted
\begin{equation*}
  T_{k-1}(n-1:n,n-1:n) = \begin{pmatrix} b_{n-1} & \bar a_{n} \\ a_{n} & b_n \end{pmatrix},
\end{equation*}
and the corresponding eigenvalue in question, known as the {\bf Wilkenson shift}, is given by
\begin{equation}
  \mu_k = b_n + t - \textrm{sgn}(t)\,\sqrt{|t|^2 + |a_n|^2} \quad \textrm{where} \quad t=(b_{n-1}-b_n)/2.
\end{equation}
Implementation is given in Algorithm \ref{alg.3.RC_EigHermitian}.
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplainB[frame=single,label=alg.3.RC_EigGeneral,caption={Compute the eigenvalues of a general complex matrix $A$ % with $|\lambda_1|>|\lambda_2|>\ldots$
via RC_Hessenberg decomposition followed by an explicitly shifted $QR$ iteration, using the lower-right element for each shift.}]{RCC1.0/chap04/RC_EigGeneral.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_EigGeneralTest.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsubsection{Accelerating the $QR$ method: the non-symmetric real case} % \label{sec.A.D.E.V}

\noindent The non-symmetric real case is more delicate.  In this case, any complex eigenvalues come in complex conjugate pairs (Fact \ref{fact.A.D.C.Bf}).
We may again begin by computing the RC_Hessenberg decomposition, which is real because the initial matrix $A$ is real.  Neither of the approaches described above, however, is suitable:
\beginmylistb
\item The approach of shifting each $QR$ iteration by the element in the lower-right corner of $T$, as done in the general (non-Hermitian complex) case above, is
not a good idea, because all elements during the $QR$ iterations are real following this approach, though some of the eigenvalues we seek come in complex conjugate pairs.
Thus, the (real) element in the lower-right corner of $T$ at any given step
will be {\it exactly} equidistant from the two complex eigenvalues, and shifting by it will not distinguish one eigenvector from the other.  Stated another way,
returning to the original description of the power method at the beginning of \S \ref{sec.A.D.E}, we do not get the necessary separation of the absolute value of the eigenvalues in the shifted problem
in order to ensure convergence of the corresponding RC_Schur vectors when we use a real shift when the eigenvalues come in complex conjugate pairs. 
\item The approach of shifting each $QR$ iteration by one of the eigenvalues of the $2\times 2$ submatrix in the
lower-right corner of $T$ would in fact work.  However, this approach unnecessarily converts a real problem into a significantly more expensive complex one.  Furthermore, round-off errors accumulate following this approach
such that the eigenvalues do not wind up being exact complex conjugates of each other.
\endmylist

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplainB[frame=single,label=alg.3.RC_EigHermitian,caption={Compute the eigenvalues of a Hermitian (if real, symmetric) matrix $A$ % with $|\lambda_1|>|\lambda_2|>\ldots$
via RC_Hessenberg decomposition followed by an explicitly shifted $QR$ iteration, using Wilkenson shifts.}]{RCC1.0/chap04/RC_EigHermitian.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_EigHermitianTest.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent An alternative approach is thus preferred.  Instead of aiming to use a shifted $QR$ method to iterate towards a complex upper triangular form,
we instead use a {\bf double-shift} method to iterate towards an eigenvalue-revealing {\it block} upper triangular form with $1\times 1$ and $2\times 2$ real blocks on the
main diagonal, known as the {\bf real RC_Schur form} (see \S \ref{sec.A.D.Ca}).  At each step, we will take {\it two} shifted $QR$ steps, one based each of the eigenvalues of the $2\times 2$ submatrix in the
lower-right corner of $A$.  The challenge is to figure out a way to do this while keeping all arithmetic real.  To accomplish this, we appeal to the following.

\begin{fact}[The Implicit $Q$ Theorem] \label{fact.implicitQ}
Consider two unitary similarity transformations of $T_{n\times n}$ such that\break $T = U G U^H = V H V^H$, where both $G$ and $H$ are upper RC_Hessenberg and $G$ is {\bf unreduced} (that is, it has no zeros
in its subdiagonal).  If $\v^1=\u^1$ (that is, if the first columns of $V$ and $U$ are equal), then it follows that $\v^i=\pm\u^i$ and $h_{i,i-1}=\pm G_{i,i-1}$ (stated loosely, the two decompositions
are ``essentially equivalent'').
\end{fact}

\noindent {\it Proof}\/: Define a unitary $W=V^H U$ and note that $HW = WG$.   It follows that, for $2\le i\le n$,
\begin{equation*}
H\w^{i-1} = \sum_{j=1}^{i-1} \w^j \cdot g_{j,i-1}  + \w^i \cdot g_{i,i-1} \quad \Rightarrow \quad \w^i = \Big( H\w^{i-1} - \sum_{j=1}^{i-1} \w^j \cdot g_{j,i-1} \Big)/ g_{i,i-1}.
\end{equation*}
Since $\v^1=\u^1$ and $U$ and $V$ are unitary, it follows from $W=V^H U$ that $\w^1=\e^1$ (that is, if the first columns of $W$ is the first Cartesian unit vector).
It thus follows from the above equation that $W$ is upper triangular; since it is also unitary, it follows that $W=\textrm{diag}(1,\pm 1,\pm 1,\ldots)$.
Since $\w^i=V^H \u^i$ and $g_{i,i-1}=(\w^i)^H H \w^{i-1}$, it also follows that $\v^i=\pm \u^i$ and $g_{i,i-1}=\pm h_{i,i-1}$.  \endproof

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\lstinputlistingplainC[frame=single,label=alg.3.RC_EigReal,caption={Compute the eigenvalues of a real nonsymmetric matrix $A$ % with
via RC_Hessenberg decomposition followed by a double implicitly shifted $QR$ iteration, using the eigenvalues of the $2\times 2$ submatrix in the
lower-right corner of $T_k$ as shifts.}]{RCC1.0/chap04/RC_EigReal.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_EigRealTest.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Leveraging the Implicit $Q$ Theorem, we can now develop a method based on real arithmetic to perform two shifted $QR$ steps based on the shifts $a_1$ and $a_2$
[given by the eigenvalues of $T(n-1:n,n-1:n)$, which are either real or a complex conjugate pair].
Noting \eqref{shQR}, these two shifted $QR$ steps may be written
\begin{subequations}
\begin{equation}
(T-a_1 I) = {Q}_{1} {R}_{1}; \qquad
T_{1} = {R}_{1} {Q}_{1} + a_1 I; \qquad
(T_{1}-a_2 I) = {Q}_{2} {R}_{2}; \qquad
T_{2} = {R}_{2} {Q}_{2} + a_2 I.
\end{equation}
Pre- and post-multiplying the third relation by $Q_1$ and $R_1$ respectively, it is straightforward to show that
\begin{equation}
(Q_1 Q_2) (R_2 R_1) = (T - a_1 I)(T - a_2 I) = T^2 - (a_1+a_2) T + (a_1\cdot a_2) I \triangleq M.
\end{equation}
\end{subequations}
Note that $t\triangleq a_1+a_2$ is real\footnote{By Fact \ref{fact.A.C.A.B}, $t$ is the sum of the eigenvalues of $T_{k-1}(n-1:n,n-1:n)$.  This statement is generalized in Fact \ref{fact.A.F.A.b}.} and
$d\triangleq a_1\cdot a_2$ is real\footnote{By Fact \ref{fact.A.D.C.Be}, $d=|T_{k-1}(n-1:n,n-1:n)|$ is the product of the eigenvalues of $T_{k-1}(n-1:n,n-1:n)$.}; thus, $M$ is real.

Recall from \eqref{eq:shQRsingle} that the shifted $QR$ method boils down to $T_k=Q_k^H T_{k-1} Q_k$; that is, the upper RC_Hessenberg matrix $T_k$ is found simply by pre-multiplying $T_{k-1}$ by
a unitary matrix $Q_k^H$ and post-multiplying the result by $Q_k$, where $Q_k$ itself is found by orthogonalization of the columns of a shifted version of $T_{k-1}$.
The idea now is similar, but instead of transforming with a single $Q_k$, we
will effectively pre-multiply by $U^H\triangleq (Q_2^H Q_1^H)$ and postmultiply by $U\triangleq (Q_1 Q_2)$, which is why this method is said to be based on a {\bf double shift}.

Unfortunately, computation of $M=T^2 - (a_1+a_2) T + (a_1\cdot a_2) I$ requires the computation of $T^2$, which is prohibitively expensive for large matrices.  Thus, we cannot simply
compute $M$, orthogonalize its columns (denoting the resulting matrix $U$), then compute $U^H T U$.  Appealing to the Implicit $Q$ Theorem, however, we may instead build up a new unitary
matrix $V$ (from a series of appropriately-constructed Householder transformation matrices embedded within identity matrices) to form a new unitary similarity transformation $V^H T V=H$.
This new unitary similarity transformation will be ``essentially equivalent'' to the similarity transformation $U^H T U =G$ discussed above so long as the {\it first column} of $U$, denoted $\u^1$, matches the first column of $V$, denoted $\v^1$,
and the rest of the transformation $V$ is constructed such that $H$ is upper RC_Hessenberg (as is $G$).  Thus, we perform an {\bf double implicitly shifted $QR$ iteration},
achieving essentially the same effect as two shifted $QR$ iterations but without ever explicitly performing the shifts $(T-a_1 I)$ or $(T_1-a_2 I)$ [cf.~Algorithms \ref{alg.3.RC_EigGeneral} and
\ref{alg.3.RC_EigHermitian}].  For matrices whose eigenvalues span a large range of magnitudes, this approach leads to a significant improvement in overall accuracy (see also Exercise
\ref{ex:04.impshiftherm}).

To proceed, it is first noted that $\m^1 = \begin{pmatrix} x & y & z & 0 & \ldots & 0 \end{pmatrix}$ where
\begin{equation*}
x=t_{11}^2 + t_{12} t_{21} - t t_{11} + d, \qquad y=t_{21}(t_{11}+t_{22} - t), \qquad z=t_{21} t_{32}.
\end{equation*}
We will construct $V$ via a series of Householder transformation matrices, $V=V_0 V_1 \cdots V_{n-2}$, where
\begin{equation*}
V_k=\textrm{diag}[ I_{k\times k},\,H^k_{3\times 3},\, I_{(n-k-3)\times (n-k-3)} ]; \quad \textrm{for example,} \quad
V_1={\footnotesize \begin{pmatrix} 1 &  &  &  &  & 0 \\  & x & x & x &  &  \\  & x & x & x &  &  \\  & x & x & x &  &  \\  &  &  &  & 1 &  \\ 0 &  &  &  &   & 1 \end{pmatrix}}.
\end{equation*}
Note that all of the $V_k$ matrices are $\e^1$ in the first column except $V_0$; thus, $V$ and $V_0$ have identical first columns.
Therefore, all we need to do is to determine $H^0_{3\times 3}$ as the upper-left block of the first Householder transformation matrix in the $QR$ decomposition
of $M$ (see Algorithm \ref{alg.3.QRHouseholder}); that is, we construct $H^0_{3\times 3}$, from which $V_0$ is formed, by designing a Householder reflection matrix which rotates the first three components of $\m^1$ into the direction $\e^1$.
We then construct the remaining $V_k$ to return the matrix $H$ to upper triangular form in the construction $V^H T V=H$, effectively ``chasing out'' the nonzero elements introduced into the second and third subdiagonals of the
transformed matrix by $V_0$.  Graphically, this process may be illustrated as follows, denoting by $*$ those elements changed by the most recent pre- and post-multiplications:
\begin{gather*}
  V^H_0 T V_0 = {\footnotesize\begin{pmatrix} * & * & * & * & * & * \\ * & * & * & * & * & * \\ * & * & * & * & * & * \\ * & * & * & x & x & x \\ 0 & 0 & 0 & x & x & x \\ 0 & 0 & 0 & 0 & x & x \end{pmatrix}}, \quad 
  V^H_1 V^H_0 T V_0 V_1 = {\footnotesize\begin{pmatrix} x & * & * & * & x & x \\ * & * & * & * & * & * \\ 0 & * & * & * & * & * \\ 0 & * & * & * & * & * \\ 0 & * & * & * & x & x \\ 0 & 0 & 0 & 0 & x & x \end{pmatrix}}, \\
  V^H_2  V^H_1 V^H_0 T V_0  V_1 V_2 = {\footnotesize\begin{pmatrix} x & x & * & * & * & x \\ x & x & * & * & * & x \\ 0 & * & * & * & * & * \\ 0 & 0 & * & * & * & * \\ 0 & 0 & * & * & * & * \\ 0 & 0 & * & * & * & x \end{pmatrix}}, \quad
  V^H_3 V^H_2  V^H_1 V^H_0 T V_0 V_1 V_2 V_3 = {\footnotesize\begin{pmatrix} x & x & x & * & * & * \\ x & x & x & * & * & * \\ 0 & x & x & * & * & * \\ 0 & 0 & * & * & * & * \\ 0 & 0 & 0 & * & * & * \\ 0 & 0 & 0 & * & * & * \end{pmatrix}}, \\
  V^H T V = {\footnotesize\begin{pmatrix} x & x & x & x & * & * \\ x & x & x & x & * & * \\ 0 & x & x & x & * & * \\ 0 & 0 & x & x & * & * \\ 0 & 0 & 0 & * & * & * \\ 0 & 0 & 0 & 0 & * & * \end{pmatrix}},
\end{gather*}
where $V=V_0 V_1 \cdots V_{n-2}$.  Implementation is relatively straightforward, as shown in Algorithm \ref{alg.3.RC_EigReal}.  Note that two convenient {\bf wrapper} routines, {\tt RC_Eig.m} and {\tt RC_Schur.m},
are given in Algorithm \ref{alg.eig.schur}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\lstinputlistingplain[frame=single,label=alg.eig.schur,caption={Convenient wrapper routines for computing the eigen and RC_Schur decompositions.}]{RCC1.0/chap04/RC_Eig.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_Schur.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsubsection{Updating an eigen decomposition of Hermitian matrix after appending a row/column} % \label{sec.A.D.E.G} ???
%\noindent {\it This section still under construction.} ???
%\subsubsubsection{Updating an eigen decomposition of Hermitian matrix after a rank 1 update} % \label{sec.A.D.E.H} ???
%\noindent {\it This section still under construction.} ???
%\subsubsubsection{The divide and conquer method} %\label{sec.A.D.E.J} ???
%\noindent {\it This section still under construction.} ???
%\subsubsubsection{Krylov methods and the implicitly-restarted RC_Arnoldi method} \label{sec.imp.res.arnoldi} ???

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Jordan decomposition}\label{sec.A.D.H}

As discussed in \S \ref{sec.A.D.D},
a square matrix with a complete set of eigenvectors is similar to a diagonal matrix; to be precise, $A=S\Lambda S^{-1}$, where $S$
is a matrix with the eigenvectors of $A$ as columns, and $\Lambda$ is a diagonal matrix with the corresponding eigenvalues of $A$ on the
main diagonal. The Jordan decomposition $A=MJM^{-1}$ represents, in a way, the closest one can get to a diagonalizing
similarity transformation when the square matrix $A$ does {\it not} have a complete
set of eigenvectors (that is, when the matrix $A$ is {\bf defective}).  The columns of the transformation matrix $M$ involved in this
decomposition include all of the linearly independent eigenvectors of $A$ together with an appropriate number of 
so-called {\bf generalized eigenvectors}, as defined below.

To illustrate, consider an $n\times n$ matrix $A$ with $p$ distinct eigenvalues\footnote{Note that the
$n$ eigenvalues $\lambda_i$ of the matrix $A$ satisfy the characteristic polynomial $|A-\lambda_i I|=0$, as described previously.}
$\lambda_1, \ldots, \lambda_p$ found, e.g., with the $QR$ method discussed previously, where the eigenvalue $\lambda_i$ is assumed
to have multiplicity\footnote{Note also that,
by the Fundamental Theorem of Algebra (Fact \ref{fact.A.C.A.A}), $m_1+m_2+\ldots m_p=n$.} $m_i$.  The Jordan decomposition 
may be built as follows\footnote{Proof that this algorithm succeeds in producing exactly the $w_i$ additional linearly independent vectors
needed for each $i$ is given in Horn \& Johnson 1985.}:
\beginmylistb
\item[1)] Initialize the index $i=1$.
\item[2)] Calculate the number of linearly independent eigenvectors\footnote{Note that $v_i$ is the dimension of the nullspace of
$(A-\lambda_i I)$, and is referred to as the {\bf geometric multiplicity} of $\lambda_i$.\label{foot:geometricmult}}, $v_i=n-\Rank(A-\lambda_i I)$, corresponding to $\lambda_i$, 
then solve\footnote{The eigenvectors $\s^{ij}$ may be determined by parameterizing all solutions to $(A-\lambda_i I)\s^{ij}=0$
using the techniques described in \S \ref{sec.A.A.H}.}\ 
$(A-\lambda_i I)\s^{ij}=0$ for the $v_i$ independent eigenvectors $\s^{ij}$ for $j=1,\ldots,v_i$.
\item[3)] Calculate the number of necessary generalized eigenvectors, $w_i=m_i-v_i$, corresponding to $\lambda_i$.
\item[4)] Initialize $\g^{i,j,0}=\s^{ij}$ for $j=1,\ldots,v_i$.  Also, keep track of
the number of generalized eigenvectors already determined that are related to the $(ij)$'th eigenvector $\s^{ij}$ by initializing $p_{ij}=0$,
for $j=1,\ldots,v_i$.
\item[5)] Initialize the index $j=1$.  
\item[6)] Initialize the index $k=1$. 
\item[7)] If $w_i-(p_{i1}+p_{i2}+\ldots+p_{i v_i})=0$, we have all the generalized eigenvectors related to $\lambda_i$.  Go to step 9.
\item[8)] If $\g^{i,j,(k-1)}\in \Ran(A-\lambda_i I)$, then solve $(A-\lambda_i I)\g^{i,j,k}=\g^{i,j,(k-1)}$ for the generalized
eigenvector $\g^{i,j,k}$, increment $k$ and $p_{ij}$, and repeat from step 7, else increment $j$ and repeat from step 6.
\item[9)] If $i<p$, increment $i$ and repeat from step 2 until finished.
\endmylist
The resulting transformation matrix $M$ is then given by
\setcounter{MaxMatrixCols}{30}
\begin{subeqnA}\label{jordanform}
\begin{equation}
  M=\begin{bmatrix} M^{11} & M^{12} & \ldots & M^{1v_1} &  M^{21} & M^{22} & \ldots & M^{2v_2} & \ldots\ldots &
 M^{p1} & M^{p2} & \ldots & M^{p v_p} \end{bmatrix}
 \label{jordanform;a}
\end{equation}
where $M^{ij}$ contains the eigenvector $\s^{ij}$ and all the generalized eigenvectors $\g^{i,j,k}$ associated with it,
\begin{equation}
M^{ij} = \begin{bmatrix} | & | & | & ~ & | \\ \s^{ij} & \g^{ij1} & \g^{ij2} & \ldots & \g^{ijp_{ij}} \\ | & | & | & ~ & | \end{bmatrix}
\label{jordanform;b}
\end{equation}
and the corresponding block diagonal {\bf Jordan form} $J$ is given by
\begin{equation}
       J=\textrm{diag}[ J^{11}, J^{12}, \ldots, J^{1v_1},  J^{21}, J^{22}, \ldots, J^{2v_2}, \ldots\ldots, J^{p1}, J^{p2}, \ldots, J^{p v_p} ],
       \label{jordanform;c}
\end{equation}
where the {\bf Jordan block} $J^{ij}$ is a $(p_{ij}+1)\times(p_{ij}+1)$ upper bidiagonal Toeplitz matrix of the form
\begin{equation}
    J^{ij}=\begin{pmatrix} \lambda_1 & 1         &    & 0 \\ 
                                     & \ddots & \ddots & \\ 
                                     &        & \lambda_1 & 1 \\
                                0    &        &  & \lambda_1 \end{pmatrix}.
\label{jordanform;d}
\end{equation}
\end{subeqnA}
Multiplying out $AM=MJ$, applying the definitions in \eqref{jordanform}, and examining each column of the resulting equation, we arrive at two types
of relations,  $A\s^{ij}=\lambda_i \s^{ij}$ and $A\g^{i,j,k}=\lambda_i\g^{i,j,k}+\g^{i,j,(k-1)}$ where $\g^{i,j,0}=\s^{ij}$,
which is consistent with the algorithm above defining the eigenvectors $\s^{ij}$ and generalized eigenvectors $\g^{i,j,k}$.
Note that, if the matrix $A$ happens to have $n$ linearly independent eigenvectors, then the Jordan blocks are all $1\times 1$, and the Jordan
decomposition reduces immediately to the eigen decomposition\footnote{That is, $J^{ij}=\lambda_i$ and $M^{ij}=\s^{ij}$, and thus $J=\Lambda$ and $M=S$.}.

A matrix is called {\bf nonderogatory} if it has only one Jordan block associated with each eigenvalue [that is, if $v_i=n-\Rank(A-\lambda_i I)=1$ for every eigenvalue $\lambda_i$ of $A$];
otherwise, it is called {\bf derogatory}.

For the purpose of the numerical solution of large-scale problems in
science, engineering, and elsewhere, computing the
Jordan decomposition is usually a bad idea; the role of this
decomposition is thus de-emphasized in this text.  The central problem with
this decomposition is that, by its definition, the Jordan form does
not depend smoothly on $A$.  For example, consider a $2\times 2$
matrix $A$ with eigenvalues $\lambda_{1}$ and $\lambda_{1}+\eps$ such
that
\begin{equation}
    A=\begin{pmatrix}
     \lambda_{1} & 1 \\ 0 & \lambda_{1}+\eps
    \end{pmatrix} \quad \Rightarrow \quad
    \s^{1}=\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \quad \s^{2}=\begin{pmatrix} 1 \\ \epsilon \end{pmatrix}.
    \label{A1example}
\end{equation}
For $\epsilon=0$, the matrix $A$ is already in Jordan form, and the
linear independence of the two eigenvectors is lost.  In this case,
the Jordan decomposition is simply $A=MJM^{-1}$ with
\begin{equation*}
    J=A=\begin{pmatrix}
	 \lambda_{1} & 1 \\ 0 & \lambda_{1}
	\end{pmatrix}\quad \textrm{and} \quad M=I.
\end{equation*}
However, for an arbitrarily small change of $\epsilon$, the Jordan
decomposition $A=MJM^{-1}$ sudddenly switches to the eigen decomposition such that
\begin{equation*}
	J=\Lambda=\begin{pmatrix} \lambda_{1} & 0 \\ 0 & \lambda_{1}+\eps
	\end{pmatrix} \quad \textrm{and} \quad M=S =\begin{pmatrix} 1 &
	1 \\ 0 & \epsilon \end{pmatrix}.
\end{equation*}
Because of this {\bf ill-posedness} (that is, a lack of smooth
dependence on the data in the problem formulation) in the definition
of the Jordan decomposition, the RC_Schur decomposition and/or the
singular value decomposition is usually be preferred over the Jordan
decomposition for most large-scale numerical computations in cases for which $A$ is, or
might be, nearly defective.

Defective matrices appear often in the study of dynamic systems.  Despite the ill-posedness of the definition of the Jorden decomposition
problem, as described above, deriving such matrices in Jordan form (or transforming them
into Jordan form) is often instructive for the purpose of analysis.  For example, consider the equation for the simple 2D
rotation of a rigid body:
\begin{equation*}
   J \frac{d^2\theta}{dt^2}=\tau. \quad \textrm{Defining} \quad \x=\begin{pmatrix} \theta \\ d\theta/dt \end{pmatrix} \quad \textrm{yields} \quad
   \frac{d\x}{dt} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \x + \begin{pmatrix} 0 \\ 1/J \end{pmatrix} \tau.
\end{equation*}
In the first-order (a.k.a. {\bf state-space}) form shown at right, the system matrix in the governing equation is already in Jordan form
(in this case, with $\lambda_1=0$).  The behavior of such dynamic systems is discussed further in \S \ref{sec.ModalCoordinateForm} and \S \ref{sec:nonorthog}.

\clearpage
\subsection{The singular value decomposition (SVD)}\label{sec.A.D.G}

As mentioned in \S \ref{sec.A.D.H}, if the set of eigenvectors $\s^{\iota}$ of a given matrix $A$ are
not linearly independent, then they cannot be arranged into a
nonsingular matrix $S$, and the matrix $A$ is called defective.
However, it is a remarkable fact that every $m \times n$ matrix $A$
has a {\bf singular value decomposition} (SVD) given by $A=U \Sigma
V^H$, where $U=U_{m\times m}$ and $V=V_{n\times n}$ are unitary 
($U U^H =U^H U=I_{m\times m}$ and $V V^H =V^H V=I_{n\times n}$)
and $\Sigma=\Sigma_{m\times n}$ is diagonal with real, non-negative
elements $\sigma_{i}$ on the main diagonal, arranged in descending
order.  The $\sigma_{i}$ are referred to as the {\bf singular values}
of $A$, and the % first $p=\min(m,n)$
columns of $U$ and $V$ are, respectively, the {\bf left} and {\bf
right singular vectors} of $A$.  {\it The number of nonzero singular
values, denoted by $r$, is the rank of the matrix $A$}; in fact,
numerically, computing the SVD is the most reliable method available
to compute the rank of a matrix.  The maximum singular value of $A$, $\sigma_1$, is often denoted $\sigma_{\textrm{max}}(A)$,
whereas the minimum singular value of $A$ is often denoted $\sigma_{\textrm{min}}(A)$.
Note that, if $A$ is Hermitian positive
semidefinite, then the SVD reduces to the eigen decomposition
with $U=V=S$ and the $\sigma_{i}=\lambda_{i}$.

As with the $QR$ decomposition when $m\ge n$, the SVD has both
complete and reduced forms.  Taking $r$ as the number of nonzero singular
values of $A$ (i.e., the rank of $A$), we may write the SVD in block
matrix form:
\begin{equation}
  A_{m\times n}=U_{m\times m} \,\Sigma_{m\times n} \,V^{H}_{n\times n} = 
  \begin{bmatrix} {\underline U}_{\,m \times r} & {\overline U}_{m \times (m-r)} \end{bmatrix}
  \begin{bmatrix} {\underline{\Sigma}}_{\,r \times r} & 0 \\ 
		  0 & 0 \end{bmatrix}
  \begin{bmatrix} {\underline V}_{\,n \times r} & {\overline V}_{n \times (n-r)} \end{bmatrix}^{H}
      ={\underline U}_{\,m\times r} \,{\underline{\Sigma}}_{\,r\times r} \,({\underline V}_{\,n\times r})^{H}. \label{SVDforms}
\end{equation}
Note in particular that $\underline U^H \underline U=I_{r\times r}$ and $\underline V^H \underline V=I_{r\times r}$,
though $\underline U \underline U^H \ne I_{m\times m}$ and $\underline V \underline V^H \ne I_{n\times n}$.
The SVD is a useful generalization of the eigen decomposition
introduced in \S \ref{sec.A.D.D} that is appropriate for defective and
nonsquare matrices.  Noting that $U$ and $V$ are unitary and applying
Fact \ref{fact.A.A.H.Ag} to the block matrix form of the SVD given
above, it is seen that
\beginmylistb
\item the columns $\underline\u^i$ of the matrix $\underline U$ form an orthogonal basis of the column space of $A$,
\item the columns $\overline\u^i$ of the matrix $\overline U$ form an orthogonal basis of the left nullspace of $A$,
\item the columns $\underline\v^i$ of the matrix $\underline V$ form an orthogonal basis of the row space of $A$, and
\item the columns $\overline\v^i$ of the matrix $\overline V$ form an orthogonal basis of the nullspace of $A$.
\endmylist
Note that, since $A=U \Sigma V^{H}$, it follows that $(A^{H}
A)=V (\Sigma^{H}\Sigma) V^{H}$ and $(A A^{H})=U (\Sigma\Sigma^{H})
U^{H}$.  Recall from Fact \ref{fact.A.D.D.B} that an eigen decomposition
of a Hermitian matrix [such as $(A^{H} A)$ or $(A
A^{H})$] always exists, and from \S \ref{sec.A.D.E} that there is a very efficient numerical algorithm for
computing the eigen decomposition in this case (see Algorithm \ref{alg.3.RC_EigHermitian}).  Thus, this algorithm may be leveraged
to find the singular value decomposition of any $m\times n$ matrix $A$
in a straightforward manner, as shown below.  Note that the
constructions given below also establish the existence of the SVD
itself.  Note also the following:

\begin{fact} \label{fact.A.A.H.D}
    $\Rank(A)=\Rank(A A^{H})=\Rank(A^{H} A)$.  Further, the nonzero
    eigenvalues of $(A A^{H})$ and $(A^{H} A)$ are the square of
    the nonzero singular values of $A$.
\end{fact} 

\noindent {\it Proof}\/: As noted previously, $(AA^{H}) = U ( \Sigma
\Sigma^{H} ) U^{H}$ and $(A^{H}A) = V (\Sigma^{H} \Sigma ) V^{H}$.  Note
that both $(\Sigma \Sigma^{H})$ and $(\Sigma^{H} \Sigma )$ are
diagonal, and thus these equations are in fact SVDs of $(AA^{H})$ and
$(A^{H}A)$.  Note also that the nonzero elements on the diagonals of
$(\Sigma \Sigma^{H})$ and $(\Sigma^{H} \Sigma )$ [that is, the nonzero
singular values of $(AA^{H})$ and $(A^{H}A)$] are identical and equal
to the square of the nonzero singular values of $A$. \endproof
\vskip0.1in

\begin{fact} \label{fact.induced2norm}
The induced 2-norm is given by the {\bf maximum singular value} [i.e., $\Vert A \Vert_{i2}=\sigma_{\textrm{max}}(A)$].
\end{fact}

\noindent {\it Proof}\/:  Square both sides of the first formula in \eqref{matrixpnormdef} and take $p=2$, then apply Facts \ref{fact.Rayleigh.Ritz} and \ref{fact.A.A.H.D}, resulting in
\begin{equation}
\Vert A\Vert_{i2}^2 = \max_{\x\ne 0} \frac{\Vert A \x\Vert^2}{\Vert\x\Vert^2}
= \max_{\x\ne 0} \frac{\x^H (A^H A)\, \x}{\x^H\,\x} = \lambda_{\max}(A^H A) = \lambda_{\max}(A A^H) = \sigma_{\max}^2(A).
\tag*{$\Box$}
\end{equation}

\clearpage

\begin{fact} \label{fact.A.A.H.E}
    The singular values of $A^{-1}$, if it exists (that is, if $r=m=n$),
	are the reciprocal of the singular values of $A$.
\end{fact} 

\noindent {\it Proof}\/: If $A=U\Sigma V^H$ is an SVD of $A$, then (by Fact \ref{fact.A.A.J.C}) $A^{-1}=V^{-H} \Sigma^{-1} U^{-1}$, which itself is
just an SVD of $A^{-1}$.  The singular values of $A^{-1}$ (that is, the diagonal elements of $\Sigma^{-1}$) are thus the reciprocal of 
the singular values of $A$ (that is, the diagonal elements of $\Sigma$).  \endproof \vskip0.1in

\begin{fact} \label{fact.2normconditionnumber}
The 2-norm condition number (see \S \ref{sec.A.F.C}) is given by $\kappa(A)=\Vert A \Vert_{i2}\,\Vert A^{-1} \Vert_{i2}=\sigma_{\textrm{max}}(A)/\sigma_{\textrm{min}}(A)$.
Thus, $\kappa(A)\ge 1$ and $\kappa(A)\rightarrow \infty$ as the matrix $A$ approaches a singular matrix
(that is, as $\sigma_{\textrm{min}}(A)\rightarrow 0$.
\end{fact}

\noindent {\it Proof}\/:  Follows directly from Facts \ref{fact.induced2norm} and \ref{fact.A.A.H.E}. \vskip0.1in

Note that the SVD is not unique; for example, if $A=U_a \,\Sigma_a \,V^{H}_a$ is an SVD, then $A=U_b \,\Sigma_b \,V^{H}_b$ is
also an SVD, where $U_b=-U_a$ and $V_b=-V_a$.

There are three natural constructions of the SVD, as presented below.  Numerically, the third construction is far superior to the other two,
as it doesn't involve multiplying $A$ times itself, which leads to a squaring of the condition number and thus a sometimes significant
loss of accuracy when using finite-precision arithmetic.
However, the other two constructions are useful to be aware of from the perspective of understanding the SVD itself; we thus present all three constructions here.

\subsubsubsection{(i) Construction of the SVD from the eigenvalues and eigenvectors of $(A^{H} A)$} %\label{sec.A.D.G.a}

\noindent The SVD may be constructed as follows:
\beginmydesca
\item[~~~~~~ $\bullet$ Step 1:] Determine the eigenvalues of the $n\times n$ matrix
$A^{H} A$.  By Fact \ref{fact.A.D.D.D}, $A^{H} A$ is Hermitian and its
eigenvalues are real and non-negative.  Let $\lambda_{i}$ denote these
eigenvalues arranged in descending order.  Determine the number $r$ of
these eigenvalues that are nonzero.  Call the square root of each of
these eigenvalues $\sigma_{i}=\sqrt{\lambda_{i}}$, noting that
$\sigma_{1}\ge\sigma_{2}\ge \ldots \ge \sigma_{r}> 0$, and place these
nonzero $\sigma_{i}$ (in order) in the first $r$ elements on the main
diagonal of $\Sigma$, setting the other diagonal elements to
zero.\medskip
\item[~~~~~~ $\bullet$ Step 2:] Determine the eigenvectors $\v^{i}$
of the matrix $A^{H} A$, and arrange them in columns in the same order
as their corresponding eigenvalues in $\Sigma$ to form the $n\times n$
matrix $V$.  By Fact \ref{fact.A.D.D.D}, $V$ is unitary.\medskip
\item[~~~~~~ $\bullet$ Step 3:] Select $\u^{i}=(1/\sigma_{i}) A\v^{i}$ for
$i=1,2,\ldots r$, and define $\underline U$ as the matrix with these
vectors as columns.  By construction, we have $AV=\underline
U\begin{bmatrix} \underline{\Sigma} & 0 \end{bmatrix}$, and thus
$A=\underline U\begin{bmatrix} \underline{\Sigma} & 0 \end{bmatrix}
V^{H}$.  We have therefore constructed most of the singular value
decomposition.  All that remains is to find $\overline U$, which may
be found simply by taking the (complete) $QR$ decomposition of
$\underline U$ and setting $\overline U=\overline Q$ in this
decomposition. 
\endmydesc
Note that the $\u^{i}$ given by the above algorithm work out simply to
be the orthogonal eigenvectors of $A A^{H}$, ordered appropriately
such that the desired decomposition of $A$ is satisfied by
construction.

\subsubsubsection{(ii) Construction of the SVD from the eigenvalues and eigenvectors of $(A A^{H})$}

\noindent An alternative construction of the SVD may be written by taking the
eigenvalues of the matrix $A A^{H}$ in Step 1 of construction (i) above,
defining the columns of $U$ as the corresponding eigenvectors of this
matrix in Step 2, and then defining the $r$ columns of $\underline V$
as $\v^{i}=(1/\sigma_{i}) A^{H}\u^{i}$, and determining the remaining
columns of $V$ via the $QR$ decomposition of $\underline V$ in Step 3.

If $A$ is rectangular, the dimensions of the square matrices $A^{H} A$
and $A A^{H}$ are different, and a choice between construction (i) and construction (ii) is
sometimes made based on which works with the smaller of these two matrices during Steps 1 and 2.  However, as mentioned previously,
construction (iii) presented below is the numerically preferred algorithm for constructing the SVD.

\clearpage

\subsubsubsection{(iii) Construction of the SVD directly from a bidiagonalization of $A$}

\noindent Following the same line of reasoning as in \S \ref{sec.A.D.A}, but now pre- and post-multiplying by {\it different} Householder reflector matrices,
it is entirely straightforward to construct a bidiagonalization $A=U B_0 V^H$ where $U$ and $V$ are unitary and $B_0$ is upper bidiagonal.  
Indicated graphically, 
\begin{gather*}
  U^H_1 A = {\footnotesize\begin{pmatrix} * & * & * & * \\ 0 & * & * & * \\ 0 & * & * & * \\ 0 & * & * & * \\ 0 & * & * & * \end{pmatrix}}, \quad
  U^H_1 A V_1 = {\footnotesize\begin{pmatrix} x & * & 0 & 0 \\ 0 & * & * & * \\ 0 & * & * & * \\ 0 & * & * & * \\ 0 & * & * & * \end{pmatrix}}, \quad
  U^H_2 U^H_1 A V_1 = {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & * & * & * \\ 0 & 0 & * & * \\ 0 & 0 & * & * \\ 0 & 0 & * & * \end{pmatrix}}, \\
  U^H_2 U^H_1 A V_1 V_2 = {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & x & * & 0 \\ 0 & 0 & * & * \\ 0 & 0 & * & * \\ 0 & 0 & * & * \end{pmatrix}}, \quad
  U^H_3 U^H_2 U^H_1 A V_1 V_2 = {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & * & * \\ 0 & 0 & 0 & * \\ 0 & 0 & 0 & * \end{pmatrix}}, \quad
  B_0 = U^H_0 A V_0 = {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & x & x \\ 0 & 0 & 0 & * \\ 0 & 0 & 0 & 0 \end{pmatrix}},
\end{gather*}
where $U_0=U_1 U_2 \cdots U_n$ and $V_0=V_1 V_2\cdots V_{n-2}$.  Implementation is straightforward (see second code listed in Algorithm \ref{alg.4.SVD}; cf.~Algorithm \ref{alg.3.RC_Hessenberg}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplain[frame=single,label=alg.4.SVD,caption={Compute the reduced SVD $A={\underline U}_{\,m\times r} \,{\underline{\Sigma}}_{\,r\times r} \,({\underline V}_{\,n\times r})^{H}$.}]{RCC1.0/chap04/SVD.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/Bidiagonalization.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/SVDTest.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Starting from this bidiagonalization, we now consider the application of the shifted $QR$ method to the
tridiagonal Hermitian matrix $T_k=B_k^H B_k$.  At each step of this iterative method, we may apply a Wilkenson shift based on the lower-right corner of $T_k$,
as done in Algorithm \ref{alg.3.RC_EigHermitian}.  However, instead of explicitly shifting the matrix $T_k$ at each step, we will apply an {\bf implicitly shifted $QR$ iteration},
as done in non-symmetric real case discussed previously and illustrated in Algorithm \ref{alg.3.RC_EigReal} (see also Exercise \ref{ex:04.impshiftherm}).
Further, instead of working on $T_k$, we will apply the rotations comprising $V_k$ that we would have applied to $T_k$ (via both pre- and postmultiplication, applied using the implicitly shifted $QR$ method)
directly to the factor $B_k$ (via postmultiplication, again using the implicitly shifted $QR$ method), while applying compensating unitary rotations $U_k$ to the factor $B_k$
(via premultiplication) in order to keep it upper bidiagonal at each step.
That is,
\begin{equation*}
T_{k+1}=V_k^H T_k V_k = V_k^H B_k^H B_k V_k =  V_k^H B_k^H U_k U_k^H B_k V_k = B_{k+1}^H B_{k+1} \quad\textrm{where}\quad B_{k+1}=U_k^H B_k V_k.
\end{equation*}
Note that $B_{k+1}$ is upper bidiagonal (by construction), and thus $T_{k+1}$ is tridiagonal.  

As discussed in detail in the second paragraph of point (iii) on page \pageref{par:T22isolation}, $B$ may at each step be partitioned into a block upper triangular form
with diagonal blocks $B_{11}$, $B_{22}$, and $B_{33}$, where $B_{22}$ is unreduced.  The algorithm described above is thus
applied\footnote{In order to apply the Implicit $Q$ Theorem (Fact \ref{fact.implicitQ}), the submatrix of $T$ to which the theorem is being applied must be unreduced
(that is, nonzero in its lower subdiagonal); if it isn't, the proof
of this theorem breaks down, and in fact the algorithm implementing it fails. Thus, this partitioning step is essential to the success of the algorithm.}
to the $B_{22}$ submatrix, not to all of $B$.

Two more clean up items must be addressed.  The first is that, if there is a zero in the lower-right element of $B_{22}$, the entire last column of $B_{22}$ may
be zeroed by postmultiplication by an appropriate sequence of Givens rotation matrices.  Indicated graphically:
\begin{equation*}
  {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & x & x \\ 0 & 0 & 0 & 0 \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} x & x & 0 & 0 \\ 0 & x & * & * \\ 0 & 0 & * & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} x & * & 0 & * \\ 0 & * & x & 0 \\ 0 & 0 & x & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} * & x & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & x & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}}.
\end{equation*}
Second, if there is a zero in any other of the diagonal elements of $B_{22}$, that entire row of $B_{22}$ may
be zeroed by premultiplication by an appropriate sequence of Givens rotation matrices.  Indicated graphically:
\begin{equation*}
  {\footnotesize\begin{pmatrix} 0 & x & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & x & x \\ 0 & 0 & 0 & x \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} 0 & 0 & * & 0 \\ 0 & * & * & 0 \\ 0 & 0 & x & x \\ 0 & 0 & 0 & x \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} 0 & 0 & 0 & * \\ 0 & x & x & 0 \\ 0 & 0 & * & * \\ 0 & 0 & 0 & x \end{pmatrix}} \Rightarrow
  {\footnotesize\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & x & x & 0 \\ 0 & 0 & x & x \\ 0 & 0 & 0 & * \end{pmatrix}}
\end{equation*}
Implementation of all of the steps described above is given in Algorithm \ref{alg.4.SVD}.

\clearpage
\subsection{Decompositions related to RC_Gaussian elimination: $LDM^{H}$, $LDL^{H}$, and Cholesky}\label{sec.A.D.I}

In the course of solving $A\x=\b$ via RC_Gaussian elimination
in \S \ref{sec.B.A}, a few useful matrix decompositions were already
encountered.  In \S \ref{sec.B.Ab}, we presented a step-by-step
procedure to construct an $LU$ decomposition, $A=LU$, where $L$ is
unit lower triangular and $U$ is upper triangular, assuming that $A$
is nonsingular and row swaps are not required in the RC_Gaussian
elimination procedure.  If row swaps were required (\S \ref{sec.B.B}),
we accounted for them with the permutaion matrix $P$ such that
$A=PLU$.  If column swaps were also incorporated (\S \ref{sec.B.B.C}),
we accounted for them with the permutation matrix $Q$ such that
$A=PLU\!Q^{T}$.

We now discuss a few different ways to re"express such
decompositions, assuming $A$ is nonsingular and focusing for the
remainder of this section (for simplicity) on the case that $P=Q=I$.

Starting from the $LU$ decomposition $A=LU$ (if it
exists\footnote{Recall that existence of the $LU$ decomposition is
guaranteed in the cases of diagonally-dominant $A$ (Fact \ref{fact.B.Aa.A})
and Hermitian positive definite $A$ (Fact \ref{fact.A.D.D.Eg}).}) of a square nonsingular matrix $A$, and
defining $D={\rm diag}(u_{11},u_{22},\ldots,u_{nn})$, noting that $D$
is nonsingular with $D^{-1}={\rm
diag}(u^{-1}_{11},u^{-1}_{22},\ldots,u^{-1}_{nn})$, we may write
$A=LDM^{H}$, where, as may be verified by inspection, $M^{H}=D^{-1}U$
is unit upper triangular.

\begin{fact} \label{fact.B.Aa.Az}
If $A$ is nonsingular and its $LU$ decomposition exists (with $L$ unit lower triangular
and $U$ upper triangular), then the $LU$ decomposition is unique.
\end{fact}

\noindent {\it Proof}\/: If $A$ is nonsingular, then $|A|\ne 0$, and thus $|L|\ne 0$ and $|U|\ne 0$, so
$L$ and $U$ are nonsingular as well.  Assume $L_{1}U_{1}$ and $L_{2}U_{2}$ are two
$LU$ decompositions of $A$.  Then $L_{1}U_{1}=L_{2}U_{2}\ \Rightarrow\
L_{2}^{-1}L_{1}=U_{2}U_{1}^{-1}$.  As $L_{2}^{-1}L_{1}$ is unit lower
triangular and $U_{2}U_{1}^{-1}$ is upper triangular and these two
expressions are equal, they must equal the identity matrix.  Thus
$L_{2}=L_{1}$ and $U_{2}=U_{1}$.   \endproof \vskip0.1in

\begin{fact} \label{fact.B.A.G.A}
If $A$ is nonsingular and Hermitian and $A=LDM^{H}$ where $L$ and $M$
are unit lower triangular and $D$ is diagonal, then $L=M$ (that is,
$A=LDL^{H}$) and $D$ is real.
\end{fact}

\noindent {\it Proof}\/: If $A$ is Hermitian ($A^{H}=A$) and
decomposed such that $A=LDM^{H}$, it follows that
$MD^{H}L^{H}=LDM^{H}$.  Note that both $MD^{H}L^{H}$ and $LDM^{H}$ are $LDM^{H}$
decompositions of $A$.  Thus, by the uniqueness of the $LDM^{H}$
decomposition [which follows directly from the uniqueness of the $LU$
decomposition (Fact \ref{fact.B.Aa.Az}) from which the $LDM^{H}$ decomposition is derived],
$M=L$ and $D^{H}=D$ (that is, $D$ is real).  \endproof \vskip0.1in

If $A>0$, then by Facts \ref{fact.B.A.G.A} and \ref{fact.A.D.D.Ef} we may write $A=LDL^{H}$ where $D$ is diagonal with positive diagonal elements.  
Defining $D^{1/2}=\textrm{diag}(u_{11}^{1/2},u_{22}^{1/2},\ldots,u_{nn}^{1/2})$
and $G=LD^{1/2}$, we obtain the {\bf Cholesky decomposition} $A=GG^{H}$,
where $G$ is lower triangular.  Further, given a $\underline{QR}$ decomposition of $A$,
$(A^H A)=(\underline{QR})^H \underline{QR} = \underline{R}^H \underline{R}$, and thus the Cholesky decomposition of $(A^H A)$
[that is, $(A^H A)=G G^H$] is given by $G=\underline{R}^H$.

\subsubsubsection{Direct determination of the Cholesky decomposition}%\label{sec:choleskydirect}

\noindent If $A>0$, rather than performing RC_Gaussian elimination on $A$ and then backing out $G=LD^{1/2}$ afterwards,
or determining the $\underline{QR}$ decomposition of $A$ and then taking $G=\underline{R}^H$,
we may instead leverage the Hermitian positive definite structure of $A$
to determine the Cholesky decomposition directly, with only $\sim(n^3/3)$ flops.

Consider again the decomposition of the Hermitian positive definite matrix $A$ given by \eqref{CholeskyIdentity}
\begin{equation*}
  A= \begin{bmatrix} \alpha_1 & \v_1^H \\ \v_1 & B_1 \end{bmatrix} =
  \begin{bmatrix} \beta_1 & 0 \\ \v_1/ \beta_1 & I \end{bmatrix}
  \begin{bmatrix} 1 & 0 \\ 0 & B_1-\v_1 \v_1^H/ \alpha_1 \end{bmatrix}
  \begin{bmatrix} \beta_1 & \v_1^H/ \beta_1 \\ 0 & I \end{bmatrix} = \tilde G_1\tilde A \tilde G_1^H,
\end{equation*}
recalling that $\alpha_1>0$, $B_1>0$, $\beta_1=\sqrt{\alpha_1}$, and $A_1=B_1-\v_1 \v_1^H/ \alpha_1>0$.

Since $A_1>0$, the above decomposition may be used to reduce $A_1$ in a similar fashion, that is, $A_1=\tilde G_2\tilde A_1 \tilde G_2^H$,
where $\tilde A_1=\textrm{diag}[1,\tilde A_2]$.  This process may
be repeated, ultimately providing the decomposition
\begin{equation*}
  A=G_1 G_2 \cdots G_{n} G_{n}^H \cdots G_2^H G_1 = G G^H \quad \textrm{where} \quad G_k = \begin{bmatrix} I_{(k-1)\times (k-1)} & 0 \\ 0 & \tilde G_k \end{bmatrix}  \quad \textrm{and} \quad
G=G_1 G_2 \cdots G_{n}.
\end{equation*}
Due to the fact that they are each built from identity matrices with only one column changed, the product of the $G_i$ matrices collapses into a single matrix in a very simple fashion,
as implemented in Algorithm \ref{alg.3.Cholesky}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.Cholesky,caption={Compute the full Cholesky decomposition $A=G G^H$ of some $A>0$.}]{RCC1.0/chap04/Cholesky.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsubsection{Approximating the Cholesky decomposition of sparse matrices}%\label{sec:choleskyincomplete}

\noindent In general, the Cholesky factor $G$ does not retain the sparsity structure of the original matrix $A$.  However, if $A$ is diagonally dominant, it is found, following Algorithm \ref{alg.3.Cholesky},
that those elements of $G$ corresponding to the zero elements of $A$ are {\it nearly} zero.  As a heuristic, we may thus construct
an {\bf approximate Cholesky decomposition} of $A$ by essentially the same algorithm, but touching only the nonzero elements of $A$ and thereby retaining the sparsity structure of $A$,
as implemented in Algorithm \ref{alg.3.IncompleteCholesky}.  As shown by the test script provided (try it!), this approach often gives a good approximation of the Cholesky factor of $A$
without much computational effort.  Note that any large scale implementation of this algorithm should, of course,
loop through only the nonzero elements of $A$, as stored in vectors instead of a sparse matrix, thus skipping the time consuming {\tt if} statements
present in the demonstration code given in Algorithm \ref{alg.3.IncompleteCholesky}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplainB[frame=single,label=alg.3.IncompleteCholesky,caption={Approximate the Cholesky decomposition $A=G G^H$
of a sparse matrix $A>0$ while maintaining the sparsity of $A$ in $G$.}]{RCC1.0/chap04/IncompleteCholesky.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/CholeskyTest.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Solution of some important matrix equations}

We now consider the efficient solution of the unknown, $X$, in the {\bf differential Lyapunov equation} ({\bf DLE})
\begin{equation*}
\frac{dX}{dt} = A^H X + X A + Q
\end{equation*}
and its steady-state solution, the {\bf continuous-time algebraic Lyapunov equation} ({\bf RC_CALE})
\begin{equation*}
0 = A^H X + X A + Q,
\end{equation*}
the generalization of the RC_CALE known as the {\bf RC_Sylvester equation}
\begin{equation*}
0= A X - X B - C,
\end{equation*}
and the {\bf differential Riccati equation} ({\bf DRE})
\begin{equation*}
- \frac{d X}{dt} = A^H X + X A - X S X + Q
\end{equation*}
and its steady-state solution, the {\bf continuous-time algebraic Riccati equation} ({\bf RC_CARE})
\begin{equation*}
0 = A^H X + X A - X S X + Q.
\end{equation*}
We similarly consider the efficient solution of the unknown, $X$, in the {\bf Lyapunov difference equation} ({\bf LDE})
\begin{equation*}
X_{k+1} = F^H X_k F + Q
\end{equation*}
and its steady-state solution, the {\bf discrete-time algebraic Lyapunov equation} ({\bf RC_DALE})
\begin{equation*}
X = F^H X F + Q,
\end{equation*}
the generalization of the RC_DALE known as the {\bf Stein equation}
\begin{equation*}
X = A^H X B + C,
\end{equation*}
and the {\bf Riccati difference equation} ({\bf RC_RDE})
\begin{equation*}
  X_k = F^H X_{k+1} (I+ S X_{k+1})^{-1} F + Q, 
\end{equation*}
and its steady-state solution, the {\bf discrete-time algebraic Riccati equation} ({\bf RC_DARE})
\begin{equation*}
X = F^H X (I+ S X)^{-1} F + Q.
\end{equation*}
The DLE, RC_CALE, DRE, and RC_CARE arise in the analysis and control of continuous-time systems, whereas
the LDE, RC_DALE, RC_RDE, and RC_DARE arise in the analysis and control of discrete-time systems.
The DLE, DRE, LDE, and RC_RDE may be solved simply by marching, whereas all of the other above equations may be solved by clever appliciation
of the RC_Schur decomposition. Some of these solution approaches require the eigenvalues on the main diagonal of $T$ in
the relevant RC_Schur decomposition to be ordered in a particular way; this section thus concludes with an algorithm (based, in fact, on
repeated computations of small RC_Sylvester equations) to reorder the RC_Schur decomposition in any desired fashion.

\subsection{Continuous-time Lyapunov equations}\label{sec:CTLE}

The {\bf differential Lyapunov equation} ({\bf DLE}) may be written
\begin{equation}
\frac{dX}{dt} = A^H X + X A + Q
\label{eq:DLE}
\end{equation}
for given $A_{n\times n}$ and Hermitian $Q_{n\times n}$.
Starting from Hermitian initial conditions, solutions $X(t)$ of this matrix equation satisfy $X^H(t)=X(t)$,
and may easily be marched in time using the techniques discussed in \S \ref{chap10}.
Equations of this form arise in the analysis of continuous-time linear systems, as discussed further in \S \ref{chap17}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplain[frame=single,label=alg.3.Cale,caption={Solve the RC_CALE, $A^H X + X A + Q = 0$, for
(a) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_CALE.m}{full} and
(b) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_CALEtri.m}{upper triangular} $A$.}]{RCC1.0/chap04/RC_CALE.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_CALEtri.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_CALE_RC_CARE_RC_DALE_RC_DARE_Test.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\enlargethispage{7pt}
The {\bf continuous-time algebraic Lyapunov equation} ({\bf RC_CALE}) may be written
\begin{equation}
0 = A^H X + X A + Q
\label{eq:RC_CALE}
\end{equation}
for given $A_{n\times n}$ and Hermitian $Q_{n\times n}$.  The RC_CALE is the steady-state solution of \eqref{eq:DLE} with $dX/dt=0$.  
The RC_CALE may be solved by performing a RC_Schur decomposition $A=U A_0 U^{H}$, defining $Q_0=U^H Q U$ and $X_0=U^H X U$, and
substituting into \eqref{eq:RC_CALE}, resulting in 
\begin{equation}
0 = A_0^H X_0 + X_0 A_0 + Q_0,
\label{eq:TRC_CALE}
\end{equation}
where $A_0$ is upper triangular and $Q_0$ is Hermitian.  Partitioning $A_0$, $X_0$, and $Q_0$ according to
\begin{equation*}
   A_0=\begin{bmatrix} a_1 & \a_1^H \\ 0    & A_1 \end{bmatrix}, \quad
   X_0=\begin{bmatrix} x_1 & \x_1^H \\ \x_1 & X_1 \end{bmatrix}, \quad
   Q_0=\begin{bmatrix} q_1 & \q_1^H \\ \q_1 & \tilde Q_1 \end{bmatrix},
\end{equation*}
it follows immediately from \eqref{eq:TRC_CALE} that
\begin{subequations}
\label{eq:calesol}
\begin{align}
& x_1 =- q_1 / (a_1 + \bar a_1),     \label{eq:calesol;a} \\
& (A_1^H + a_1 I ) \x_1 = - \q_1 - x_1 \a_1, \label{eq:calesol;b}\\
&  0 = A_1^H X_1 + X_1 A_1 + Q_1, \quad \textrm{where} \quad Q_1 = \tilde Q_1 + \a_1 \x_1^H + \x_1 \a_1^H. \label{eq:calesol;c}
\end{align}
\end{subequations}
After $x_1$ is determined from \eqref{eq:calesol;a} and $\x_1$ is determined (using RC_Gaussian elimination) from \eqref{eq:calesol;b},
the remaining problem to be solved for $X_1$, \eqref{eq:calesol;c}, is identical to \eqref{eq:TRC_CALE} but of order $(n-1)\times (n-1)$.  
Thus, the process of partitioning $A_k$, $X_k$, and $Q_k$ and solving for the element and vector in the first
column of $X_k$ may be repeated on progressively smaller matrices, ultimately building
the entire Hermitian matrix $X_0$, from which it follows that $X = U X_0 U^H$.  Efficient implementation of these equations
is provided in Algorithm \ref{alg.3.Cale}.

\enlargethispage{7pt}
\subsubsection{The RC_Sylvester equation}\label{sec:RC_Sylvester}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplain[frame=single,label=alg.3.RC_Sylvester,caption={Solve the RC_Sylvester equation $A X - X B = g C$ for
(a) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_Sylvester.m}{full} and
(b) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_SylvesterTri.m}{upper triangular} $A$ and $B$.}]{RCC1.0/chap04/RC_Sylvester.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_SylvesterTri.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The RC_CALE is a special case of the {\bf RC_Sylvester equation}
\begin{equation}
   A X - X B = C
   \label{eq:RC_Sylvester;a}
\end{equation}
for the unknown $X_{m\times n}$ given $A_{m\times m}$, $B_{n \times n}$ and $C_{m \times n}$.
As we now show, similar techniques to those seen in the previous section may be used to solve this more general form.  
We begin by performing the RC_Schur decompositions $A=U A_0 U^H$ and $B=V B_0 V^H$
and defining $C_0=U^H C V$ and $X_0=U^H X V$, thereby transforming \eqref{eq:RC_Sylvester;a} to the form
\begin{equation}
   A_0 X_0 - X_0 B_0 = C_0.
   \label{eq:RC_Sylvester;b}
\end{equation}
where $A_0$ and $B_0$ are upper triangular.  Partitioning $A_0$, $X_0$, $B_0$, and $C_0$ according to
\begin{equation*}
  A_0 = \begin{bmatrix} {A_1} & \a_1 \\ 0 & a_1 \end{bmatrix}, \quad
  X_0 = \begin{bmatrix} \x_1 & X_1 \\ x_1 & \y^H_1 \end{bmatrix}, \quad
  B_0 = \begin{bmatrix} b_1 & \b^H_1 \\ 0 & {B_1} \end{bmatrix}, \quad
  C_0 = \begin{bmatrix} \c_1 & \tilde C_1 \\ c_1 & \d^H_1 \end{bmatrix}, \quad
\end{equation*}
it follows immediately from \eqref{eq:RC_Sylvester;b} that
\begin{subequations}
\label{eq:sylsol}
\begin{align}
  & x_1 = c_1/(a_1 - b_1), \label{eq:sylsol;a}\\
  & (A_1   -      b_1 I) \x_1 =   \c_1 -      x_1 \a_1, \label{eq:sylsol;b}\\
  & \y^H_1 (a_1 I - B_1)      =   \d^H_1  + x_1 \b^H_1, \label{eq:sylsol;c}\\
  & A_1 X_1 - X_1 B_1 = C_1, \quad \textrm{where} \quad C_1 = \tilde C_1 + \x_1 \b_1^H - \a_1 \y_1^H. \label{eq:sylsol;d}
\end{align}
\end{subequations}
After $x_1$ is determined from \eqref{eq:sylsol;a} and $\x_1$ and $\y_1$ are determined (using RC_Gaussian elimination) from \eqref{eq:sylsol;b}
and \eqref{eq:sylsol;c},
the remaining problem to be solved for $X_1$, \eqref{eq:sylsol;d}, is identical to \eqref{eq:RC_Sylvester;b} but of order $(n-1)\times (n-1)$.  
Thus, the process of partitioning $A_k$, $X_k$, $B_k$, and $C_k$ and solving for the element and vectors in the first
column and last row of $X_k$ may be repeated on progressively smaller matrices, ultimately building
the entire Hermitian matrix $X_0$, from which it follows that $X = U X_0 V^H$.  
Efficient implementation of these equations is provided in Algorithm \ref{alg.3.RC_Sylvester} where, for later convenience, we have scaled all components of $C$ by a scalar $g$.

\clearpage
\subsection{Continuous-time Riccati equations}\label{sec:CTRE}

Consider the continuous-time system written in the block form
\begin{equation}
  \frac{d\z}{dt} = H\z \quad \textrm{where} \quad H=H_{2n\times 2n}=\begin{bmatrix} A & -S \\ -Q & -A^H \end{bmatrix}, \quad
  \z=\begin{bmatrix} \x \\ \r \end{bmatrix},
\label{eq:cont.time.system}
\end{equation}
where $S\ge 0$, $Q\ge 0$.  Assuming $\r=X\x$ for some $X=X(t)$ (which we will seek to determine) and inserting this assumed form of the solution into the above
to eliminate $\r$, combining rows to eliminate $d{\x}/dt$, factoring
out $\x$ to the right, and requiring that the result holds for all
$\x_0$, it follows that $X$ obeys the {\bf differential Riccati equation} ({\bf DRE})
\begin{equation}
- \frac{d X}{dt} = A^H X + X A - X S X + Q.
\label{eq:cont.time.hamil}
\end{equation}
Starting from Hermitian initial conditions, solutions $X(t)$ of this matrix equation satisfy $X^H(t)=X(t)$,
and may easily be marched in time using the techniques discussed in \S \ref{chap10}.
Equations of this form arise in the control and estimation of continuous-time linear systems, as discussed further in \S \ref{sec:riccati-based-control}
and \S \ref{sec:riccati-based-estimation}.
It is easily verified that $H$ is {\bf Hamiltonian} (see \S \ref{sec.A.D.D.B}) and thus satisfies the 
symmetric root property (Fact \ref{fact.A.D.D.Ec}); that is, for every eigenvalue of $H$ in the LHP,
there is a corresponding eigenvalue of $H$ in the RHP.  
[We assume for now that the $H$ defined in \eqref{eq:cont.time.system} has no eigenvalues on the imaginary axis; we will later make precise when this assumption is justified.]
In other words, the vector space $\Zss$ that $\z$ belongs to can be divided into two subspaces,
a stable subspace $\Zss^s$ and an unstable subspace $\Zss^u$.  For any $\z\in\Zss^s$, $\z(t)$ decays exponentially in time, and
for any $\z\in\Zss^u$, $\z(t)$ grows exponentially in time.

We now seek to find an appropriate relation $\r=X\x$ that restricts the evolution of $\z$ in \eqref{eq:cont.time.system}
to the stable subspace $\z\in\Zss^s$, and thus $\z\rightarrow 0$ as $t\rightarrow\infty$.  In terms of the DRE \eqref{eq:cont.time.hamil}, we seek the (finite) constant value of $X$ that
\eqref{eq:cont.time.hamil} marches to for large time, thereby satisfying the {\bf continuous-time algebraic Riccati equation} ({\bf RC_CARE})
\begin{equation}
0 = A^H X + X A - X S X + Q.
\label{eq:RC_CARE}
\end{equation}
The following subsections discuss two ways to accomplish this.

\subsubsubsection{Approach based on eigen decomposition}%\label{sec:RC_CAREsolution:eigen}

\noindent Assume now that an eigen decomposition of $H$ is available such that
\begin{equation*}
H=S\Lambda S^{-1}\quad \textrm{where} \quad 
S=\begin{bmatrix} S_{11} & * \\ S_{21} & * \end{bmatrix}
= \displaystyle \begin{bmatrix} | & | & & | & \\ \s^1 & \s^2 & \ldots & \s^n & * \\  | & | & & | & \end{bmatrix}
\quad \textrm{and} \quad
\s^i = \begin{bmatrix} \x^i \\ \r^i \end{bmatrix},
\end{equation*}
where the eigenvalues of $H$ on the main diagonal of diagonal matrix $\Lambda$ are enumerated
such that the LHP eigenvalues appear first, followed by the RHP eigenvalues.
Defining $\y=S^{-1}\z$, it follows from \eqref{eq:cont.time.system} that $d\y/dt=\Lambda \y$.
The stable solution of $\y$ are thus spanned by the first $n$ columns
of $\Lambda$ (that is, they are nonzero only in the first $n$ elements of $\y$).  Since $\z=S\y$, it follows that the stable
solutions of $\z$ are spanned by the first $n$ columns of $S$.  To achieve stability of $\z$ via the relation $\r=X\x$ for
each of these directions, denoted $\s^i$ and decomposed as shown above, we must have $\r^i=X\x^i$ for $i=1\ldots n$.
Assembling these equations in matrix form, we have
\begin{equation*}
\begin{bmatrix} | & | & & | \\ \r^1 & \r^2 & \ldots & \r^n \\  | & | & & | \end{bmatrix} = X
\begin{bmatrix} | & | & & | \\ \x^1 & \x^2 & \ldots & \x^n \\  | & | & & | \end{bmatrix} \quad \Rightarrow \quad
S_{21} = X S_{11}.
\end{equation*}
Unfortunately, an eigen decomposition of $H$ is {\it not} always available, and even when it is, it turns out that $S_{11}$ will be nearly singular
when $H$ is nearly defective.  Thus, in general, {\it solution of the RC_CARE via eigen decomposition is not recommended in practice}; the method
described in the followed section is preferred.

\subsubsubsection{Approach based on RC_Schur decomposition}%\label{sec:RC_CAREsolution:schur}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.RC_CARE,caption={Solve the RC_CARE, $A^H X + X A - X S X + Q=0$.}]{RCC1.0/chap04/RC_CARE.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Now assume an RC_Schur decomposition of $H$ is available such that
\begin{subequations}
\begin{equation}
H=U T U^{H}\quad \textrm{where} \quad 
U=\begin{bmatrix} U_{11} & * \\ U_{21} & * \end{bmatrix}
= \displaystyle \begin{bmatrix} | & | & & | & \\ \u^1 & \u^2 & \ldots & \u^n & * \\  | & | & & | & \end{bmatrix}
\quad \textrm{and} \quad
\u^i = \begin{bmatrix} \x^i \\ \r^i \end{bmatrix},
\label{eq:shurdecompofH}
\end{equation}
where the eigenvalues of $H$ on the main diagonal of the upper triangular matrix $T$ are enumerated
such that the LHP eigenvalues appear first, followed by the RHP eigenvalues.
Defining $\y=U^{H}\z$, it follows from \eqref{eq:cont.time.system} that $d\y/dt=T \y$.  Again,
the stable solution of $\y$ are spanned by the first $n$ columns
of $T$ (that is, they are nonzero only in the first $n$ elements of $\y$).  Since $\z=U\y$, it follows that the stable
solutions of $\z$ are spanned by the first $n$ columns of $U$.  To achieve stability of $\z$ via the relation $\r=X\x$ for
each of these directions, denoted $\u^i$ and decomposed as shown above, we must have $\r^i=X\x^i$ for $i=1\ldots n$.
Assembling these equations in matrix form, we have
\begin{equation}
\begin{bmatrix} | & | & & | \\ \r^1 & \r^2 & \ldots & \r^n \\  | & | & & | \end{bmatrix} = X
\begin{bmatrix} | & | & & | \\ \x^1 & \x^2 & \ldots & \x^n \\  | & | & & | \end{bmatrix} \quad \Rightarrow \quad
U_{21} = X U_{11} \quad \Rightarrow \quad X = U_{21} U_{11}^{-1}.
\end{equation}
\end{subequations}
In order for the last relation to be solvable for the unknown $X$, we must assume that $U_{11}$ is nonsingular.  
To summarize, the RC_CARE \eqref{eq:RC_CARE} is solvable via the above approach under the following two assumptions:
\beginmylistb
\item the matrix $H$ [see \eqref{eq:cont.time.system}] has no eigenvalues on the imaginary axis, and
\item the upper-left factor $U_{11}$ [see \eqref{eq:shurdecompofH}] in the ordered RC_Schur decomposition of $H$ is nonsingular.
\endmylist
Precise conditions under which these two assumptions are guaranteed to be satisfied are discussed in \S \ref{chap19}.  

As opposed to the eigen decomposition, note that the RC_Schur decomposition is guaranteed to exist (Fact \ref{fact.A.D.C.Bb}),
and that reliable algorithms are well developed to compute it (see \S \ref{sec.A.D.E}).
Thus, the RC_Schur decomposition approach discussed here is the most common approach for solving the RC_CARE, as implemented in Algorithm \ref{alg.3.RC_CARE}.

\subsubsection{The Chandrasekhar method for approximate solution of DREs}\label{subsec:chan}

As derived in \S \ref{sec:riccati-based-control} and \ref{sec:CTCkalman} and discussed in much further detail there, two DREs of interest for solving
the {\bf state feedback control} matrix $K$ and the {\bf output injection} matrix $L$ are:
\begin{subequations}
\begin{alignat}{3}
- \frac{dX(t)}{dt} &= A^H X(t) + X(t) A    - X(t) B Q_{\u}^{-1} B^H X(t) + Q_{\x} \quad && \textrm{with} \quad X(T)=Q_T, \quad && K(t) = - Q_{\u}^{-1} B X(t),  \label{chandre1}\\
\frac{dP(t)}{dt} &= A P(t)   + P(t) A^H  - P(t) C^H Q_{2}^{-1} C  P(t) + Q_{1}    \quad && \textrm{with} \quad P(0)=P_0, \quad && L(t) = - P(t) C^H Q_{2}^{-1}, \label{chandre} 
\end{alignat}
\end{subequations}
where $X=X_{n\times n}$, $P=P_{n\times n}$, $K=K_{m_\u \times n}$, and $L=L_{n\times m_\y}$, where $n$ is the {\bf state dimension},
the $m_\u$ is the {\bf control dimension}, and $m_\y$ is the {\bf measurement dimension}.  
If $n\gg m_\u$ and $n\gg m_\y$, which is typical in high-dimensional systems (that is, when $n\gg 1$), then solving Riccati equations for the $n\times n$ matrices $X$ and $P$ in order to compute the
$m_\u \times n$ matrix $K$ and the $n\times m_\y$ matrix $L$ is inefficient,
as this approach computes enormous $n\times n$ Riccati matrices only to take narrow ``slices'' of these matrices to determine
the desired feedback matrices $K$ and $L$.  Chandrasekhar's method (Kailath 1973) addresses this inefficiency in a clever way. 

To be sepecific, consider the DRE for the estimator, as given in \eqref{chandre} [the DRE for the feedback control problem in \eqref{chandre1} may be addressed in a similar fashion,
{mutatis mutandis}].
Chandrasekhar's method solves an evolution equation for a
low-dimensional factored form of $dP(t)/dt$ and another
evolution equation for $L(t)$.  To this end, define
\begin{equation*}
\frac{dP(t)}{dt} = {Y}_1 {Y}_1^H - {Y}_2 {Y}_2^H = {Y} {H} {Y}^{*}, \quad
{Y}=\begin{pmatrix} {Y}_{1} & {Y}_{2} \end{pmatrix}, \quad
{H}=\begin{pmatrix} {I} & 0 \\ 0 & -{I} \end{pmatrix},
\end{equation*}
where the number of columns of ${Y}_1$ and $Y_2$ are the number of
positive and negative eigenvalues of $(dP/dt)$, respectively,
retained in the approximation. Differentiating \eqref{chandre} with
respect to time and inserting $dP/dt={Y} {H} {Y}^{*}$, assuming
$\{A,B,C,Q_{1},Q_{2}\}$ are LTI, it is easily verified that the
following set of equations are equivalent to \eqref{chandre}, but
much cheaper to compute if the factors $Y_1$ and $Y_2$ are low rank:
\begin{alignat*}{2}
\frac{dL(t)}{dt}&=-{Y}(t) {H} {Y}^H(t) {C}^H{Q_{2}}^{-1} \, ,\quad &&
{L}(0)=-{P}(0){C}^H{Q_{2}}^{-1} \, ,\\
\frac{dY(t)}{dt}&=[{A}+{L}(t){C}]{Y}(t) \,  ,\quad && {Y}(0) {H}
{Y}^{*}(0)=\frac{dP(t)}{dt}\Big|_{t=0},
\end{alignat*}
where $dP/dt|_{t=0}$ is determined from the original DRE
\eqref{chandre} evaluated at $t=0$, and $Y(0)$ is determined by its factorization. Note that Chandrasekhar's method may be used either to approximate the (time-accurate)
solution of the original DRE \eqref{chandre}, or simply marched to steady state to obtain the solution of the corresponding continuous-time algebraic Riccati equation.

\subsection{Discrete-time Lyapunov equations}\label{sec:DTLE}

The {\bf Lyapunov difference equation} ({\bf LDE}) may be written
\begin{equation}
X_{k+1} = F^H X_k F + Q
\label{eq:LDE}
\end{equation}
for given square $F$ and Hermitian $Q$.
Starting from Hermitian initial conditions, solutions $X_k$ of this matrix equation satisfy $X^H_k=X_k$,
and may easily be marched in $k$.
Equations of this form arise in the analysis of discrete-time linear systems, as discussed further in \S \ref{chap17}.   

The {\bf discrete-time algebraic Lyapunov equation} ({\bf RC_DALE}) may be written
\begin{equation}
X = F^H X F + Q
\label{eq:RC_DALE}
\end{equation}
for given square $F$ and Hermitian $Q$.  The RC_DALE is the steady-state solution of \eqref{eq:LDE} with $X_{k+1}=X_k=X$.
Following an analogous procedure as developed in \S \ref{sec:CTLE}, the RC_DALE may be solved by performing a RC_Schur decomposition $F=U F_0 U^{H}$,
defining $Q_0=U^H Q U$ and $X_0=U^H X U$, and substituting into \eqref{eq:RC_DALE}, leading to 
\begin{equation}
X_0 = F_0^H X_0 F_0 + Q_0
\label{eq:TRC_DALE}
\end{equation}
where $F_0$ is upper triangular and $Q_0$ is Hermitian.  Partitioning $F_0$, $X_0$, and $Q_0$ according to
\begin{equation*}
   F_0=\begin{bmatrix} f_1 & \f_1^H \\ 0 & F_1 \end{bmatrix}, \quad
   Q_0=\begin{bmatrix} q_1 & \q_1^H \\ \q_1 & \tilde Q_1 \end{bmatrix}, \quad
   X_0=\begin{bmatrix} x_1 & \x_1^H \\ \x_1 & X_1 \end{bmatrix},
\end{equation*}
it follows that
\begin{subequations}
\label{eq:dalesol}
\begin{align}
 &x_1 = q_1 / (1-f_1 \bar f_1),     \label{eq:dalesol;a} \\
 &(I - f_1 F_1^H) \x_1 = \q_1 + f_1 x_1 \f_1, \label{eq:dalesol;b}\\
 &X_1 = F_1^H X_1 F_1 + Q_1 \quad \textrm{where} \quad Q_1 = \tilde Q_1 + x_1 \f_1 \f_1^H + \f_1 (F_1^H \x_1)^H + (F_1^H \x_1) \f_1^H.  \label{eq:dalesol;c}
\end{align}
\end{subequations}
After $x_1$ is determined from \eqref{eq:dalesol;a} and $\x_1$ is determined (using RC_Gaussian elimination) from \eqref{eq:dalesol;b},
the remaining problem to be solved for $X_1$, \eqref{eq:dalesol;c}, is identical to \eqref{eq:TRC_DALE} but of order $(n-1)\times (n-1)$.  
Thus, the process of partitioning $F_k$, $X_k$, and $Q_k$ and solving for the element and vector in the first
column of $X_k$ may be repeated on progressively smaller matrices, ultimately building
the entire Hermitian matrix $X_0$, from which we may determine $X = U X_0 U^H$.  Efficient implementation of these equations
is provided in Algorithm \ref{alg.3.Dale}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlistingplain[frame=single,label=alg.3.Dale,caption={Solve the RC_DALE, $X = F^H X F + Q$, for
(a) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_DALE.m}{full} and
(b) \href{http://renaissance.ucsd.edu/RCC1.0/chap04/RC_DALEtri.m}{upper triangular} $F$.}]{RCC1.0/chap04/RC_DALE.m}
\lstinputlistingA[frame=single,aboveskip=-1pt]{RCC1.0/chap04/RC_DALEtri.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Extension of the above approach to the {\bf Stein equation} $X = A^H X B + C$ is similar to the extension from the RC_CALE to the RC_Sylvester equation
(see \S \ref{sec:RC_Sylvester}), and is addressed in Exercise \ref{ex:04.stein}.

\subsection{Discrete-time Riccati equations}\label{sec:DTRE}

Consider the discrete-time system written in the block form
\begin{equation}
  \z_{k+1} = M\z_k \quad \textrm{where} \quad M=M_{2n\times 2n}=\begin{bmatrix} F^{-H} & F^{-H}S \\ Q F^{-H} & F+ Q F^{-H}S \end{bmatrix}, \quad
  \z=\begin{bmatrix} \x \\ \r \end{bmatrix},
\label{eq:D.time.system}
\end{equation}
where $S\ge 0$, $Q\ge 0$.  Assuming $\r_k=X_k\x_k$ and inserting this assumed form of the solution into the above
to eliminate $\r_k$, combining rows to eliminate $\x_{k+1}$, factoring out $\x_k$ to the right, noting that this equation holds for all
$\x_k$, it follows that $X_k$ obeys the {\bf Riccati difference equation} (RC_RDE)
\begin{subequations}
\label{eq:RC_RDE}
\begin{equation}
  X_{k+1} = [Q F^{-H} + (F + Q F^{-H} S) X_{k}] [F^{-H} + F^{-H} R X_{k}]^{-1} \triangleq [C+D X_k] [A+B X_k]^{-1},
\label{eq:RC_RDE;z}
\end{equation}
which may be rearranged and written
\begin{equation}
  X_{k+1} = F X_{k} (I+ S X_{k})^{-1} F^H + Q.
\label{eq:RC_RDE;a}
\end{equation}
Under the assumption that $X$ is invertible, noting that $XY^{-1}=(YX^{-1})^{-1}$, the RC_RDE \eqref{eq:RC_RDE;a} may be written in the simpler form
\begin{equation}
  X_{k+1} = F^H (X_{k}^{-1} +  S)^{-1} F + Q.
  \label{eq:RC_RDE;b}
\end{equation}
Under the assumption that $S=G R^{-1} G^H$ for some $G$ and some $R>0$, but not assuming that $X$ is invertible,
noting the matrix inversion lemma (Fact \ref{fact.A.A.J.E}), the RC_RDE \eqref{eq:RC_RDE;a} may be written in the form
\begin{equation}
  X_{k+1} = F^{-H} X_{k} F - F^H X_{k} G (R + G^H X_{k} G)^{-1} G^H X_{k} F + Q.
  \label{eq:RC_RDE;c}
\end{equation}
\end{subequations}
Starting from Hermitian initial conditions on $X$, the forms \eqref{eq:RC_RDE;b} and \eqref{eq:RC_RDE;c}
reveal that solutions $X_k$ of these equations satisfy $X^H_k=X_k$.  Any of these four forms may easily be marched in $k$.
Equations of this form arise in the estimation and control of discrete-time linear systems, as discussed further in
\S \ref{sec:Driccati-based-estimation} and \S \ref{sec:Driccati-based-control}.
It is easily verified that $M$ is {\bf symplectic} (see \S \ref{sec.A.D.D.B}) and thus satisfies the 
reciprocal root property (Fact \ref{fact.A.D.D.Ed}); that is, for every eigenvalue of $M$ inside the unit circle,
there is a corresponding eigenvalue of $M$ outside the unit circle.  
[We assume for now that the $M$ defined in \eqref{eq:D.time.system} has no eigenvalues on the unit circle.]
In other words, the vector space $\Zss$ that $\z$ belongs to can be divided into two subspaces,
a stable subspace $\Zss^s$ and an unstable subspace $\Zss^u$.  For any $\z\in\Zss^s$, $z_k$ decays exponentially with $k$, and
for any $\z\in\Zss^u$, $z_k$ grows exponentially with $k$.

We now seek to find an appropriate relation $\r_k=X_k\x_k$ that restricts the evolution of $\z_k$ in \eqref{eq:D.time.system}
to the stable subspace $\z\in\Zss^s$, and thus $\z_k\rightarrow 0$ as $k\rightarrow\infty$.  In terms of the RC_RDE \eqref{eq:RC_RDE}, we seek the (finite) constant value of $X_k=X_{k+1}=X$ that
\eqref{eq:RC_RDE;a} marches to for large $k$, thereby satisfying the {\bf discrete-time algebraic Riccati equation} ({\bf RC_DARE})
\begin{equation}
X = F^H X (I+ S X)^{-1} F + Q.
\label{eq:RC_DARE}
\end{equation}
Solution of the RC_DARE may be achieved via RC_Schur decomposition of $M$: first decompose
\begin{subequations}
\begin{equation}
M=U T U^{H}\quad \textrm{where} \quad 
U=\begin{bmatrix} U_{11} & * \\ U_{21} & * \end{bmatrix},
\label{eq:shurdecompofM}
\end{equation}
where the eigenvalues of $M$ on the main diagonal of the upper triangular matrix $T$ are enumerated
such that those eigenvalues inside the unit circle appear first, followed by those eigenvalues outside the unit circle.
Following the same line of reasoning as in \S \ref{sec:CTRE}, the resulting expression for $X$ is given by
\begin{equation}
U_{21} = X U_{11} \quad \Rightarrow \quad X = U_{21} U_{11}^{-1},
\end{equation}
\end{subequations}
as implemented in Algorithm \ref{alg.3.RC_DARE}.  To summarize, the RC_DARE \eqref{eq:RC_DARE} is solvable via the above approach under the following two assumptions:
\beginmylistb
\item the matrix $M$ [see \eqref{eq:cont.time.system}] has no eigenvalues on the unit circle, and
\item the upper-left factor $U_{11}$ [see \eqref{eq:shurdecompofM}] in the ordered RC_Schur decomposition of $M$ is nonsingular.
\endmylist
Precise conditions under which these two assumptions are guaranteed to be satisfied are discussed in \S \ref{chap19}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.RC_DARE,caption={Solve the RC_DARE, $X = F^H X (I+ S X)^{-1} F + Q$.}]{RCC1.0/chap04/RC_DARE.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The period doubling algorithm for solution of the RC_DARE}\label{subsec:Ddoubling}

It is easily verified that we may write $M^k\cdot M^k = M^{2k}$ as
\begin{equation}
\label{eq:idenperdub}
\begin{bmatrix}  F_k^{-H} & F_k^{-H}S_k \\ Q_k F_k^{-H} & F_k+ Q_k F_k^{-H}S_k  \end{bmatrix} \cdot \begin{bmatrix} F_k^{-H} & F_k^{-H}S_k \\ Q_k F_k^{-H} & F_k+ Q_k F_k^{-H}S_k  \end{bmatrix} =
\begin{bmatrix} F_{2k}^{-H} & F_{2k}^{-H}S_{2k} \\ Q_{2k} F_{2k}^{-H} & F_{2k}+ Q_{2k} F_{2k}^{-H}S_{2k} \end{bmatrix},
\end{equation}
where $F_1=F$, $S_1=S$, $Q_1=Q$, and
\begin{equation}
F_{2k} = F_k (I+ S_k Q_k)^{-1} F_k, \quad Q_{2k} = Q_k + F_k Q_k (I+S_k Q_k)^{-1} F^H_k, \quad S_{2k} = S_k + F^H_k (I+S_k Q_k)^{-1}  S_k F_k.
\label{eq:perdubiter}
\end{equation}
Now assume $U_k$ is invertible, define $X_k \triangleq V_k U_k^{-1}$, and consider the discrete-time evolution equation
\begin{subequations}
\label{eq:perdubev}
\begin{gather}
\begin{bmatrix} U_{k+1} \\ V_{k+1} \end{bmatrix} = M \begin{bmatrix} U_{k} \\ V_{k} \end{bmatrix}
   = \begin{bmatrix} F^{-H} & F^{-H}S \\ Q F^{-H} & F+ Q F^{-H}S \end{bmatrix} \begin{bmatrix} U_{k} \\ V_{k} \end{bmatrix}
   \triangleq \begin{bmatrix} A & B \\ C & D \end{bmatrix}\begin{bmatrix} U_{k} \\ V_{k} \end{bmatrix} \label{eq:perdubev;a} \\
\Rightarrow \ldots \Rightarrow \quad X_{k+1}=[C+D X_k][A+B X_k]^{-1}; \label{eq:perdubev;b}
\end{gather}
\end{subequations}
comparing \eqref{eq:perdubev;b} with \eqref{eq:RC_RDE;z}, it is seen that \eqref{eq:perdubev;a} is a valid expression for the evolution of $X_k \triangleq V_k U_k^{-1}$.  Initializing $U_1=I$ and $V_1=0$ and leveraging the identity \eqref{eq:idenperdub}, the
evolution equation \eqref{eq:perdubev;a} may now be marched to steady state quicky by {\bf period doubling}; that is,
by marching $\ell=0,1,2,3,\ldots$ and thus $k\triangleq 2^\ell=1,2,4,8,\ldots$
In fact, the intermediate $U_k$ and $V_k$ in this march need not be computed, as we may write simply
\begin{equation*}
\begin{bmatrix} U_{2^\ell} \\ V_{2^\ell} \end{bmatrix} = M^{2^\ell} \begin{bmatrix} I \\ 0 \end{bmatrix} \quad \Rightarrow \quad X_{2^\ell} = V_{2^\ell} U_{2^\ell}^{-1} = Q_{2^\ell},
\end{equation*}
where $Q_{2^\ell}$ may be determined via the iteration proposed in \eqref{eq:perdubiter}, which converges rapidly as $\ell \rightarrow \infty$.

\enlargethispage{7pt}
\subsection{Reordering the RC_Schur decomposition}\label{sec:ReorderedRC_Schur}

The algorithms to solve the RC_CALE [in \S \ref{sec:CTLE}] and RC_DALE [in \S \ref{sec:DTLE}] were built on {\it any} RC_Schur decomposition of
the matrix $A_{n\times n}$, whereas the algorithms to solve the RC_CARE [in \S \ref{sec:CTRE}] and RC_DARE [in \S \ref{sec:DTRE}] were built on appropriately {\it ordered} RC_Schur
decompositions of, respectively, a ${2n\times 2n}$ Hamiltonian matrix $H$ [see \eqref{eq:cont.time.system}] and ${2n\times 2n}$ symplectic matrix $M$ [see \eqref{eq:D.time.system}],
where the stable eigenvalues of these ordered RC_Schur decompositions appear in the first $n$ elements on the main diagonal of $T$, followed by the unstable eigenvalues in the last $n$
elements on the main diagonal of $T$.  (Recall that, for the RC_CARE, the stable eigenvalues are in the LHP, whereas for the RC_DARE, the stable eigenvalues are within the unit disk.)
Of course, a general RC_Schur decomposition algorithm (for example, one based on the $QR$ method described in \S \ref{sec.A.D.E})
will, in general, not return the elements of $T$ in these desired orderings.  Transforming an arbitrarily-ordered RC_Schur decomposition
to one of these desired orderings in an efficient manner is not trivial, and is thus discussed further here.

The key building block of an efficient algorithm to accomplish the desired reordering of a RC_Schur decomposition is the computation of a unitary matrix $\tilde Q$ such that
\begin{equation}
  \tilde Q^H \begin{bmatrix} T_{11} & T_{12} \\  0 & T_{22} \end{bmatrix} \tilde Q = \begin{bmatrix} \tilde T_{11} & \tilde T_{12} \\  0 & \tilde T_{22} \end{bmatrix},
  \label{blockswap}
\end{equation}
where $T_{11}$ is $n_1 \times n_1$ and $T_{22}$ is $n_2 \times n_2$ and the matrices $T_{11}$, $T_{22}$, $\tilde T_{11}$, and $\tilde T_{22}$ are upper triangular,
with $\lambda(\tilde T_{11})=\lambda(T_{22})$ and $\lambda(\tilde T_{22})=\lambda(T_{11})$.  
Note that the form on the right is unitarily similar to the form on
the left, though the eigenvalues of the blocks on the main diagonal have been swapped.
To build this transformation, consider the identity
\begin{equation}
  \begin{bmatrix} T_{11} & T_{12} \\ 0 & T_{22} \end{bmatrix} =
  \begin{bmatrix} I & -X \\ 0 & g\, I \end{bmatrix} \begin{bmatrix} T_{11} & 0 \\ 0 & T_{22} \end{bmatrix} \begin{bmatrix} I & X/g \\ 0 & I/g \end{bmatrix}
    \label{blockswap2}  
\end{equation}
for some $g$ between zero and one\footnote{Note that $g$ is introduced simply to scale the transformation well for large $n$ when several swaps need
to be performed, thereby keeping the norms
of the blocks from getting large as several swaps are performed in succession.  Any of a number of heuristic strategies for selecting $g$ work well;
for small $n$, $g=1$ is sufficient.}, where $X$ is the solution of the RC_Sylvester equation
\begin{subequations}
    \label{blockswap3}  
\begin{equation}
  g\,T_{12} = T_{11} X - X T_{22},
      \label{blockswap3;a}  
\end{equation}
which may be solved using Algorithm \ref{alg.3.RC_Sylvester}.   Now consider the (complete) ${Q} {R}$ decomposition
\begin{equation}
  \begin{bmatrix}  -X \\ g\, I \end{bmatrix} = {Q} {R} \quad \textrm{where} \quad
  {Q}=\begin{bmatrix} Q_{11} & Q_{12} \\ Q_{21} & Q_{22} \end{bmatrix}, \quad {R}=\begin{bmatrix} R_{11} \\ 0 \end{bmatrix}, \quad
  Q^H Q=I,
  \label{blockswap3;b}
\end{equation}
and $R_{11}$ is upper triangular.
This decomposition may be solved efficiently using the methods described in \S \ref{sec.A.D.B}.  It follows from \eqref{blockswap3;b} that
$R_{11}^{-1} = Q_{21}/g$ and $Q_{12}^{-H} = Q_{12} - {Q_{11} R_{11} Q_{22}}/{g}$.
Now multiplying \eqref{blockswap2} from the left by $Q^H$ and from the right by $Q$ and applying the relations in \eqref{blockswap3;a} and \eqref{blockswap3;b},
it may be shown that this value for $Q$ almost accomplishes the swap sought in \eqref{blockswap}, with
\begin{equation*}
  Q^H \begin{bmatrix} T_{11} & T_{12} \\  0 & T_{22} \end{bmatrix} Q = \begin{bmatrix} R_{11} T_{22} R_{11}^{-1} & * \\  0 & Q_{12}^H T_{11} Q_{12}^{-H} \end{bmatrix}
  \triangleq \begin{bmatrix} \tilde T_{11} & * \\  0 & B \end{bmatrix}.
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\lstinputlisting[frame=single,label=alg.3.RC_ReorderSchur,caption={Reorder the RC_Schur decomposition, putting the stable eigenspace first.}]{RCC1.0/chap04/RC_ReorderSchur.m}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that the block $\tilde T_{11}$ of the matrix so generated is the product of upper triangular matrices, and is thus itself upper triangular.  However, the block
$B$ of the matrix generated by this process is not yet upper triangular.  Performing a RC_Schur decomposition of this block, $B=V \tilde T_{22} V^H$,
finally leads us to the desired form
\begin{equation}
   \tilde Q^H \begin{bmatrix} T_{11} & T_{12} \\  0 & T_{22} \end{bmatrix} \tilde Q = \begin{bmatrix} \tilde T_{11} & \tilde T_{12} \\  0 & \tilde T_{22} \end{bmatrix} \quad \textrm{where} \quad
   \tilde Q = Q \begin{bmatrix} I & 0 \\ 0 & V \end{bmatrix} \quad \textrm{with} \quad B = V \tilde T_{22} V^H.
  \label{blockswap3;c}
\end{equation}
\end{subequations}
To better understand this result, note that if $T_{12}=0$, the procedure described above leads to $X=0$ and $R_{11}=I$, and thus the resulting $\tilde Q$
which accomplishes the reordering is simply the permutation matrix 
\begin{equation*}
   \tilde Q=\begin{bmatrix} 0 & I \\ I & 0 \end{bmatrix}.
\end{equation*}

The $\tilde Q$ defined by the relations given in \eqref{blockswap3} may now be used to swap any two adjacent blocks of a RC_Schur decomposition $A=U T U^H$.
For example, partitioning $T$ and defining $P$, $\tilde T$ and $\tilde U$ such that
\begin{equation*}
T=\begin{bmatrix} T_{00} & T_{01} & T_{02} & T_{03} \\  & T_{11} & T_{12} & T_{13} \\ &  & T_{22} & T_{23} \\ 0 &  &  & T_{33} \end{bmatrix}, \quad
P=\begin{bmatrix} I & &  & 0 \\ & \tilde Q_{11} & \tilde Q_{12} & \\ & \tilde Q_{21} & \tilde Q_{22} & \\ 0 & & & I \end{bmatrix}, \quad
\tilde T=P^H T P, \quad \tilde U=U P,
\end{equation*}
it follows that $A=\tilde U \tilde T \tilde U^H$, where $\tilde T$ is upper triangular with the eigenvalues of the $\tilde T_{11}$ and $\tilde T_{22}$ blocks swapped from those
appearing in $T_{11}$ and $T_{22}$.
Applying such swaps in succession, in a block insertion sort fashion (see \S \ref{sec:RC_BlockInsertionSort}),
allows us to reorder a RC_Schur decomposition in any desired fashion, as implemented in Algorithm \ref{alg.3.RC_ReorderSchur}. 

\section{The Trace}\label{sec.A.F.A}

The {\bf trace} of a square matrix is the sum of its diagonal elements, and is defined using summation notation as
\begin{equation*}
    \Trace(A)= a_{jj} = (\e^j)^T A \e^j.
\end{equation*}
Based on this definition,  is easy to verify that the trace obeys the following:

\begin{fact} \label{fact.A.F.A.a}
$\Trace(A)=\Trace(A^T)$;\quad
$\Trace(AB)= a_{ij} b_{ji} = \Trace(BA) = \Trace(B^T A^T) = \Trace(A^T B^T)$;\hfill\break
$\Trace(ABC)= a_{ij} b_{jk} c_{ki} =\Trace(CAB)=\Trace(BCA)=\Trace(C^T B^T A^T)=\Trace( A^T C^T B^T)=\Trace(B^T A^T C^T)$.
\end{fact}

\noindent By the RC_Schur decomposition theorem, $A$ may be decomposed
such that $A=UTU^{H}$, and thus

\begin{fact} \label{fact.A.F.A.b}
$\Trace(A)=\Trace(UTU^{H})=\Trace(U^{H}UT)=\Trace(T)=\lambda_{1}+\lambda_{2}+\ldots+\lambda_{n}$.  \hfill\break
Similarly,
$\Trace(A^k)=\Trace[(UTU^{H})^k]=\Trace[UT^kU^{H}]=\Trace(U^{H}UT^k)=\Trace(T^k)=\lambda_{1}^k+\lambda_{2}^k+\ldots+\lambda_{n}^k$.
\end{fact}

As discussed further in \S \ref{sec:CTCkalman;a}, a {\bf covariance matrix} $P$ is
defined as the {\bf expected value} of the outer product of two vectors. 
In particular, denoting $\Es[\cdot]$ as the {\bf expectation operator} indicating the average over a
large number of statistical samples of the quantity in brackets, if we define
$P={\mathscr E}(\x \x^{H})$, then it is easy to verify (noting that both the expectation and the trace are linear operators) that
$\Trace(P)=\Trace({\mathscr E}(\x \x^{H}))={\mathscr E}(\Trace(\x \x^{H}))={\mathscr E}(\Trace(\x^{H}\x))={\mathscr E}(|x_{1}|^{2}+|x_{2}|^{2}+\ldots+|x_{n}|^{2})$; 
that is, $\Trace(P)$ is the expected 2-norm, or ``energy'', of the vector $\x$.  Thus, the trace of a covariance matrix is a measure of particular significance.

\subsection{Identities involving the derivative of the trace of a product}

Defining the derivative of a scalar $J(X)$ with respect to the matrix $X$ as
\begin{equation*}
  \frac{\partial J}{\partial X} \triangleq
  \begin{pmatrix} \frac{\partial J}{\partial x_{11}} & \ldots & \frac{\partial J}{\partial x_{1n}} \\
                  \vdots & \ddots & \vdots \\
                  \frac{\partial J}{\partial x_{n1}} & \ldots & \frac{\partial J}{\partial x_{nn}} \end{pmatrix},
\end{equation*}
it follows that the derivative of the trace with respect to a constituent matrix $X$ is easily computed using summation notation.  For example,
\begin{subequations}
\label{eq.tr.id}
\begin{alignat}{3}
\frac{\partial\, a_{ij} x_{jk} b_{ki}}{\partial x_{lm}} &= a_{ij} b_{ki} \delta_{jl} \delta_{km} = a_{il} b_{mi} \quad &&\Rightarrow & \quad
\frac{\partial\, \Trace(AXB)}{\partial X} &= A^T B^T, \label{eq.tr.id;a}\\
\frac{\partial\, x_{ji} a_{jk} x_{ki}}{\partial x_{lm}} &= a_{jk} x_{ki} \delta_{jl} \delta_{im} +  x_{ji} a_{jk} \delta_{kl} \delta_{im} 
= a_{lk} x_{km} + a_{jl} x_{jm} \quad &&\Rightarrow & \quad
\frac{\partial\, \Trace(X^T A X)}{\partial X} &= (A+A^T) X. \label{eq.tr.id;b}
\end{alignat}
\end{subequations}
Several alternative forms of the above two identities may be derived by applying Fact \ref{fact.A.F.A.a} and setting either $A$ or $B$ equal to $I$.
The derivative of the trace of other matrix expressions is computed in an analogous fashion.

\clearpage
\subsection{Computing the sensitivity of eigenvalues}\label{sec.eig.sensitivity}

Let the equations $A\s=\lambda \s$ and $\r^T A = \lambda \r^{T}$ define the right and left eigenvectors $\s$ and $\r$ corresponding to the eigenvalue $\lambda$ of the matrix $A$.
Fixing $\r$ and $\s$, the sensitivity of the eigenvalue $\lambda$ arising due to an infinitesimal perturbation of the matrix $A$ may be determined via \eqref{eq.tr.id;a} as follows:
\begin{equation*}
\r^T \Big[A\s = \lambda \s \Big] \quad \Rightarrow \quad
\frac{\partial}{\partial A} \Big[\Trace(\r^T A \s) = \lambda (\r^T \s)\Big] \quad \Rightarrow \quad
\frac{\partial\, \Trace(\r^T A \s)}{\partial A} = \frac{\partial \lambda}{\partial A} (\r^T \s)  \quad \Rightarrow \quad
\frac{\partial \lambda}{\partial A} = \frac{\r\,\s^T}{\r^T \s}.
\end{equation*}

\subsection{Rewriting the characteristic polynomial using the trace}

Combining Fact \ref{fact.A.F.A.b} with Fact \ref{fact.A.D.C.Be}, we see that
{\it the determinant of $A$ is equal to the product of its eigenvalues,
whereas the trace of $A$ is equal to the sum of its eigenvalues}.  Thus, multiplying out the characteristic polynomial \eqref{chareqn} of a matrix $A$, we may write
\begin{equation}
  (\lambda-\lambda_1) (\lambda-\lambda_2) \cdots (\lambda - \lambda_n) = \lambda^n - \Trace(A) \lambda^{n-1} + \ldots + (-1)^k a_{n-k} \lambda^{n-k} + \ldots + (-1)^n |A| = 0.
  \label{chareqnaaa}
\end{equation}
It is easily verified that each coefficient in this polynomial,  $a_{n-k}$ for $k=1,\ldots,n$, is given by the sum of the products of all sets of $k$ eigenvalues of the matrix $A$.  For example, in the $n=3$ case, we have
\begin{equation*}
  \lambda^3 - (\lambda_1+\lambda_2+\lambda_3) \lambda^{2} + (\lambda_1\lambda_2+\lambda_1\lambda_3+\lambda_2\lambda_3) \lambda  - \lambda_1\lambda_2\lambda_3 = 0,
\end{equation*}
and in the $n=4$ case, we have
\begin{align*}
  \lambda^4 - (\lambda_1+\lambda_2+\lambda_3+\lambda_4) \lambda^{3}
  + & (\lambda_1\lambda_2+\lambda_1\lambda_3+\lambda_1\lambda_4+\lambda_2\lambda_3+\lambda_2\lambda_4+\lambda_3\lambda_4) \lambda^2 \\
  - & (\lambda_1\lambda_2\lambda_3+\lambda_1\lambda_2\lambda_4+\lambda_1\lambda_3\lambda_4+\lambda_2\lambda_3\lambda_4) \lambda
  + \lambda_1\lambda_2\lambda_3\lambda_4 = 0.
\end{align*}

We now state and prove an intermediate fact that will help us establish a useful formula for the other coefficients of the characteristic polynomial based on the trace.

\begin{fact}\label{fact:pz-plambda/z-lambda} If $p_n(z)=z^n + a_{n-1} z^{n-1} + \ldots + a_{1} z + a_{0}$ for $n\ge 2$, then
\begin{equation*}
  \frac{p_n(z)-p_n(\lambda)}{z-\lambda} = z^{n-1} + (\lambda + a_{n-1}) z^{n-2} + (\lambda^2 + a_{n-1} \lambda + a_{n-2}) z^{n-1} + \ldots + (\lambda^{n-1} + a_{n-1} \lambda^{n-2} + \ldots + a_{1}).
\end{equation*}
\end{fact}

\noindent {\it Proof (by induction)}\/: The base case $n=2$ follows immediately from
\begin{equation*}
p_2(z)-p_2(\lambda) = (z^2+a_1 z + a_0) - (\lambda^2+a_1 \lambda + a_0) = (z-\lambda)[z + (\lambda + a_1)].
\end{equation*}
Assume the theorem holds for order $n-1$.  Now consider the case of order $n$, and apply the inductive hypothesis to the underbraced term:
\begin{align}
  & p_n(z) - p_n(\lambda)
    = (z^n + a_{n-1} z^{n-1} + \ldots + a_{1} z + a_{0}) - (\lambda^n + a_{n-1} \lambda^{n-1} + \ldots + a_{1} \lambda + a_{0}) \notag \\
   & = z (z^{n-1} + a_{n-1} z^{n-2} + \ldots + a_{1}) - \lambda (\lambda^{n-1} + a_{n-1} \lambda^{n-2} + \ldots + a_{1}) \notag \\
   & = z [\underbrace{(z^{n-1} + a_{n-1} z^{n-2} + \ldots + a_{1}) - (\lambda^{n-1} + a_{n-1} z^{n-2} + \ldots + a_{1})}_{=p_{n-1}(z) - p_{n-1}(\lambda)}]
   + (z- \lambda) (\lambda^{n-1} + a_{n-1} \lambda^{n-2} + \ldots + a_{1}) \notag \\
   & = z [(z-\lambda)(z^{n-2} + (\lambda + a_{n-1}) z^{n-3} + \ldots +  (\lambda^{n-2} + a_{n-1} \lambda^{n-3} + \ldots + a_{2}))] + (z- \lambda) (\lambda^{n-1} + a_{n-1} \lambda^{n-2} + \ldots + a_{1}) \notag \\
   & = (z-\lambda) [z^{n-1} + (\lambda + a_{n-1}) z^{n-2} + \ldots +  (\lambda^{n-2} + a_{n-1} \lambda^{n-3} + \ldots + a_{2}) z + (\lambda^{n-1} + a_{n-1} \lambda^{n-2} + \ldots + a_{1})].     \tag*{$\Box$}
\end{align}
\clearpage

\noindent With this fact established, we may now state and prove the following useful result:

\begin{fact}\label{fact:Tkfork=12n} If $p(z)=z^n + a_{n-1} z^{n-1} + \ldots + a_{1} z + a_{0}=(z-\lambda_1)(z-\lambda_2)\cdots(z-\lambda_n)$ is the characteristic polynomial of $A_{n\times n}$, denoting
$T_k=\Trace(A^k)=\lambda_1^k + \lambda_2^k + \ldots + \lambda_n^k$, we may write
\begin{equation*}
  T_k + a_{n-1} T_{k-1} + \ldots + a_{n-k+1} T_{1} + a_{n-k} k =0 \quad \textrm{for}\quad k=1,2,\ldots,n.
\end{equation*}
\end{fact}

\noindent {\it Proof}\/: First note that
\begin{align*}
p'(z) &= \frac{d p(z)}{dz} = n z^{n-1} + (n-1) a_{n-1} z^{n-2} + (n-2) a_{n-2} z^{n-3} + \ldots + a_1 \\
      &= \sum_{k=1}^n \frac{p(z)}{z-\lambda_k} = \sum_{k=1}^n \frac{p(z) - p(\lambda_k)}{z-\lambda_k} \\
      &= \sum_{k=1}^n [z^{n-1} + (\lambda_k + a_{n-1}) z^{n-2} + (\lambda_k^2 + a_{n-1} \lambda_k + a_{n-2}) z^{n-3} + \ldots + (\lambda_k^{n-1} + a_{n-1} \lambda_k^{n-2} + \ldots + a_{1})]\\
      & = n z^{n-1} + (T_1 + a_{n-1} n) z^{n-2} + (T_2 + a_{n-1} T_1 + a_{n-2} n) z^{n-3} + \ldots + (T_{n-1} + a_{n-1} T_{n-2} + \ldots + a_{2} T_{1} + a_1 n).
\end{align*}
Equating coefficients of like powers of $z$ between the first line and the last, we obtain
\begin{equation*}
\begin{aligned}
  (n-1) a_{n-1} &= T_1 + a_{n-1} n \\
  (n-2) a_{n-2} &= T_2 + a_{n-1} T_1 + a_{n-2} n \\
  & \vdots \\
  a_1 &= T_{n-1} + a_{n-1} T_{n-2} + \ldots + a_{2} T_{1} + a_1 n
\end{aligned}
\quad \Rightarrow \quad 
\begin{aligned}
  0 &= T_1 + a_{n-1} \\
  0 &= T_2 + a_{n-1} T_1 + a_{n-2} 2 \\
  & \vdots \\
  0 &= T_{n-1} + a_{n-1} T_{n-2} + \ldots + a_{2} T_{1} + a_1 (n-1).
\end{aligned}
\end{equation*}
Since each $\lambda_k$ satisfies $p(\lambda_k)=0$, we may also write
\begin{equation}
  \sum_{k=1}^{n} p(\lambda_k) = T_{n} + a_{n-1} T_{n-1} + \ldots + a_{1} T_{1} + a_0 (n) = 0.  \tag*{$\Box$}
\end{equation}

\noindent Note that we may easily rewrite the $n$ relations of Fact \ref{fact:Tkfork=12n} as
\begin{equation}
\begin{alignedat}{2}
a_{n-1} &= - T_1 &&= -\Trace(A)\\
a_{n-2} &= -\frac{1}{2} (T_2+a_{n-1} T_1) &&= -\frac{1}{2} \Trace(A^2 + a_{n-1} A) \\
a_{n-3} &= -\frac{1}{3} (T_3+a_{n-1} T_2 +a_{n-2} T_1) &&= -\frac{1}{3} \Trace(A^3 + a_{n-1} A^2 + a_{n-2} A) \\
  & \vdots && \\
a_{1} &= -\frac{1}{n-1} (T_{n-1}+a_{n-1} T_{n-2} + \ldots + a_{2} T_1) &&= -\frac{1}{n-1} \Trace(A^{n-1} + a_{n-1} A^{n-2} + \ldots + a_{2} A) \\
a_{0} &= -\frac{1}{n}   (T_{n}+a_{n-1} T_{n-1} + \ldots + a_{1} T_1) &&= -\frac{1}{n}   \Trace(A^{n} + a_{n-1} A^{n-1} + \ldots + a_{1} A) 
\end{alignedat}
\label{eq:Tkfork=12nsum}
\end{equation}
This formula will be particularly useful when developing the resolvent algorithm \eqref{resolventalgorithm}.

\clearpage
\section{The Moore-Penrose pseudo"inverse}\label{sec.A.E}

The Moore-Penrose pseudo"inverse is a useful generalization of the
matrix inverse discussed in \S \ref{sec.A.A.J} that is appropriate for
singular and nonsquare matrices.  It is established as follows:

\begin{fact} \label{fact.A.E.A}
Given any $m \times n$ matrix $A$, the Moore-Penrose pseudo"inverse $A^{+}=\underline{V \Sigma}^{-1} \underline{U}^{H}$
is the unique $n \times m$ matrix such that
\begin{equation}
    A A^{+} A = A, \qquad
    A^{+} A A^{+} = A^{+}, \qquad
    A A^{+} = (A A^{+})^{H}, \qquad
    A^{+} A = (A^{+} A)^{H},
    \label{MoorePenroseRelations}
\end{equation}
where $A=\underline{U \Sigma V}^{H}$ is a reduced SVD of $A$.
\end{fact}
\noindent {\it Proof}\/: It is trivial to verify that $A^{+}=\underline{V \Sigma}^{-1} \underline{U}^{H}$ satisfies the four
Moore-Penrose conditions given in \eqref{MoorePenroseRelations}:
\begin{align*}
  A A^{+} A & = \underline{U \Sigma V}^{H} \,\underline{V \Sigma}^{-1} \underline{U}^{H}\, \underline{U \Sigma V}^{H}
              = \underline{U \Sigma \Sigma}^{-1} \underline{\Sigma V}^{H} = A\\
  A^{+} A A^{+} &= \underline{V \Sigma}^{-1} \underline{U}^{H}\, \underline{U \Sigma V}^{H}\,\underline{V \Sigma}^{-1} \underline{U}^{H}
              = \underline{V \Sigma}^{-1} \underline{\Sigma} \underline{\Sigma}^{-1} \underline{U}^{H} = A^{+}\\
  A A^{+} &= \underline{U \Sigma V}^{H}\, \underline{V \Sigma}^{-1} \underline{U}^{H} = \underline{U U}^H = (\underline{U U}^H)^H = (A A^{+})^{H} \\
  A^{+} A & = \underline{V \Sigma}^{-1} \underline{U}^{H}\,\underline{U \Sigma V}^{H} = \underline{V V}^H = (\underline{V V}^H)^H = (A^{+} A)^{H}.
\end{align*}
To show that this value of $A^+$ is unique, consider two matrices $B$ and $C$ that both satisfy the conditions on
$A^+$ given in \eqref{MoorePenroseRelations}:
\begin{gather*}
    A B A = A, \qquad B A B = B, \qquad A B = (A B)^{H}, \qquad B A = (B A)^{H}, \\
    A C A = A, \qquad C A C = C, \qquad A C = (A C)^{H}, \qquad C A = (C A)^{H}.
\end{gather*}
It follows that $AB=B^H A^H=B^H A^H C^H A^H = A B C^H A^H = ABAC=AC$, and, similarly, that $BA=CA$.  We thus conclude that
$B=BAB=BAC=CAC=C$.
\endproof \medskip
 
Note that, if $A$ is a (square) invertible matrix, then $A^{+}=A^{-1}$, and
if $(A^{H} A)$ is invertible, then $A^{+}=(A^{H} A)^{-1} A^{H}$, as may be
verified by substitution into \eqref{MoorePenroseRelations}.

If $A$ is either singular (that is, square with
determinant equal zero) or nonsquare, then the dimension of either the
nullspace and/or the left nullspace of $A$, as depicted in Figure \ref{cartoon1},
is nonzero.  In this case, the mapping between $\Xss$ and $\Yss$ in Figure
\ref{cartoon1} is not one-to-one.  If the dimension of the left nullspace is
nonzero, then for some $\y\in \Yss$, there is no corresponding $\x\in \Xss$
such that $\y=A\x$.  If the dimension of the nullspace is nonzero,
then for some $\y\in \Yss$, there are multiple $\x\in \Xss$ such that
$\y=A\x$.  However, 
{\it the mapping between the row space and the column space is still one-to-one}.
Two distinguishing characteristics
of the Moore-Penrose pseudo"inverse $A^{+}$ are:
\beginmylistb
\item The Moore-Penrose pseudo"inverse $A^{+}$ maps the column
space back to the row space in such a way that, if $\x_{\scriptscriptstyle \Rss}$ is in the row space, then 
$\x_{\scriptscriptstyle \Rss}$ gets mapped back to itself when operated on first by $A$ and then by $A^+$, that is,
\begin{equation*}
A^{+}(A\x_{\scriptscriptstyle \Rss})=
[\underline{V} \underline{\Sigma}^{-1} \underline{U}^{H}] [\underline{U \Sigma V}^{H}] \x_{\scriptscriptstyle \Rss}=
\underline{V V}^H \x_{\scriptscriptstyle \Rss} = \underline\v^j (\underline\v^j,\x_{\scriptscriptstyle \Rss})=\x_{\scriptscriptstyle \Rss},
\end{equation*} 
as $\x_{\scriptscriptstyle \Rss}$ is in the row space of $A$, for which the $\underline\v^j$
form an orthogonal basis.
\item If $\y_{\scriptscriptstyle \Lss}$ is in the left
nullspace (that is, the subspace of $\Yss$ that cannot be reached by the
operation $A\x$ for any $\x$), then the Moore-Penrose pseudo"inverse simply maps $\y_{\scriptscriptstyle \Lss}$
back to zero; that is, 
\begin{equation*}
A^{+}\y_{\scriptscriptstyle \Lss}=\underline{V} \underline{\Sigma}^{-1} \underline{U}^{H} \y_{\scriptscriptstyle \Lss} =0,
\end{equation*}
as $\y_{\scriptscriptstyle \Lss}$ is in the left nullspace of $A$, to which the $\underline\u^j$ (that is, the columns of $\underline U$)
are all orthogonal.
\endmylist
Thus, if $\y=A\x$ where $A$ is possibly singular or nonsquare, {\it the pseudo"inverse in a sense does the
best job possible at mapping a given value of $\y$ back to the
corresponding $\x$} via the mapping $\x=A^{+}\y$, as illustrated graphically in Figure \ref{cartoon3}.

\subsection{Inconsistent and/or underdetermined systems}\label{sec.A.E.A}

Given $A$ and $\b$, the ``best'' solution $\{\x,\epsb\}$ to the
problem
\begin{equation}
    A\x=\b+\epsb
\label{Axpbpeps}
\end{equation}
with $m$ equations and $n$ unknowns (that is, ``best'' in a {\bf
least-squares} sense, minimizing the length of the error vector
$\epsb$) in the inconsistent case (with two or more equations which
are impossible to satisfy simultaneously for any $\x$ when $\epsb=0$)
and/or the underdetermined case (with fewer independent equations than
unknowns) may be found directly using the Moore-Penrose pseudo"inverse.
In order to ensure that $\epsb$ is as small as possible [in the
inconsistent case, a solution $\x$ to \eqref{Axpbpeps} is not possible
with $\epsb=0$], we seek the value of $\x$ such that $\epsb$ is at
least orthogonal to all vectors that may be reached by the operation
$A\x$ for any $\x$.  That is, we want $\epsb$ to be in the left
nullspace, ${\cal N}(A^{H})$, from which it follows that
\begin{equation}
A^{+}\epsb=0.
\label{ape=z}
\end{equation}
This may be enforced by taking $A^{H}\epsb= A^{H}(A\x-\b)=0$, from
which we deduce that
\begin{equation}
    A^{H} A \x=A^{H}\b.
\label{AHAx=AHb}
\end{equation}
If $(A^{H} A)$ is invertible, the solution is seen immediately to be\footnote{In the case that $m\ge n=r$ and thus $(A^{H} A)$ is invertible, $B=A^+=(A^H A)^{-1} A^H$ may also be referred to as a {\bf left inverse} of the (square or tall) matrix $A=A_{m\times n}$, as $B A=I$.  Alternatively, in the case that $n\ge m=r$ and thus $(A A^H)$ is invertible, $C=A^H (A A^H)^{-1}$ may be referred to as a {\bf right inverse} of the (square or fat) matrix $A=A_{m\times n}$, as $A C=I$.  Note that the left inverse of tall matrices and the right inverse of fat matrices, when they exist, are not unique.  In particular, any column vector in the nullspace of a fat matrix $A$ (which necessarily has dimension greater than zero) may be added to any column of a right inverse $C$ without changing the fact that $A C=I$.  Similarly, any row vector in the left nullspace of the tall matrix $A$ (which necessarily has dimension greater than zero) may be added to any row of a left inverse $B$ without changing the fact that $B A =I$.}
\begin{equation}
\x=(A^{H} A)^{-1}A^{H}\b=A^{+}\b.
\label{Aplussol}
\end{equation}
In the underdetermined case, however, $B=(A^{H} A)$ will not be
invertible.  In this case, there are several vectors $\x$ that satisfy
\eqref{AHAx=AHb}; following a similar least-squares mindest, we may
seek the smallest one.  That is, we want $\x$ to be in the row space
of $B$ (and, therefore, in the row space of $A$).  We may ensure this
by selecting an $\x$ such that
\begin{equation}
A^{+} A\x=\x.
\label{ApAx=x}
\end{equation}
Premultiplying \eqref{Axpbpeps} by $A^{+}$ and applying \eqref{ApAx=x}
and \eqref{ape=z}, we again recover the answer $\x=A^{+}\b$, where
$A^{+}$ must now be determined from the reduced SVD of $A$; that is, $A^{+}=\underline{V \Sigma}^{-1} \underline{U}^{H}$ (Fact \ref{fact.A.E.A}).

\subsubsection{The \protect{\underline{$QR$}} approach to inconsistent systems}\label{sec:QRinconsys}

If $(A^{H} A)$ is invertible (that is, if $A=A_{m\times n}$ has rank
$n$), there is a more economical way of finding the least-squares
solution than by using $A^{+}$ or by applying RC_Gaussian elimination to
\eqref{AHAx=AHb}.  Starting from \eqref{AHAx=AHb} and performing the
(reduced) decomposition $A=\underline{QR}$ described in \S
\ref{sec.A.D.B} (noting that $\underline{R}$ also has rank $n$) we
have
\begin{equation*}
    \underline{R}^{H} \underline{Q}^{H} \underline{Q R} \x = \underline{R}^{H} \underline{Q}^{H} \b \quad \Rightarrow \quad
    \underline{R}\x=\underline{Q}^{H} \b.
\end{equation*}
Thus, if we perform the decomposition $A=\underline{QR}$, we may solve
$A\x=\b$ in the inconsistent case (with $m>n$) by solving
$\underline{R}\x=\underline{Q}^{H} \b$ instead of calculating
$\x=A^{+}\b$.  This makes sense computationally, as computing
the $\underline{QR}$ decomposition is much less expensive than
computing the SVD upon which the pseudo"inverse is based.  Further, as
$\underline{R}$ is triangular, the efficient backsubstitution
technique (introduced in \S \ref{sec.B.D} and described in detail in \S
\ref{sec.B.Ab}) may be used to solve $\underline{R}\x=\underline{Q}^{H}
\b$.

The benefit of the $\underline{QR}$ approach to this problem is that it
effectively solves \eqref{AHAx=AHb} without ever performing the product
$A^H A$, thereby avoiding the loss of information caused by performing
this product on a machine with finite-precision arithmetic.  % (see ???).

\subsection{The least-squares solution to the data fitting problem}\label{sec.A.E.B}

The problem of {\bf data fitting} may be described as the problem of
adjusting a relatively small number, $n$, of undetermined coefficients
in a relatively large number, $m$, of realizations of an equation in
order to fit a proposed mathematical model to the available
experimental data as accurately as possible.  If the model is linear
in these undetermined coefficients, such a problem may be written in
the form \eqref{Axpbpeps}, where $\x$ is the vector of undetermined
coefficients, $A$ and $\b$ are related to the data taken, and $\epsb$
is a vector containing the (unknown) measurement errors; the least
squares solution to this problem then gives one ``fit'' of the model
to the data by minimizing the cost function $J=\Vert A\x - \b
\Vert^{2} = \Vert \epsb \Vert^{2}$.  For example, if an $n$'th-order polynomial model
$y=a_0+a_{1} x+ \ldots + a_{n} x^n$ is proposed to fit a set of $(m+1)$ datapoints
$\{x_{j},y_{j}\}$ for $j=0,1,\ldots,m$, as illustrated with $n=2$ in Figure \ref{fig.fit},
then the problem can be written in the form
\begin{equation*}
    \begin{pmatrix} 1 & x_{0} & \ldots & x_{0}^n \\ 1 & x_{1} & \ldots & x_{1}^n \\ \vdots & \vdots & \ddots & \vdots
    \\ 1 & x_{m} & \ldots & x_{m}^n \end{pmatrix}
    \begin{pmatrix} a_{0} \\ a_{1} \\ \vdots \\ a_{n} \end{pmatrix}
	= \begin{pmatrix} y_{0} \\ y_{1} \\ \vdots \\ y_{m} \end{pmatrix}+\epsb \quad \Leftrightarrow \quad A\x=\b+\epsb.
\end{equation*}
If a sufficient amount of data is taken in such an experiment
[that is, if $(m+1)>(n+1)$, where $(m+1)$ is the number of datapoints and $(n+1)$ is the number of coefficients in the
model being adjusted], then $A$ is tall [often, $m\gg n$] and, if the experiments are
well chosen, $\Rank(A)=(n+1)$.  That is, the system is inconsistent but
not underdetermined, in which case the coefficients that minimize $J$,
thereby reconciling the data with the model with the smallest
additional term $\epsb$, may be determined uniquely according to
\eqref{Aplussol}.  If the number of coefficients to be determined is
large, the most efficient way to solve this problem is the
$\underline{QR}$ approach described in \S \ref{sec:QRinconsys}.

\begin{figure}[t!]
\centerline{\psfig{figure=figs/chap04/least_squares.eps,width=2.8in}}

\caption{The data fitting problem [cf.~the interpolation problem in Figure \ref{fig.interp}]: tune a small number of
adjustable parameters to pass a smooth curve (\solid\!) as close as possible to
the (perhaps, numerous) available datapoints ($\times$).}\label{fig.fit}
\end{figure}

\subsubsubsection{Weighted least squares}

\noindent If some measurements are expected to be disrupted by more or less
measurement error than others, or if the error of some measurements is
expected to be correlated with the error of other measurements, we may
account for this knowledge in the formulation of the data fitting
problem, which we now denote
$\overline{A}\x=\overline{\b}+\overline{\epsb}$, by weighting the norm
used in the cost function to be minimized such that $J=\Vert
\overline{A}\x - \overline{\b} \Vert^{2}_{Q} = \Vert \overline{\epsb}
\Vert^{2}_{Q}=\overline{\epsb}^{H} Q \overline{\epsb}$, where
$Q=W^{-1}$ and $W$ is the {\bf covariance matrix} describing (that is,
modeling) the expected statistics of measurement errors such that
$W=\Es[\overline{\epsb} \, \overline{\epsb}^{H}]$, where, again, $\Es[\cdot]$
denotes the expectation operator indicating the average over a
large number of statitical samples of the quantity in brackets.  In
the case that $W$ is diagonal, the weighted cost function takes the
simple form $J=\sum_{j=1}^{m} |\eps_{j}|^{2}/\Es[|\eps_{j}|^{2}]$,
thereby reflecting the fact that such a formulation effectively
``normalizes'' the significance of each available measurement in the
calculation by the inverse of the error expected in the measurement.

The weighted least squares problem may be solved in a straightforward
manner simply by defining $A=Q^{1/2} \overline{A}$, $\b=Q^{1/2}
\overline{\b}$, and $\epsb=Q^{1/2} \overline{\epsb}$, and determining
$\x$ according to the unweighted least-squares algorithm
described previously.

%\section{Calculus of vectors and matrices}
%
%{\it This section still under construction.}
%
%\subsection{The gradient}
%
%\subsection{The Hessian}
%
%\subsection{The derivative of a scalar with respect to a matrix}

\section{Chapter summary}

Linear algebra forms the foundation upon which efficient methods may
be built to solve many important classes of problems numerically.  
At first blush, the subject seems like it must certainly be quite simple and dull, as all it amounts to is
the organization of addition, subtraction, multiplication, and division of blocks
of numbers.  Indeed, in a very real sense, that's all that linear algebra really is.
However, upon further inspection, it is seen that this seemingly simple subject is in fact quite
deep.  The facts that
\beginmylistb
\item not all systems of equations have any solutions at all,
\item those that do might have multiple solutions,
\item any solutions sought using a computer must be calculated using finite-precision arithmetic, and
\item that arithmetic can sometimes take an unacceptably long time to complete,
\endmylist
\noindent are all facets of the explanation of why linear algebra is both difficult and important.  In particular,
we find there are several natural ways to decompose a matrix into the product of other matrices with
special structure (RC_Hessenberg, tridiagonal, triangular, diagonal, unitary, etc.).  These decompositions
may in fact be put to very good use by numerical algorithms that use such matrices.  The most
important of these decompositions are the $LU$/$PLU$/Cholesky, RC_Hessenberg, $QR$, RC_Schur, real RC_Schur, eigen,
Jordan, and singular value decompositions, all of which the reader should become very familiar with before proceeding.  
Various useful measures of matrices have also been presented, including the determinant, trace, matrix norms,
and condition number. A wide variety of useful facts have also be noted along the way that will all be leveraged later in this text.

With this foundation set, we have seen that linear algebra can be used right away for a wide variety practical
problems, including determining the modes of oscillation of a dynamic system and
efficiently solving the problem $A\x=\b$ (even if this system is inconsistent and/or underdetermined).

The present chapter summarizes a vast number of results covering a variety of topics from linear algebra which
are necessary to continue this study.  It is a dense chapter and therefore will likely require the reader to review
a few times to completely digest; rest assured that this effort will not be in vain.

We conclude this chapter by remarking that the subject of numerical linear algebra is a rich
and fascinating subject that may (indeed, should) be studied at a deeper level
than the present, brief review of this subject can possibly achieve,
especially concerning the analysis and quantification of both the rate of convergence and the accumulation
of numerical errors by various schemes that have been set forth to solve the several linear-algebraic problems laid out
in this chapter, and how these schemes may be implemented efficiently in modern parallel computer systems with
cache-based memory architectures.  It is hoped that the introduction provided here will
help to set your compass to guide you through the vast and creative literature on this subject.

% \section{Other matrix measures}\label{sec.A.F}
%
% As discussed previously, the {\bf determinant} is a useful measure of a
% square matrix $A$ that indicates immediately whether or not the matrix
% is singular.  Based on this characterization, the $n$ {\bf eigenvalues} of
% $A_{n\times n}$ (and their corresponding eigenvectors) may also be
% determined, which are immensely valuable in characterizing matrices.
% This concept may be generalized to nonsquare matrices by the
% definition of the $p=\min(m,n)$ {\bf singular values} of $A_{m\times n}$ (and
% their corresponding singular vectors).


% \footnote{When the discrete-time system \eqref{eq:D.time.system} under consideration represents a control problem, the
% existence of such an $X$ is equivalent to the condition that the control problem under consideration is {\bf stabilizable},
% as discussed further in \S \ref{sec.MA.G.Hb}. When the system \eqref{eq:D.time.system} represents an estimation problem, the
% existence of such an $X$ is equivalent to the condition that the estimation problem under consideration is {\bf detectable},
% as discussed further in \S \ref{sec.ZA.G.Hd}.}

\clearpage
\begin{figure}[t!]~\vskip-0.2in
\centerline{\psfig{figure=figs/truss02a.eps,height=1.3in} \hskip0.2in
            \psfig{figure=figs/truss02b.eps,height=1.3in}}

\centerline{CONFIGURATION I \hskip2.1in CONFIGURATION II}
\vskip0.1in

\centerline{\psfig{figure=figs/truss02c.eps,height=1.3in} \hskip0.2in
            \psfig{figure=figs/truss02d.eps,height=1.3in}}

\centerline{CONFIGURATION III \hskip2.1in CONFIGURATION IV}
\vskip0.05in

\caption{Cantilevered trusses attached at a vertical wall at
$\{x_A,y_A\}=\{0,0\}$ and $\{x_B,y_B\}=\{0,1\}$, designed to support a
weight at $\{x_G,y_G\}=\{3,1\}$.  All structural members are assumed
to be joined by frictionless pins and are assumed to be confined to a
2D plane (these idealizations simplify the computations).  Note that SI units will be used
throughout this discussion (forces in Newtons, distances in meters,
etc.).}\label{fig:4.trusses}
\end{figure}

\subsection*{Exercises}\label{sec:04.Exercises}

Problems \ref{ex:04.Truss;1}, \ref{ex:04.Truss;2}, and \ref{ex:04.Truss;3} consider the static loading, displacement under load, and dynamic modes of vibration of a simple cantilevered truss designed to support a weight a fixed distance
from a vertical wall, as illustrated in Figure \ref{fig:4.trusses}.  Choose one of the four configurations illustrated for all three of these questions (indicate clearly
which one you are considering if turning this in for a class!).
\vskip-0.1in

\begin{exercise} \label{ex:04.Truss;1} \rm {\bf Static loading of the structure}\vskip0.05in

\noindent We first determine the forces $\f$ in the truss when $W=1000\
N$, assuming the displacement of each node is negligible and the mass of each member
is negligible.  We begin by writing the equations for static
equilibrium of the loaded system.  Note that the nodes $C$, $D$, $E$,
$F$, and $G$ are free to move in both the horizontal ($x$) and
vertical ($y$) directions.  At equilibrium, the sum of the forces in
both the $x$ and $y$ directions at each of these nodes must be exactly
zero.  Define positive forces $f_i>0$ as members under compression and
negative forces $f_i<0$ as members under tension.  Also, define the 10 angles
$\theta_i$ as the angles each rod makes from
horizontal [e.g., noting \eqref{idenatan2}, $\theta_9=\textrm{atan2}(y_G-y_E,x_G-x_E)$].
The equations of {\bf static equilibrium} are:
\begin{equation*}
\begin{aligned}[t]
{\rm forces\ at\ node\ C:}\ \   &
\begin{cases}\sum \textrm{Forces}_x = \ldots = 0,&\\
             \sum \textrm{Forces}_y = \ldots = 0,&\end{cases}\\[0.05in]
{\rm forces\ at\ node\ D:}\ \   &
\begin{cases}\sum \textrm{Forces}_x = \ldots = 0,&\\
             \sum \textrm{Forces}_y = \ldots = 0,&\end{cases}\\[0.05in]
{\rm forces\ at\ node\ E:}\ \   &
\begin{cases}\sum \textrm{Forces}_x = \ldots = 0,&\\
             \sum \textrm{Forces}_y = \ldots = 0,&\end{cases}
\end{aligned}\ \ \ 
\begin{aligned}[t]
{\rm forces\ at\ node\ F:}\ \   &
\begin{cases}\sum \textrm{Forces}_x = \ldots = 0,&\\
             \sum \textrm{Forces}_y = \ldots = 0,&\end{cases}\\[0.05in]
{\rm forces\ at\ node\ G:}\ \   &
\begin{cases}\sum \textrm{Forces}_x = f_9 \cos(\theta_9) + f_{10} \cos(\theta_{10}) = 0,&\\
             \sum \textrm{Forces}_y = f_9 \sin(\theta_9) + f_{10} \sin(\theta_{10}) = W.&\end{cases}
\end{aligned}
\end{equation*}
\clearpage
There are 10 unknowns in this system, $\{f_1, \ldots, f_{10}\}$. 
The system is {\bf statically determinant},
meaning that the forces in the members may be determined by the conditions of static equilibrium.  
Assume that the nominal configuration of the structure is: $\{x_C,y_C\}=\{1,0\}$, $\{x_D,y_D\}=\{1,1\}$, $\{x_E,y_E\}=\{2,0\}$, $\{x_F,y_F\}=\{2,1\}$.
\vskip0.1in

\noindent ({\bf a}) Defining a geometry vector
$\g=[x_C\ y_C\ x_D\ y_D\ x_E\ y_E\ x_F\ y_F]^T$,
write a function {\tt [f]=TrussForces(g)} which
sets up the system of 10 equations listed above (in the order given)
as $A\f = \b$, then solves this system for the member
forces using one of the RC_Gaussian elimination routines of \S \ref{chap02}.  Is pivoting required to solve this system?
For $\g=[1\ 0\ 1\ 1\ 2\ 0\ 2\ 1]^T$ and
$W=1000$, what are the forces in each member?  Compute the
condition number of $A$.  Do you trust this numerical result?  Which
members are under tension, and which are under compression?
Physically, does this result make sense?
\end{exercise}

The members in the truss which bear
tensile forces when the load is applied will be built with cables with
negligible mass.  The members which bear compressive forces will be built with {\tt I} beams, with size (and
mass) selected appropriately to bear the required load without failing,
subject to an appropriate safety factor. \vskip0.1in

\def\TE{TE}
\def\KE{KE}
\def\PE{PE}
\def\DE{DE}
\def\alphad{{\boldsymbol{\alpha}}}
\def\thetad{{\pmb{\theta}}}
\def\xid{{\boldsymbol{\xi}}}

\begin{exercise} \label{ex:04.Truss;2} \rm {\bf Displacement of the structure under load}\vskip0.05in

\noindent The displacements of the nodes from the nominal design, $\x$, due to
both the applied load $W$ and the acceleration due to gravity, are now
computed.  The displacements are assumed to be small, so the
configuration of the structure is close to nominal (this assumption
will be verified {\it a posteriori}).  Note that Exercise \ref{ex:04.Truss;1} identified
which members are {\tt I} beams (with mass) and which members are tendons (with
negligible mass) in the structure considered.
The weight of the structure itself is modeled with a {\bf lumped mass}
approximation, accounting for half of the mass of each {\tt I} beam as a point
mass at each end of the beam.  Stock beams are initially chosen with a
mass per unit length of $\rho = 5\,$kg/m (this could be optimized once the nominal forces under load are calculated),
and the acceleration due to gravity is $g = 9.81\,\textrm{m/s}^2$.  Assume that the mass per unit length of a
member is $\rho_i = 5\,$kg/m if the member $i$ is under compression ($f_i>0$)
in the loaded structure, and $\rho_i = 0\,$kg/m if the member $i$ is under
tension ($f_i < 0$) in the loaded structure.

The vector $\x$ is referred to as the {\bf configuration} of the
system.  Together, $\x$ and $d\x/dt$ are referred to as the {\bf
state} of the system.  When combined with the equation of motion, as derived in the following section,
initial conditions on the state completely specify how the system will
evolve in time.  The {\bf potential energy} $\PE$ of the system, due
both to the applied external forces and the weight of the structure
itself, the {\bf deformation energy} $\DE$ of the system, due to the
compression or extension of the (elastic) members, and the {\bf
kinetic energy} $\KE$ of the system, due to the motion of the members,
are functions of $\x$ and $d\x/dt$.  Our first task is to
represent these energies in matrix form.  Note that, when the
(unmodeled) damping in the system causes the loaded structure to
approach a static equilibrium, the {\bf total energy}
$\TE=\PE+\DE+\KE$ of the system is minimum and the kinetic energy
$\KE=0$.

A configuration vector $\x$ containing the displacements of the
nodes with respect to their nominal positions, a mass
vector $\m$ containing the lumped masses associated
with the elements of $\x$, and a load vector $\z$ describing the
forces associated with the elements of $\x$ may now be defined such
that\footnote{Note that the mass vector shown is for
Configuration I; the mass vectors for the other three configurations
are slightly different.}
\begin{equation*}
\x = \begin{pmatrix} x'_C \\ y'_C \\ x'_D \\ y'_D \\ x'_E \\ y'_E \\ x'_F \\ y'_F \\ x'_G \\ y'_G
\end{pmatrix},
\qquad
\m = m_0 + \frac{1}{2} \begin{pmatrix} \rho_1 l_1 + \rho_2 l_2 + \rho_4 l_4 + \rho_5 l_5 \\
                                       \rho_1 l_1 + \rho_2 l_2 + \rho_4 l_4 + \rho_5 l_5 \\
                                       \rho_3 l_3 + \rho_4 l_4 + \rho_6 l_6 + \rho_7 l_7 \\
                                       \rho_3 l_3 + \rho_4 l_4 + \rho_6 l_6 + \rho_7 l_7 \\
                                         \vdots
\end{pmatrix},
\qquad
\z = \begin{pmatrix} 0 \\ m_2 a \\
                     0 \\ m_4 a \\
                     0 \\ m_6 a \\
                     0 \\ m_8 a \\
                     0 \\ m_{10} a+W
\end{pmatrix},
\end{equation*}
\clearpage

\noindent where, for example, $x'_C$ and $y'_C$ denote (respectively) the
horizontal and vertical displacements of node $C$ from their nominal
(unloaded) positions, and $\rho_i$ is the mass per unit length of
member $i$.  Note that $m_0=0.1\,$kg is the mass of the hardware at
each joint.  Based on these definitions, the potential energy due to
displacements of the nodes (accounting both for the weight of the
structure and for the applied external loads) may be expressed in
matrix form as
\begin{equation*}
\PE=\z^T \x.
\end{equation*}
Note that upward (positive $y'_i$) displacements of the nodes result in
increases of the potential energy.

The elastic deformation energy due to small deformations of the structure is
\begin{equation*}
\DE=\sum_{i=1}^{10} c_{i} d_{i}^2 = \d^T C \d,
\end{equation*}
where $C=\textrm{diag}(\c)$,
$c_i$ is the {\bf spring constant} of member $i$, and
$d_{i}$ is the difference in length of member $i$ from its nominal length $\ell_i$.  For the present analysis,
assume that $c_i=10^7/\ell_i\,\,\textrm{kg}/\textrm{s}^2$.  Taking
the leading terms only (that is, assuming small deformations), we have
\begin{equation*}
d_{1}=(x'_C-x'_A)\cos(\theta_1) + (y'_C-y'_A) \sin(\theta_1), \hskip0.2in \ldots
\end{equation*}
Applying the definition of $\x$ given above, we may write this system
of equations in matrix form as
\begin{equation*}
\d=D\x,
\end{equation*}
where $D$ is a $10\times 10$ matrix with at most 4 nonzero entries per row.  The deformation energy of the entire
structure is
\begin{equation*}
\DE = \frac{1}{2}\,\d^T C \d = \frac{1}{2}\,\x^T D^T C D \x = \frac{1}{2}\,\x^T K \x,
\end{equation*}
where $K=D^T C D$ is a symmetric $10\times 10$ ``stiffness matrix''.
Note that $K$ is positive definite since (in this problem) for any
nonzero set of displacements $\x$, $\DE$ is positive.

The total energy of the truss at equilibrium may now be written
\begin{equation*}
\TE = \PE+\DE+\KE =\z^T \x_e + {1\over 2}\,\x_e^T K \x_e + 0.
\end{equation*}
\vskip0.1in

\noindent ({\bf a}) Minimizing the above expression for $\TE$ at equilibrium with
respect to $\x_e$, show (using index notation) that equilibrium is
achieved at
\begin{equation*}
\z + K \x_e = 0.
\end{equation*}
Note that this equation may be solved for $\x_e$ once $K$ and $\z$ are
computed.  The forces in the members of the perturbed structure at equilibrium are then given
by $\f_e=-C \d_e=-C D \x_e$.
\vskip0.2in

\noindent ({\bf b}) Write a function {\tt [K]=TrussK(g)} to compute
$K$ and a function {\tt [xe,fe]=TrussLoading(g,z)} to compute the
displacements $\x_e$ and the structural forces $\f_e$ at equilibrium due to the applied
external loads $\z$.  For $\g=[1\ 0\ 1\ 1\ 2\ 0\ 2\ 1]^T$, $W=1000$,
and the loads due to the weight of the structure (as discussed above),
what are $\x_e$ and $\f_e$?  Plot the shape of the displacement of the perturbed structure in Matlab, magnifying the
perturbation enough in the plot so the shape of the perturbation may easily
be seen (indicate how much such magnification is used).  Is the ``small perturbation'' assumption mentioned previously valid?
\end{exercise}
\clearpage

\begin{exercise} \label{ex:04.Truss;3} \rm {\bf Vibration modes of the structure.}\vskip0.05in

We now examine the modes of vibration of the structure.  As the
vibration problem is dynamic, we must now include the kinetic energy
$\KE$ in the computation of the total energy.  The kinetic energy of
the entire structure may be expressed in matrix form as
\begin{equation*}
\KE = {1\over 2}\, \left[\Dnorm{\x}{t}\right]^T \! M\,\Dnorm{\x}{t},
\end{equation*}
where $M=\textrm{diag}(\m)$.  The potential and deformation energies,
$\PE$ and $\DE$, have the same form as in the previous section.  The
total energy of the structure in oscillation (neglecting damping) is:
\begin{equation*}
\TE = \PE+\DE+\KE = \z^T \x + {1\over 2}\,\x^T K \x + {1\over 2}\,\left[\Dnorm{\x}{t}\right]^T \! M\, \Dnorm{\x}{t}.
\end{equation*}\vskip0.1in

\noindent ({\bf a}) Setting the time derivative of the above expression
for the total energy $\TE$ equal to zero for arbitrary $d\x/dt$,
show (using index notation) that
\begin{equation*}
M\,\Dnormtwo{\x}{t} + K \x = -\z.
\end{equation*}\vskip0.1in

\noindent This is referred to as the {\bf equation of motion} of our idealized,
undamped system.  The dynamics of the response do not depend on the
steady-state deformation of the structure, which satisfy $K \x_e = -\z_e$, due to
the applied loading $\z$.  Thus, subtracting this equation from that given above, and defining $\x'(t)=\x(t)-\x_e$, we now focus on the dynamic
modes $\x'(t)$ which satisfy the homogeneous equation
\begin{equation*}
M\Dnormtwo{\x'(t)}{t} + K \x'(t) = 0.
\end{equation*}
We can obtain useful information by extracting the frequencies and
shapes of the normal modes of vibration in this idealization.
Following the SOV approach, we seek modes of this second-order differential equation of the following form:
\begin{equation}
\x(t) = \y_\kappa e^{\imath \omega_\kappa t},
\label{eq:trussexp}
\end{equation}
where $\omega_\kappa$ is the (temporal) frequency of vibration mode $\kappa$, and $\s_\kappa$ is the corresponding natural mode of
vibration of the structure.  This leads to a problem of the form
\begin{equation}
K \y_\kappa = \omega^2_\kappa M \y_\kappa,
\label{eq:trussgenprob}
\end{equation}
which is a {\bf generalized eigenvalue problem} for the eigenvalues $\omega^2_\kappa$ and the corresponding eigenvectors $\y_\kappa$; note that $K$ and $M$ are symmetric.\vskip0.1in

\noindent ({\bf b}) 
Since the matrix $M$ is nonsingular, one could simply multiply \eqref{eq:trussgenprob} from the left by $M^{-1}$ to convert this problem into a regular eigenvalue problem;
however, this would destroy the symmetry of the matrices involved, leading to a more difficult eigenvalue problem to solve. Instead, define an
intermediate vector $\s$ such that $\s=T\y$.  How should $T$ be selected such that this definition transforms \eqref{eq:trussgenprob} into a regular eigenvalue problem
of the form $A\y=\lambda \y$ where $A$ is symmetric?  What is the corresponding equation for $A$?  Is it positive definite?  What can you say about 
the eigenvalues $\lambda_\kappa$ and the corresponding $\omega_\kappa$ (that is, are they real, imaginary, positive, \ldots)?  Noting \eqref{eq:trussexp}, does $\x'(t)$ oscillate in time?  Does it (or, do its oscillations) decay or grow in time?
\vskip0.1in

\noindent ({\bf c}) For $\g=[1\ 0\ 1\ 1\ 2\ 0\ 2\ 1]^T$, find the
natural modes of vibration of the structure $\y_\kappa=T^{-1} \s_\kappa$ and the corresponding frequencies of vibration
$\omega_\kappa$.  Plot (in Matlab) the structure deformed into the three modes with the lowest frequencies.
(These modes are usually the ones that are the least damped in the actual structure, and are thus generally the most significant in practice.)  Discuss.
\end{exercise}

\clearpage
\begin{exercise} \label{ex.nonuniformbar} \rm The PDE governing a longitudinal wave in a 100 cm long aluminum bar is given by
\begin{equation*}
\frac{\partial^2 f}{\partial t^2} = \frac{1}{\rho A} \frac{\partial}{\partial x}\left(E A \frac{\partial f}{\partial x}\right),
\end{equation*}
where the density $\rho=2.7$ g/cm$^3$, Young's modulus $E=6.9\times 10^{11}$ dyne/cm${}^2$, and the bar is assumed to be fixed at both ends.
Consider three cases: (a) the area of the bar $A=1$ cm${}^2$ is constant; (b) the area of the bar varies linearly, from $A=2$ cm${}^2$ at one end to $A=0$ at the other end;
and (c) the area of the bar, shaped like a right circular cone, varies quadratically, from $A=3$ cm${}^2$ at one end to $A=0$ at the other end.  Calculate the first
three mode shapes, and the corresponding frequencies of oscillation, in all three cases.\end{exercise}

\begin{exercise} \label{ex.4.cyclicreduction} \rm {\bf Cyclic reduction.}\vskip0.05in

{\it Under construction.}
\end{exercise}

\begin{exercise} \label{ex.4.Jacobi.RC_Gauss-Seidel} \rm {\it Convergence of the Jacobi and RC_Gauss-Seidel methods (see \S \ref{sec.C.A}).}\vskip0.05in

\noindent ({\bf a}) Writing \eqref{eq:diagdominantdef} for the elements of the matrix $(\lambda A)$ rather than the elements of $A$,
establish that, if $A$ is strictly diagonally dominant and
$|\lambda|\ge 1$, then $B\triangleq \lambda D+L+U$ is strictly diagonally dominant (and, thus, nonsingular) as well.\vskip0.1in

\noindent ({\bf b}) Note that the diagonal elements of $C\triangleq D$ are nonzero as a consequence of Fact \ref{fact.B.Aa.A} (because $A$ is strictly diagonally dominant).
Consider now the characteristic polynomial of $P\triangleq -D^{-1}(L+U)$ and, leveraging part (a), establish that $|\lambda I - P|\ne 0$ if $|\lambda|\ge 1$.  This implies that all eigenvalues
$\lambda$ of $P$ satisfy $|\lambda|\le 1$ and thus, by Fact \ref{fact.induced2norm}, that $\Vert P\Vert_{i2} = \sigma_{\textrm{max}}(P)<1$ as well,
thereby proving that the Jacobi method is convergent [see \eqref{eq:convergent}] if $A$ is strictly diagonally dominant.\vskip0.1in

\noindent ({\bf c}) Repeat parts (a) and (b) taking $B\triangleq \lambda (D+L)+U$ and $C\triangleq D+L$ and $P\triangleq -(D+L)^{-1} U$,
thereby proving that the RC_Gauss-Seidel method is convergent if $A$ is strictly diagonally dominant.
\end{exercise}

\begin{exercise} \label{ex:04.QRmgspivot} \rm Apply the Modified Gram-Schmidt method implemented in Algorithm \ref{alg.3.QRmgs} to a randomly-generated $4\times 4$ matrix in which the third column is an exact copy of the first.  Discuss.
Then, as discussed in \S \ref{sec.A.D.B}, develop an algorithm which applies column pivoting (as implemented in Algorithm \ref{alg.2.3}) to the Modified Gram-Schmidt method (Algorithm \ref{alg.3.QRmgs})
in order to develop an efficient code to calculate a pivoted $\underline{QR}$ decomposition.  Test your new code on the randomly-generated matrix discussed previously.  Discuss.
\end{exercise}

\begin{exercise} \label{ex:04.convshiftquad} \rm Show that \eqref{eq:convshiftquad;b} follows from \eqref{eq:convshiftquad;a} with $\delta \propto \epsilon^2$,
thus establishing that convergence of the shifted $QR$ method is quadratic.
\end{exercise}

\begin{exercise} \label{ex:04.impshiftherm} \rm Noting the Implicit $Q$ Theorem (Fact \ref{fact.implicitQ}) and the development of the implicitly shifted $QR$ iteration
applied in Algorithm \ref{alg.3.RC_EigReal}, modify Algorithm \ref{alg.3.RC_EigHermitian} to apply implicitly shifted $QR$ iterations rather than explicit shifts.  Apply as many
techniques as possible to make this algorithm maximally efficient, and discuss each of them.  Test your code on a wide variety of singular and nonsingular matrices
to make sure it works.  The resulting code will in fact be superior to Algorithm \ref{alg.3.RC_EigHermitian} for the Hermitian eigenvalue problem.
\end{exercise}

\begin{exercise} \label{ex:04.det} \rm Leveraging Algorithm \ref{alg.2.4c} and the discussion in \S \ref{Algorithm},
write an efficient code to calculate the determinant of a large RC_Circulant matrix.
\end{exercise}

\begin{exercise} \label{ex:04.stein} \rm Extending Algorithm \ref{alg.3.Dale}, write an efficient code to solve the Stein equation $X = A^H X B + C$.
\end{exercise}

\begin{exercise} \label{ex:04.pseudoinverse} \rm Calculate (by hand) the Moore-Penrose pseudo"inverse of:
\begin{equation*}
A=\begin{pmatrix} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0 \end{pmatrix}.
\end{equation*}
\end{exercise}

\begin{exercise} \label{ex:04.nilpotent.singular} \rm Is a nilpotent matrix always singular?  What property of the determinint establishes this fact most
directly?  Explain.
\end{exercise}

\subsection*{References}\label{sec:04.References}

\noindent Golub, GH, \& Van Loan, CF (1996) \href{http://books.google.com/books?id=mlOa7wPX6OYC&printsec=frontcover}{\it Matrix Computations}. Johns Hopkins.\vskip0.1in

\noindent Higham, NJ (2002) \href{http://books.google.com/books?id=epilvM5MMxwC&printsec=frontcover}{\it Accuracy and Stability of Numerical Algorithms}.  SIAM.\vskip0.1in

\noindent Horn, RA, \& Johnson, CR (1990) \href{http://books.google.com/books?id=9wTacOjHE6IC&printsec=frontcover}{\it Matrix Analysis}.  Cambridge.\vskip0.1in

\noindent Lancaster, P, \& Tismenetsky, M (1985) {\it The Thoery of Matrices}.  Academic Press.\vskip0.1in

\noindent Pesic, P (2004) \href{http://books.google.com/books?id=nIpGp9Y5AlEC&printsec=frontcover}{\it Abel's proof:
an essay on the sources and meaning of mathematical unsolvability}.  MIT Press.

\noindent Postnikov, MM (2004) {\it Foundations of Galois Theory}. Dover.\vskip0.1in

\noindent Press, WH, Teukolsky, SA, Vetterling, WT, \& Flannery, BP (2007). \href{http://www.nr.com/}{\it Numerical Recipes, the Art of Scientific}
\href{http://www.nr.com/}{\it Computing}. Cambridge.\vskip0.1in

\noindent Stoer, J, \& Bulirsch, R (1980) {\it Introduction to Numerical Analysis}.  Springer-Verlag.\vskip0.1in

\noindent Strang, G (1988) {\it Linear Algebra and its Applications}.  Harcourt Brace Jovanovich.\vskip0.1in

\noindent Trefethen, LN, \& Bau, D (1997) \href{http://books.google.com/books?id=bj-Lu6zjWbEC&printsec=frontcover}{\it Numerical Linear Algebra}.  SIAM.\vskip0.1in

\noindent Wilkenson, JH (1965) \href{http://books.google.com/books?id=5wsK1OP7UFgC&printsec=frontcover}{\it The Algebraic RC_Eigenvalue Problem}.  Oxford.\vskip0.1in

\noindent Kailath, T (1973) Some New Algorithms for Recursive Estimation in Constant Linear Systems, {\it IEEE Trans. Information Theory}, {\bf 19}, 750-760.
